WEBVTT

00:02.000 --> 00:26.000
<v Burr Sutter></v>

00:02.000 --> 00:03.000
I'm Burr Sutter.

00:03.000 --> 00:08.000
I'm a Red Hatter who spends a lot of time talking to technologists about technologies.

00:08.000 --> 00:10.000
We say this a lot at Red Hat.

00:10.000 --> 00:14.000
No single technology provider holds the key to success, including us.

00:14.000 --> 00:17.000
And I would say the same thing about myself.

00:17.000 --> 00:22.000
I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners.

00:22.000 --> 00:29.000
This is Code Comments, an original podcast from Red Hat.

00:29.000 --> 01:14.000
<v Burr Sutter></v>

00:29.000 --> 00:36.000
I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning.

00:36.000 --> 00:41.000
I've been thinking about that for quite some time and I actually had the opportunity to work on a few successful projects, here at Red Hat, 

00:41.000 --> 00:45.000
using those technologies, actually enabling a data set,

00:45.000 --> 00:49.000
gathering a data set, working with a data scientist and data engineering team,

00:49.000 --> 00:45.000
and then training a model and putting that model into production runtime environment.

00:52.000 --> 00:58.000
It was an exciting set of projects and you can see those on numerous YouTube videos that have published out there before.

00:58.000 --> 01:04.000
But I want you to think about the problem space a little bit,

00:58.000 --> 01:04.000
because there are some interesting challenges about a AI/ML.

01:04.000 --> 01:05.000
One is simply just getting access to the data,

01:05.000 --> 01:08.000
and while there are numerous publicly available data sets,

01:08.000 --> 01:14.000
when it comes to your specific enterprise use case, you might not be to find publicly available data.

01:14.000 --> 01:17.000
In many cases you cannot, even for our applications that we created,

01:17.000 --> 01:22.000
we had to create our data set, capture our data set, explore the data set,

01:22.000 --> 01:24.000
and of course, train a model accordingly.

01:24.000 --> 01:28.000
And we also found there's another challenge to be overcome in this a AI/ML world,

01:28.000 --> 01:31.000
and that is access to certain types of hardware.

01:31.000 --> 01:36.000
If you think about an enterprise environment and the creation of an enterprise application specifically for a AI/ML,

01:36.000 --> 01:39.000
end users need an inference engine to run on their hardware.

01:39.000 --> 01:43.000
Hardware that's available to them, to be effective for their application.

01:43.000 --> 01:49.000
Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor.

01:49.000 --> 01:56.000
As those things are whizzing by on the factory line there, looking at them and trying to determine if there is an error or not.

01:56.000 --> 2:01.000
Well, how do you actually make it run on your hardware, your accessible technology that you have today.

02:01.000 --> 02:05.000
Well, there's a solution for this as an open toolkit called OpenVINO.

02:05.000 --> 02:11.000
And you might be thinking, "Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning?"

02:11.000 --> 02:11.000
Well, not according to Ryan Loney, product manager of OpenVINO Developer Tools at Intel.

02:20.000 --> 02:57.000
<v Ryan Loney></v>

2:20.000 --> 02:22.000
I guess I'll start with trying to maybe dispel a myth.

2:22.000 --> 02:26.000
I think that CPUs are widely used for inference today.

2:26.000 --> 02:36.000
So if we look at the data center segment, about 70% of the AI inference is happening on Intel Xeon, on our data center CPUs.

2:36.000 --> 02:43.000
And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO,

2:43.000 --> 02:50.000
is that we're taking models that may have been trained on a GPU using deep learning frameworks like PyTorch or TensorFlow,

2:50.000 --> 02:57.000
and then optimizing them to run on Intel hardware.