<link rel="stylesheet" href="./demo.css"/>
<link rel="stylesheet" href="../rh-audio-player-lightdom.css"/>
<script type="module" src="./rh-audio-player.js"></script>
<h2>rh-audio-player</h2>
<form>
    <p>Options:</p>
    <ul>
        <li><label>Poster: <input name="poster" type="checkbox" checked/></label></li>
        <li><label>Detailed Transcript: <input name="transcript" type="checkbox" checked/></label></li>
        <li><label>Color Palette: 
            <select name="palette">
                <option value="light" selected>Light</option>
                <option value="dark">Dark</option>
                <option value="purple">Purple (saturated)</option>
                <option value="purple-img">Purple with Image (dark, no poster)</option>
                <option value="cyan">Cyan (saturated) </option>
            </select>
        </label></li>
        <li>
            <label>Layout Mode: 
                <select name="mode">
                    <option value="full" selected>Full</option>
                    <option value="compact-wide">Compact Wide</option>
                    <option value="compact">Compact</option>
                    <option value="mini">Mini</option>
                </select>
            </label>
        </li>
        <li><label>Series: <input name="series" value="Code Comments"></label></li>
        <li><label>Episode Title: <input name="title" value="Bringing Deep Learning to Enterprise Applications"></label></li>
        <li><label>About Panel Title: <input name="about" value="About the Episode"></label></li>
        <li><label>Subscribe Panel Title: <input name="subscribe" value="Subscribe"></label></li>
        <li><label>Transcript Panel Title: <input name="transcript" value="Transcript"></label></li>
        <li><label>Right to Left: <input name="rtl" type="checkbox"/></label></li>
    </ul>
</form>
<rh-audio-player
    id="player"
    poster="https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png">
    <p slot="series">Code Comments</p>
    <h3 slot="title">Bringing Deep Learning to Enterprise Applications</h3>
    <rh-audio-player-about slot="about">
        <h4 slot="heading">
            About the Episode
        </h4>
        <p>
            There are a lot of publicly available data sets out there. But when it 
            comes to specific enterprise use cases, you&apos;re not necessarily going to 
            able to find one to train your models. To realize the power of AI/ML in 
            enterprise environments, end users need an inference engine to run on 
            their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open 
            toolkits from Intel that do precisely that. He looks specifically at 
            anomaly detection in use cases as varied as medical imaging and 
            manufacturing.
        </p>
        <p>
            Want to learn more about Anomalib? Check out the research paper that 
            introduces the deep learning library.
        </p>
        <rh-audio-player-profile slot="profile" src="https://www.redhat.com/cms/managed-files/ryan-loney.png">
          <span slot="fullname">Ryan Loney</span><br>
          <span slot="role">Product manager, OpenVINO Developer Tools</span>, <span slot="company">Intel&reg;</span>
        </rh-audio-player-profile>
    </rh-audio-player-about>
    <audio crossorigin="anonymous" slot="media" controls>        
        <source type="audio/mp3" srclang="en" src="https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3">
    </audio>
    <rh-audio-player-subscribe slot="subscribe">
        <h4 slot="heading">Subscribe</h4>
        <p>Subscribe here:</p>
        <a slot="link" href="https://podcasts.apple.com/us/podcast/code-comments/id1649848507" target="_blank" title="Listen on Apple Podcasts" data-analytics-linktype="cta" data-analytics-text="Listen on Apple Podcasts" data-analytics-category="Hero|Listen on Apple Podcasts">
            <img src="https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg" alt="Listen on Apple Podcasts">
        </a>
        <a slot="link" href="https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD" target="_blank" title="Listen on Spotify" data-analytics-linktype="cta" data-analytics-text="Listen on Spotify" data-analytics-category="Hero|Listen on Spotify">
            <img src="https://www.redhat.com/cms/managed-files/badge_spotify.svg" alt="Listen on Spotify">
        </a>
        <a slot="link" href="https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw" target="_blank" title="Listen on Google Podcasts" data-analytics-linktype="cta" data-analytics-text="Listen on Google Podcasts" data-analytics-category="Hero|Listen on Google Podcasts">
            <img src="https://www.redhat.com/cms/managed-files/badge_google-podcast.svg" alt="Listen on Google Podcasts">
        </a>
        <a slot="link" href="https://feeds.pacific-content.com/codecomments" target="_blank" title="Subscribe via RSS Feed" data-analytics-linktype="cta" data-analytics-text="Subscribe via RSS Feed" data-analytics-category="Hero|Subscribe via RSS Feed">
            <img class="img-fluid" src="https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg" alt="Subscribe via RSS Feed">
        </a>
    </rh-audio-player-subscribe>
    <rh-audio-player-transcript id="detail" slot="transcript">
        <rh-audio-player-cue>
            <span slot="start">00:02</span>
            <span slot="voice">Burr Sutter</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:02</span>
            <span slot="text">I'm Burr Sutter.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:04</span>
            <span slot="text">I'm a Red Hatter who spends a lot of time talking to technologists about technologies.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:09</span>
            <span slot="text">We say this a lot at Red Hat.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:11</span>
            <span slot="text">No single technology provider holds the key to success, including us.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:15</span>
            <span slot="text">And I would say the same thing about myself.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:18</span>
            <span slot="text">I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:23</span>
            <span slot="text">This is Code Comments, an original podcast from Red Hat.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:29</span>
            <span slot="voice">Burr Sutter</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:30</span>
            <span slot="text">I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:37</span>
            <span slot="text">I've been thinking about that for quite some time and I actually had the opportunity to work on a few successful projects, here at Red Hat, </span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:42</span>
            <spa slot="text">using those technologies, actually enabling a data set,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:45</span>
            <span slot="text">gathering a data set, working with a data scientist and data engineering team,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:49</span>
            <span slot="text">and then training a model and putting that model into production runtime environment.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:53</span>
            <span slot="text">It was an exciting set of projects and you can see those on numerous YouTube videos that have published out there before.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:58</span>
            <span slot="text">But I want you to think about the problem space a little bit,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:58</span>
            <span slot="text">because there are some interesting challenges about a AI/ML.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:04</span>
            <span slot="text">One is simply just getting access to the data,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:05</span>
            <span slot="text">and while there are numerous publicly available data sets,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:08</span>
            <span slot="text">when it comes to your specific enterprise use case, you might not be to find publicly available data.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:14</span>
            <span slot="text">In many cases you cannot, even for our applications that we created,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:17</span>
            <span slot="text">we had to create our data set, capture our data set, explore the data set,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:22</span>
            <span slot="text">and of course, train a model accordingly.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:24</span>
            <span slot="text">And we also found there's another challenge to be overcome in this a AI/ML world,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:28</span>
            <span slot="text">and that is access to certain types of hardware.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:31</span>
            <span slot="text">If you think about an enterprise environment and the creation of an enterprise application specifically for a AI/ML,</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:36</span>
            <span slot="text">end users need an inference engine to run on their hardware.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:39</span>
            <span slot="text">Hardware that's available to them, to be effective for their application.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:43</span>
            <span slot="text">Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:49</span>
            <span slot="text">As those things are whizzing by on the factory line there, looking at them and trying to determine if there is an error or not.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">02:01</span>
            <span slot="text">Well, there's a solution for this as an open toolkit called OpenVINO.</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">02:05</span>
            <span slot="text">And you might be thinking, "Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning?"</span>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">02:11</span>
            <span slot="text">Well, not according to Ryan Loney, product manager of OpenVINO Developer Tools at Intel.</span>
        </rh-audio-player-cue>
    </rh-audio-player-transcript>
    <rh-audio-player-transcript id="regular">
        <h4 slot="heading">Transcript</h4>
        <rh-audio-player-cue>
            <span slot="start">00:02</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">Hi, I'm Burr Sutter. I'm a Red Hatter who spends a lot of time talking to technologists about technologies. We say this a lot at Red Hat. No single technology provider holds the key to success, including us. And I would say the same thing about myself. I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners. This is Code Comments, an original podcast from Red Hat.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">00:29</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning. I've been thinking about that for quite some time and I actually had the opportunity to work on a few successful projects, here at Red Hat, using those technologies, actually enabling a data set, gathering a data set, working with a data scientist and data engineering team, and then training a model and putting that model into production runtime environment. It was an exciting set of projects and you can see those on numerous YouTube videos that have published out there before. But I want you to think about the problem space a little bit, because there are some interesting challenges about a AI/ML. One is simply just getting access to the data, and while there are numerous publicly available data sets, when it comes to your specific enterprise use case, you might not be to find publicly available data.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:14</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">In many cases you cannot, even for our applications that we created, we had to create our data set, capture our data set, explore the data set, and of course, train a model accordingly. And we also found there's another challenge to be overcome in this a AI/ML world, and that is access to certain types of hardware. If you think about an enterprise environment and the creation of an enterprise application specifically for a AI/ML, end users need an inference engine to run on their hardware. Hardware that's available to them, to be effective for their application. Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor. As those things are whizzing by on the factory line there, looking at them and trying to determine if there is an error or not.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">01:56</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">Well, how do you actually make it run on your hardware, your accessible technology that you have today? Well, there's a solution for this as an open toolkit called OpenVINO. And you might be thinking, "Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning? Well, not according to Ryan Loney, product manager of OpenVINO Developer Tools at Intel.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">02:20</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">I guess I'll start with trying to maybe dispel a myth. I think that CPUs are widely used for inference today. So if we look at the data center segment, about 70% of the AI inference is happening on Intel Xeon, on our data center CPUs. And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO, is that we're taking models that may have been trained on a GPU using deep learning frameworks like PyTorch or TensorFlow, and then optimizing them to run on Intel hardware.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">02:57</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">Ryan joined me to discuss AI/ML in the enterprise across various industries and exploring numerous use cases. Let's talk a little bit about the origin story behind OpenVINO. Tell us more about it and how it came to be and why it came out of Intel.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">03:12</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Definitely. We had the first release of OpenVINO, was back in 2018, so still relatively new. And at that time, we were focused on Computer Vision and pretty tightly coupled with OpenCV, which is another open source library with origins at Intel. It had its first release back in 1999, so it's been around a little bit longer. And many of the software engineers and architects at Intel that were involved with and contributing to OpenCV are working on OpenVINO. So you can think of OpenVINO as complimentary software to OpenCV and we're providing an engine for executing inferences as part of a Computer Vision pipeline, or at least that's how we started.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">03:58</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">But since 2018, we've started to move beyond just Computer Vision inference. So when I say Computer Vision inference, I mean image classification, object detection, segmentation, and now we're moving into natural language processing. Things like speech synthesis, speech recognition, knowledge graphs, time series forecasting and other use cases that don't involve Computer Vision and don't involve inference on pixels. Our latest release, the 2022.1 that came out earlier this year, that was the most significant update that we've had to OpenVINO, since we started in 2018. And the major focus of that release was optimizing for use cases that go beyond Computer Vision.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">04:41</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">And I like that concept that you just mentioned right there, Computer Vision, and you said that you extended those use cases and went beyond that. Could you give us some more concrete examples of Computer Vision?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">04:50</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Sure. When you think about manufacturing, quality control in factories, everything from arc welding, defect detection to inspecting BMW cars on assembly lines, they're using cameras or sensors to collect data and usually it's cameras collecting images like RGB images that you and I can see and looks like something taken from a camera or video camera. But also, things like infrared or computerized tomography scans used in healthcare, X-ray, different types of images where we can draw bounding boxes around regions of interest and say, "This is a defect," or, "This is not a defect." And also, "Is this worker wearing a safety hat or did they forget to put it on?" And so, you can take this and integrate it into a pipeline where you're triggering an alert if somebody forgets to wear their safety mask, or if there's a defect in a product on an assembly line, you can just use cameras and OpenVINO and OpenCV running these on Intel hardware and help to analyze.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">05:58</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And that's what a lot of the partners that we work with are doing, so these independent software vendors. And there's other use cases for things like retail. You think about going to a store and using an automated checkout system. Sometimes people use those automated checkouts and they slide a few extra items into their bag that they don't scan and it's a huge loss for the retail outlets that are providing this way to check out realtime shelf monitoring. We have a Vispera, one of our ISVs that helps keep store shelves stocked by just analyzing the cameras in the stores, detecting when objects are missing from the shelves so that they can be restocked. We have Vistry, another ISV that works with quick service restaurants. When you think about automating the process of, when do I drop the fries into the fryer so that they're warm when the car gets to the drive through window, there's quite a bit of industrial healthcare retail examples that we can walk through.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">06:55</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">And we should dig into some more of those, but I got to tell you, I have a personal experience in this category that I want to share with and you can tell me how silly you might think at this point in time it is. We actually built a keynote demonstration for the Red Hat big stage back in 2015. And I really want to illustrate the concept of asset tracking. So we actually gave everybody in the conference a little Bluetooth token with a little battery, a little watch battery, and a little Bluetooth emitter. And we basically tracked those things around the conference. We basically put a raspberry pi in each of the meeting rooms and up in the lunch room and you could see how the tokens moved from room to room to room.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">07:28</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">It was a relatively simple application, but it occurred to me, after we figured out how to do that with Bluetooth and triangulating Bluetooth signals by looking at relative signal strength from one radio to another and putting that through an Apache Spark application at the time, we then realized, "You know what? This is easier done with cameras." And just simply looking at a camera and having some form of a AI/ML model, a machine learning model, that would say, "There are people here now," or, "There are no people here now." What do you think about that?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">07:56</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">What you just described is exactly the product that Pathr, one of our partners is offering, but they're doing it with Computer Vision and cameras. So when Pathr tries to help retail stores analyze the foot traffic and understand, with heat maps, where are people spending the most time in stores, how many people are coming in, what size groups are coming into the store and trying to help understand if there was a successful transaction from the people who entered the store and left the store, to help with the retail analytics and marketing sales and positioning of products. And so, they're doing that in a way that also protects privacy. And that's something that's really important. So when you talked about those Bluetooth beacons, probably if everyone who walked into a grocery store was asked to put a tracking device in their cart or on their person and say, "You're going to be tracked around the store," they probably wouldn't want to do that.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">08:53</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">The way that you can do this with cameras, is you can detect people as they enter and remove their face. So you can ignore any biometric information and just track the person based on pixels that are present in the detected region of interest. So they're able to analyze... Say a family walks in the door and they can group those people together with object detection and then they can track their movement throughout the store without keeping track of their face, or any biometric, or any personal identifiable information, to avoid things like bias and to make sure that they're protecting the privacy of the shoppers in the store, while still getting that really useful marketing analytics data. So that they can make better decisions about where to place their products. That's one really good example of how Computer Vision, AI with OpenVINO is being used today.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">09:49</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">And that is a great example, because you're definitely spot on. It is invasive when you hand someone a Bluetooth device and say, "Please, keep this with you as you go throughout our store, our mall or throughout our hospital, wherever you might be." Now you mentioned another example earlier in the conversation which was related to worker safety. "Are they wearing a helmet?" I want to talk more about that concept in a real industrial setting, a manufacturing setting, where there might be a factory floor and there's certain requirements. Or better yet there's like a quality assurance requirement, let's say, when it comes to looking at a factory line. I've run that use case often with some of our customers. Can you talk more about those kinds of use cases?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">10:23</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">One of our partners, Robotron, we published a case study, I think last year, where they were working with BMW at one of their factories. And they do quality control inspection, but they're also doing things related to worker safety and analyzing. I use the safety hat example. There's a number of our ISVs and partners who have similar use cases and it comes down to, there's a few reasons that are motivating this and some are related to insurance. It's important to make sure that if you want to have your factory insured, that your workers are protecting themselves and wearing the gear regulatory compliance, you're being asked to properly protect from exposure to chemicals or potentially having something fall and hit someone on the head. So wearing a safety vest, wearing goggles, wearing a helmet, these are things that you need to do inside the factory and you can really easily automate and detect and sometimes without bias.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">11:21</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">I think that's one of the interesting things about the Robotron-BMW example is that they were also blurring, blacking out, so drawing a box to cover the face of the workers in the factory, so that somebody who was analyzing the video footage and getting the alerts saying that, "Bay 21 has a worker without a hat on," that it's not sending their face and in the alert and potentially invading or going against privacy laws or just the ethics of the company. They don't want to introduce bias or have people targeted because it's much better to blur the face and alert and have somebody take care of it on the floor. And then, if you ever need to audit that information later, they have a way to do it where people who need to be able to see who the employee was and look up their personal information, they can do that.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">12:17</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">But then just for the purposes of maintaining safety, they don't need to have access to that personal information, or biometric information. Because that's one thing that when you hear about Computer Vision or person tracking, object detection, there's a lot of concern, and rightfully so, about privacy being invaded and about tracking information, face re-identification, identifying people who may have committed crimes through video footage. And that's just not something that a lot of companies want to... They want to protect privacy and they don't want to be in a situation where they might be violating someone's rights.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">12:56</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">Well, privacy is certainly opening up Pandora's box. There's a lot to be explored in that area, especially in a digital world that we now live in. But for now, let's move on and explore a different area. I'm interested in how machines and computers offer advantages specifically in certain use cases like a quality control scenario. I asked Ryan to explain how a AI/ML and specifically machines, computers, could augment that capability.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">13:20</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">I can give a specific example where we have a partner that's doing defect detection, looking for anomalies in batteries. I'm sure you've heard there's a lot of interest right now in electric vehicles, a lot of batteries being produced. And so, if you go into one of these factories, they have images that they collect of every battery that's going through this assembly line. And through these images, people can look and see and visually inspect what their eyes and say, "This battery has a defect, send it back." And that's one step in the quality control process, there's other steps I'm sure, like running diagnostic tests and measuring voltage and doing other types of non-visual inspection. But for the visual inspection piece, where you can really easily identify some problems, it's much more efficient to introduce Computer Vision. And so, that's where we have this new library that we've introduced, called Anomalib.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">14:17</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">So OpenVINO, while we're focused on inference, we're also thinking about the pipeline, or the funnel, that gets these models to OpenVINO. And so, we've invested in this anomaly segmentation, anomaly detection library that we've recently open sourced and there's a great research paper about it, about Anomalib, but the idea is you can take just a few images and train a model and start detecting these defects. And so, for this battery example, that's a more advanced example, but to make it simpler, take some bolts and... Take 10 bolts. You have one that has a scratch on it, or one that is chipped, or has some damage to it, and you can easily get started in training to recognize the bolts that do not have an anomaly and the ones that do, which is a small data set. And I think that's really one of the most important things today.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">15:11</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Challenges, one is access to data, but the other is needing a massive amount of data to do something meaningful. And so we're starting to try to change that dynamic with Anomalib. You may not need a 100,000 images, you may need 100 images and you can start detecting anomalies in everything from batteries to bolts to, maybe even the wood varnish use case that you mentioned.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">15:37</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">That is a very key point because often in that data scientist process, that data engineering data scientist process, the one key thing is, can you gather the data that you need for the input for the model training? And we've often said, at least people I've worked with over the last couple years, "You need a lot of data, you need tens of thousands of correct images, so we can sort out the difference between dogs versus cats," let's say. Or you need dozens and dozens of situations where if it's a natural language processing scenario, a good customer interaction, a good customer conversation. And this case it sounds like what you're saying is, "Show us just the bad things, fewer images, fewer incorrect things, and then let us look for those kind of anomalies." Can you tell us more about that? Because that is very interesting. The concept that I can use a much smaller data set as my input, as opposed to gathering terabytes of data in some cases, to just simply get my model training underway.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">16:30</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Like you described, the idea is, if you have some good images and then you have some of the known defects, and you can just label, "Here's a set of good images and here's a few of the defects." And you can right away start detecting those specific defects that you've identified. And then, also be able to determine when it doesn't match the expected appearance of a non defective item. So if I have the undamaged screw and then I introduce one with some new anomaly that's never been seen before, I can say this one is not a valid screw. And so, that's the approach that we're taking and it's really important because so often you need to have subject matter experts. Take the battery example, there's these workers who are on the floor, in a factory and they're the ones who know best when they look at these images, which one's going to have an issue, which one's defective.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">17:31</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And then they also need to take that subject matter expertise and then use it to annotate data sets. And when you have these tens of thousands of images you need to annotate, it's asking those people to stop working on the factory floor so they can come annotate some images. That's a tough business call to make, right? But if you only need them to annotate a handful of images, it's a much easier ask to get the ball rolling and demonstrate value. And maybe over time you will want to annotate more and more images because you'll get even better accuracy in the model. Even better, even if it's just small incremental improvements, that's something that if it generates value for the business, it's something the business will invest in over time. But you have to convince the decision makers that it's worth the time of these subject matter experts to stop what they're doing and go and label some images of the things that they're working on in the factory.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">18:27</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">And that labeling process can be very labor intensive. If the annotation is basically saying what is correct, what's wrong, what is this, what is that. And therefore if we can minimize that timeframe to get the value quicker, then there's something that's useful for the business, useful for the organization, long before we necessarily go through a whole huge model training phase.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">18:49</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">So we talked about labeling and how that is labor intensive activity, but I love the idea of helping the human. And helping the human most specifically not get bored. Basically if the human is eyeballing a bunch of widgets flying by, over time they make mistakes, they get bored and they don't pay as close attention as they should. That's why the constant of AI/ML, and specifically Computer Vision augmenting that capability and really helping the human identify anomalies faster, more quickly, maybe with greater accuracy, could be a big win. We focused on manufacturing, but let's actually go into healthcare and learn how these tools can be used in that sector and that industry. Ryan talked me about how OpenVINO's run time can be incorporated into medical imaging equipment with Intel processors embedded in CT, MRI and ultrasound machines. While these inferences, this AI/ML workload, can be operating and executing right there in the same physical room as the patient.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">19:44</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">We did a presentation with GE last year, I think they said there's at least 80 countries that have their x-ray machines deployed. And they're doing things like helping doctors place breathing tubes in patients. So during COVID, during the pandemic, that was a really important tool to help with nurses and doctors who were intubating patients, sometimes in a parking lot or a hallway of a hospital. And when they had a statistic that GE said, I think one out of four breathing tubes gets placed incorrectly when you're doing it outside the operating room. Because when you're in an operating room it's much more controlled and there's someone who's an expert at placing the tubes, it's something you have more of a controlled environment. But when you're out, in a parking lot, in a tent, when the hospital's completely full and you're triaging patients with COVID, that's when they're more likely to make mistakes.And so, they had this endotracheal tube placement, ETT, model that they trained and it helped to use an x-ray and give an alert and say, "This tube is placed wrong, pull it out and do it again." And so, things like that help doctors so that they can avoid mistakes. And having a breathing tube placed incorrectly can cause collapsed lung and a number of other unwanted side effects. So it's really important to do it correctly. Another example is Samsung Medison. They actually are estimating fetal angle of progression. So this is analyzing ultrasound of pregnant women being able to help take measurements that are usually hard to calculate, but it can be done in an automated way. They're already taking an ultrasound scan and now they're executing this model that can take some of these measurements to help the doctor avoid potentially more intrusive alternative methods. So the patient wins, it makes their life better and the doctor is getting help from this AI model. And those are just a few examples.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">21:42</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">Those are some amazing examples when it comes to all these things, we're talking CT scans and x-rays, other examples of Computer Vision. One thing that's kind of interesting in this space, I think, whenever I get a chance to work on, let's say an object detection model, and one of our workshops, by the way, is actually putting that out in front of people to say, "Look, you can use your phone and it basically sends the image over to our OpenShift with our data science platform and then analyzes what you see." And even in my case, where I take a picture of my dog as an example, it can't really decide, is it a dog or a cat? I have a very funny looking dog.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">22:15</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">And so there's always a percentage outcome. In other words, "I think it's a dog, 52%." So I want to talk about that more. How important is it to get to that a hundred percent accuracy? How important is it to really, depending on the use case, to allow for the gray area if you will, where it's an 80% accuracy or a 70% accuracy, and what are the trade offs there associated with the application? Can you discuss that more?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">22:38</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Accuracy is definitely a touchy subject, because how you measure it makes a huge difference. I think what you were describing with the dog example, there's sort of a top five potential classes that might maybe be identified. So let's say you're doing object detection and you detect a region of interest, and it says 65% confidence this is a dog. Well, the next potential label that could be maybe 50% confidence or 20% confidence might be something similar to a dog. Or in the case of models that have been trained on the ImageNet dataset or on COCO dataset, they have actual breeds of dogs. If I want to look at the top five labels for a dog, for my dog for example, she's a mix, mostly a Labrador retriever, but I may look at the top five labels and it may say 65% confidence that she's a flat coated retriever.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">23:32</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And then confidence that she's a husky as 20%, and then 5% confidence that she's a greyhound or something. Those labels, all of them are dogs. So if I'm just trying to figure out, is this a dog? I could probably find all of the classes within the data set and say, "Well, these all, class ID 65, 132, 92 and 158, all belong to a group of dogs." So if I want to just write an application to tell me if this is a dog or not, I would probably use that to determine if it's a dog. But how you measure that as accuracy, well that's where it gets a little bit complicated. Because if you're being really strict about the definition and you're trying to validate against the data set of labeled images, and I have specific dog breeds or some specific detail and it doesn't match, well then, the accuracy's going to go down.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">24:25</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And that's especially important when we talk about things like compression and quantization, which historically, has been difficult to get adoption in some domains, like healthcare, where even the hint of accuracy going down implies that we're not going to be able to help. In some small case, maybe if it's even half a percent of the time, we won't detect that that tube is placed incorrectly or that that patient's lung has collapsed or something like that. And that's something that really prevents adoption of some of these methods that can really boost performance, like quantization. But if you take that example of... Different from the dog example, and you think about segmentation of kidneys. If I'm doing kidney segmentation, which is taking a CT scan and then trying to pick the pixels out of that scan that belong to a kidney, how I measure accuracy may be how many of those pixels I'm able to detect and how many did I miss?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">25:25</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Missing some of the pixels is maybe not a problem, depending on how you've built the application, because you still detect the kidney, and maybe you just need to apply padding around the region of interest, so that you don't miss any of the actual kidney when you compress the model and when you quantize the model. But that requires a data scientist, an ML engineer, somebody to really, they have to be able to go and apply that after the fact, after the inference happens, to make sure that you're not losing critical information. Because the next step from detecting the kidney, may be detecting a tumor.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">26:04</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And so, maybe you can use the more optimized model to detect the kidney, but then you can use a slower model to detect the tumor. But that also requires somebody to architect and make that decision or that trade off and say, "Well, I need to add padding," or, "I should only use the quantized model to detect the region of interest for the kidney." And then, use the model that takes longer to do the inference just to find the tumor, which is going to be on a smaller size. The dimensions are going to be much smaller once we crop to the region of interest. But all of those details, that's maybe not easy to explain in a few sentences and even the way I explained it is probably really confusing.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">26:45</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">I do love that use case, like you mentioned, the cropping, even in one scenario that we worked on for another project, we specifically decided to pixelate the image that we had taken, because we knew that we could get the outcome we wanted by even just using a smaller or having less resolution in our image. And therefore, as we transferred it from the mobile device, the edge device, up into the cloud, we wanted that smaller image just for transfer purposes. And still, we could get the accuracy we needed by a lot of testing.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">27:11</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">And one thing that's interesting about that, from my perspective, is, if you're doing image processing, sometimes it takes a while for this transaction to occur. I come from a traditional application background, where I'm reading and writing things from a database, or a message broker, or moving data from one place to another. Those things happen sub-second normally, even with great latency between your data centers, it's still sub-second in most cases. While a transaction like this one can actually take two seconds or four seconds, as it's doing its analysis and actually coming back with its, "I think it's a dog, I think it's a kidney, I think it's whatever." And providing me that accuracy statement. That concept of optimization is very important in the overall application architecture. Would you agree with that or how do you think about that concept?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">27:56</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Definitely. It depends too on the use case. So if you think about how important it is to reduce the latency and increase the number of frames per second that you can process when you're talking about a loss prevention model that's running at a grocery store. You want to keep the lines moving, you don't want every person who's at the self checkout to have to wait five seconds for every item they scan. You need it to happen as quickly as possible. And if sometimes the accuracy decreases slightly, or I'd say the accuracy of the whole pipeline, so not just looking at the individual model or the individual inference, but let's say that the whole pipeline is not as successful at detecting when somebody steals one item from the self checkout, it's not going to be a life threatening situation. Whereas being hooked up to the x-ray machine with the tube placement model, they might be willing to have the doctor or the nurse wait five seconds to get the result.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">28:55</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">They don't need it to happen in 500 milliseconds. Their threshold for waiting is a little bit higher. That, I think, also drives some of the decision. You want to keep people moving through the checkout line and you can afford to, potentially, if you lose a little bit of accuracy here and there, it's not going to cost the company that much money or it's not going to be life threatening. It's going to be worth the trade off of keeping the line moving and not having people leave the store and not check out at all, to say, "I'm not going to shop today because the line's too long."</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">29:30</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">There are so many trade-offs in enterprise AI/ML use cases, things like latency, accuracy and availability, and certainly complexities abound, especially in an obviously ever-evolving technological landscape where we are still very early in the adoption of AI/ML. And to navigate that complexity, that direct feedback from real world end users is essential to Ryan and his team at Intel. What would you say are some of the big hurdles or big outcomes, big opportunities in that space? And do you agree that we're still at the very beginning, in our infancy if you will, of adopting these technologies and discovering what they can do for us?</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">30:06</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Yeah, I think we're definitely in the infancy and I think that what we've seen is, our customers are evolving and the people who are deploying on Intel hardware, they're trying to run more complicated models. They're the models that are doing object detection or detecting defects and doing segmentation. In the past you could say, "Here's a generic model that will do face detection, or person detection, or vehicle detection, license plate detection." And those are general purpose models that you can just grab off the shelf and use them. But now we're moving into the Anomalib scenarios, where I've got my own data and I'm trying to do something very specific and I'm the only one that has access to this data. You don't have that public data set that you can go download that's under Creative Commons license for car batteries. It's just not something that's available.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">30:57</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And so, those use cases, the challenge with training those models and getting them optimized is the beginning of the pipeline. It's the data. You have to get the data, you have to annotate it and the tools have to exist for you to do that. And that's part of the problem that we're trying to help solve. And then, the models are getting more complex. So if you think, just from working with customers recently, they're no longer just trying to do image classification, "Is it a dog or a cat?" They've moved on to 3D point clouds and 3D segmentation models and things that are like the speech synthesis example. These GPT models that are generating... You put a text input and it generates an image for you. It's just becoming much more advanced, much more sophisticated and on larger images.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">31:50</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">And so things like running super resolution and enhancing images, upscaling images, instead of just trying to take that 200 by 200 pixel image and classifying if it's a cat, now we're talking about gigantic, huge images that we're processing and that all requires more resources or more optimized models. And every Computer Vision conference or AI conference, there's a new latest and greatest architecture, there's new research paper, and things are getting adopted much faster. The lead time for a NeurIPS paper, CVPR, for a company to actually adopt and put those into production, the time shortens every year.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">32:34</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">Well Ryan, I got to tell you, I could talk to you, literally, all day about these topics, the various use cases, the various ways models are being optimized, how to put models into a pipeline for average enterprise applications. I've enjoyed learning about OpenVINO and Anomalib. I'm fascinated by this, because I'll have a chance to go try this myself, taking advantage of Red Hat OpenShift and taking advantage of our data science platform. On top of that, I will definitely go be poking at this myself. Thank you so much for your time today.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">33:00</span> — <span slot="voice">Ryan Loney</span>
            <p slot="text">Thanks, Burr. This was a lot of fun. Thanks for having me.</p>
        </rh-audio-player-cue>
        <rh-audio-player-cue>
            <span slot="start">33:05</span> — <span slot="voice">Burr Sutter</span>
            <p slot="text">You can check out the full transcript of our conversation and more resources, like a link to a white paper on OpenVINO and Anomalib at redhat.com/codecommentspodcast. This episode was produced by Brent Simoneaux and Caroline Creaghead. Our sound designer is Christian Prohom. Our audio team includes Leigh Day, Stephanie Wonderlick, Mike Esser, Laura Barnes, Claire Allison, Nick Burns, Aaron Williamson, Karen King, Boo Boo Howse, Rachel Ertel, Mike Compton, Ocean Matthews, Laura Walters, Alex Traboulsi, and Victoria Lawton. I'm your host, Burr Sutter. Thank you for joining me today on Code Comments. I hope you enjoyed today's session and today's conversation, and I look forward to many more.</p>
        </rh-audio-player-cue>
    </rh-audio-player-transcript>
</rh-audio-player>
<div dir="rtl">
    <rh-audio-player
        id="rtl"
        poster="https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png">
        <p slot="series">Code Comments</p>
        <h3 slot="title">Bringing Deep Learning to Enterprise Applications</h3>
        <rh-audio-player-about slot="about">
            <h4 slot="heading">
                About the Episode
            </h4>
            <p>
                There are a lot of publicly available data sets out there. But when it 
                comes to specific enterprise use cases, you&apos;re not necessarily going to 
                able to find one to train your models. To realize the power of AI/ML in 
                enterprise environments, end users need an inference engine to run on 
                their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open 
                toolkits from Intel that do precisely that. He looks specifically at 
                anomaly detection in use cases as varied as medical imaging and 
                manufacturing.
            </p>
            <p>
                Want to learn more about Anomalib? Check out the research paper that 
                introduces the deep learning library.
            </p>
            <rh-audio-player-profile slot="profile" src="https://www.redhat.com/cms/managed-files/ryan-loney.png">
            <span slot="fullname">Ryan Loney</span><br>
            <span slot="role">Product manager, OpenVINO Developer Tools</span>, <span slot="company">Intel&reg;</span>
            </rh-audio-player-profile>
        </rh-audio-player-about>
        <audio crossorigin="anonymous" slot="media" controls>        
            <source type="audio/mp3" srclang="en" src="https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3">
        </audio>
        <rh-audio-player-subscribe slot="subscribe">
            <h4 slot="heading">Subscribe</h4>
            <p>Subscribe here:</p>
            <a slot="link" href="https://podcasts.apple.com/us/podcast/code-comments/id1649848507" target="_blank" title="Listen on Apple Podcasts" data-analytics-linktype="cta" data-analytics-text="Listen on Apple Podcasts" data-analytics-category="Hero|Listen on Apple Podcasts">
                <img src="https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg" alt="Listen on Apple Podcasts">
            </a>
            <a slot="link" href="https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD" target="_blank" title="Listen on Spotify" data-analytics-linktype="cta" data-analytics-text="Listen on Spotify" data-analytics-category="Hero|Listen on Spotify">
                <img src="https://www.redhat.com/cms/managed-files/badge_spotify.svg" alt="Listen on Spotify">
            </a>
            <a slot="link" href="https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw" target="_blank" title="Listen on Google Podcasts" data-analytics-linktype="cta" data-analytics-text="Listen on Google Podcasts" data-analytics-category="Hero|Listen on Google Podcasts">
                <img src="https://www.redhat.com/cms/managed-files/badge_google-podcast.svg" alt="Listen on Google Podcasts">
            </a>
            <a slot="link" href="https://feeds.pacific-content.com/codecomments" target="_blank" title="Subscribe via RSS Feed" data-analytics-linktype="cta" data-analytics-text="Subscribe via RSS Feed" data-analytics-category="Hero|Subscribe via RSS Feed">
                <img class="img-fluid" src="https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg" alt="Subscribe via RSS Feed">
            </a>
        </rh-audio-player-subscribe>
        <rh-audio-player-transcript id="detail" slot="transcript">
            <rh-audio-player-cue>
                <span slot="start">00:02</span>
                <span slot="voice">Burr Sutter</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:02</span>
                <span slot="text">I'm Burr Sutter.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:04</span>
                <span slot="text">I'm a Red Hatter who spends a lot of time talking to technologists about technologies.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:09</span>
                <span slot="text">We say this a lot at Red Hat.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:11</span>
                <span slot="text">No single technology provider holds the key to success, including us.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:15</span>
                <span slot="text">And I would say the same thing about myself.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:18</span>
                <span slot="text">I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:23</span>
                <span slot="text">This is Code Comments, an original podcast from Red Hat.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:29</span>
                <span slot="voice">Burr Sutter</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:30</span>
                <span slot="text">I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:37</span>
                <span slot="text">I've been thinking about that for quite some time and I actually had the opportunity to work on a few successful projects, here at Red Hat, </span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:42</span>
                <spa slot="text">using those technologies, actually enabling a data set,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:45</span>
                <span slot="text">gathering a data set, working with a data scientist and data engineering team,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:49</span>
                <span slot="text">and then training a model and putting that model into production runtime environment.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:53</span>
                <span slot="text">It was an exciting set of projects and you can see those on numerous YouTube videos that have published out there before.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:58</span>
                <span slot="text">But I want you to think about the problem space a little bit,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:58</span>
                <span slot="text">because there are some interesting challenges about a AI/ML.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:04</span>
                <span slot="text">One is simply just getting access to the data,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:05</span>
                <span slot="text">and while there are numerous publicly available data sets,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:08</span>
                <span slot="text">when it comes to your specific enterprise use case, you might not be to find publicly available data.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:14</span>
                <span slot="text">In many cases you cannot, even for our applications that we created,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:17</span>
                <span slot="text">we had to create our data set, capture our data set, explore the data set,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:22</span>
                <span slot="text">and of course, train a model accordingly.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:24</span>
                <span slot="text">And we also found there's another challenge to be overcome in this a AI/ML world,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:28</span>
                <span slot="text">and that is access to certain types of hardware.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:31</span>
                <span slot="text">If you think about an enterprise environment and the creation of an enterprise application specifically for a AI/ML,</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:36</span>
                <span slot="text">end users need an inference engine to run on their hardware.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:39</span>
                <span slot="text">Hardware that's available to them, to be effective for their application.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:43</span>
                <span slot="text">Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:49</span>
                <span slot="text">As those things are whizzing by on the factory line there, looking at them and trying to determine if there is an error or not.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:01</span>
                <span slot="text">Well, there's a solution for this as an open toolkit called OpenVINO.</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:05</span>
                <span slot="text">And you might be thinking, "Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning?"</span>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:11</span>
                <span slot="text">Well, not according to Ryan Loney, product manager of OpenVINO Developer Tools at Intel.</span>
            </rh-audio-player-cue>
        </rh-audio-player-transcript>
        <rh-audio-player-transcript id="regular">
            <h4 slot="heading">Transcript</h4>
            <rh-audio-player-cue>
                <span slot="start">00:02</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">Hi, I'm Burr Sutter. I'm a Red Hatter who spends a lot of time talking to technologists about technologies. We say this a lot at Red Hat. No single technology provider holds the key to success, including us. And I would say the same thing about myself. I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners. This is Code Comments, an original podcast from Red Hat.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">00:29</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning. I've been thinking about that for quite some time and I actually had the opportunity to work on a few successful projects, here at Red Hat, using those technologies, actually enabling a data set, gathering a data set, working with a data scientist and data engineering team, and then training a model and putting that model into production runtime environment. It was an exciting set of projects and you can see those on numerous YouTube videos that have published out there before. But I want you to think about the problem space a little bit, because there are some interesting challenges about a AI/ML. One is simply just getting access to the data, and while there are numerous publicly available data sets, when it comes to your specific enterprise use case, you might not be to find publicly available data.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:14</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">In many cases you cannot, even for our applications that we created, we had to create our data set, capture our data set, explore the data set, and of course, train a model accordingly. And we also found there's another challenge to be overcome in this a AI/ML world, and that is access to certain types of hardware. If you think about an enterprise environment and the creation of an enterprise application specifically for a AI/ML, end users need an inference engine to run on their hardware. Hardware that's available to them, to be effective for their application. Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor. As those things are whizzing by on the factory line there, looking at them and trying to determine if there is an error or not.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:56</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">Well, how do you actually make it run on your hardware, your accessible technology that you have today? Well, there's a solution for this as an open toolkit called OpenVINO. And you might be thinking, "Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning? Well, not according to Ryan Loney, product manager of OpenVINO Developer Tools at Intel.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:20</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">I guess I'll start with trying to maybe dispel a myth. I think that CPUs are widely used for inference today. So if we look at the data center segment, about 70% of the AI inference is happening on Intel Xeon, on our data center CPUs. And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO, is that we're taking models that may have been trained on a GPU using deep learning frameworks like PyTorch or TensorFlow, and then optimizing them to run on Intel hardware.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:57</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">Ryan joined me to discuss AI/ML in the enterprise across various industries and exploring numerous use cases. Let's talk a little bit about the origin story behind OpenVINO. Tell us more about it and how it came to be and why it came out of Intel.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">03:12</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Definitely. We had the first release of OpenVINO, was back in 2018, so still relatively new. And at that time, we were focused on Computer Vision and pretty tightly coupled with OpenCV, which is another open source library with origins at Intel. It had its first release back in 1999, so it's been around a little bit longer. And many of the software engineers and architects at Intel that were involved with and contributing to OpenCV are working on OpenVINO. So you can think of OpenVINO as complimentary software to OpenCV and we're providing an engine for executing inferences as part of a Computer Vision pipeline, or at least that's how we started.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">03:58</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">But since 2018, we've started to move beyond just Computer Vision inference. So when I say Computer Vision inference, I mean image classification, object detection, segmentation, and now we're moving into natural language processing. Things like speech synthesis, speech recognition, knowledge graphs, time series forecasting and other use cases that don't involve Computer Vision and don't involve inference on pixels. Our latest release, the 2022.1 that came out earlier this year, that was the most significant update that we've had to OpenVINO, since we started in 2018. And the major focus of that release was optimizing for use cases that go beyond Computer Vision.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">04:41</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">And I like that concept that you just mentioned right there, Computer Vision, and you said that you extended those use cases and went beyond that. Could you give us some more concrete examples of Computer Vision?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">04:50</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Sure. When you think about manufacturing, quality control in factories, everything from arc welding, defect detection to inspecting BMW cars on assembly lines, they're using cameras or sensors to collect data and usually it's cameras collecting images like RGB images that you and I can see and looks like something taken from a camera or video camera. But also, things like infrared or computerized tomography scans used in healthcare, X-ray, different types of images where we can draw bounding boxes around regions of interest and say, "This is a defect," or, "This is not a defect." And also, "Is this worker wearing a safety hat or did they forget to put it on?" And so, you can take this and integrate it into a pipeline where you're triggering an alert if somebody forgets to wear their safety mask, or if there's a defect in a product on an assembly line, you can just use cameras and OpenVINO and OpenCV running these on Intel hardware and help to analyze.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">05:58</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And that's what a lot of the partners that we work with are doing, so these independent software vendors. And there's other use cases for things like retail. You think about going to a store and using an automated checkout system. Sometimes people use those automated checkouts and they slide a few extra items into their bag that they don't scan and it's a huge loss for the retail outlets that are providing this way to check out realtime shelf monitoring. We have a Vispera, one of our ISVs that helps keep store shelves stocked by just analyzing the cameras in the stores, detecting when objects are missing from the shelves so that they can be restocked. We have Vistry, another ISV that works with quick service restaurants. When you think about automating the process of, when do I drop the fries into the fryer so that they're warm when the car gets to the drive through window, there's quite a bit of industrial healthcare retail examples that we can walk through.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">06:55</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">And we should dig into some more of those, but I got to tell you, I have a personal experience in this category that I want to share with and you can tell me how silly you might think at this point in time it is. We actually built a keynote demonstration for the Red Hat big stage back in 2015. And I really want to illustrate the concept of asset tracking. So we actually gave everybody in the conference a little Bluetooth token with a little battery, a little watch battery, and a little Bluetooth emitter. And we basically tracked those things around the conference. We basically put a raspberry pi in each of the meeting rooms and up in the lunch room and you could see how the tokens moved from room to room to room.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">07:28</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">It was a relatively simple application, but it occurred to me, after we figured out how to do that with Bluetooth and triangulating Bluetooth signals by looking at relative signal strength from one radio to another and putting that through an Apache Spark application at the time, we then realized, "You know what? This is easier done with cameras." And just simply looking at a camera and having some form of a AI/ML model, a machine learning model, that would say, "There are people here now," or, "There are no people here now." What do you think about that?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">07:56</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">What you just described is exactly the product that Pathr, one of our partners is offering, but they're doing it with Computer Vision and cameras. So when Pathr tries to help retail stores analyze the foot traffic and understand, with heat maps, where are people spending the most time in stores, how many people are coming in, what size groups are coming into the store and trying to help understand if there was a successful transaction from the people who entered the store and left the store, to help with the retail analytics and marketing sales and positioning of products. And so, they're doing that in a way that also protects privacy. And that's something that's really important. So when you talked about those Bluetooth beacons, probably if everyone who walked into a grocery store was asked to put a tracking device in their cart or on their person and say, "You're going to be tracked around the store," they probably wouldn't want to do that.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">08:53</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">The way that you can do this with cameras, is you can detect people as they enter and remove their face. So you can ignore any biometric information and just track the person based on pixels that are present in the detected region of interest. So they're able to analyze... Say a family walks in the door and they can group those people together with object detection and then they can track their movement throughout the store without keeping track of their face, or any biometric, or any personal identifiable information, to avoid things like bias and to make sure that they're protecting the privacy of the shoppers in the store, while still getting that really useful marketing analytics data. So that they can make better decisions about where to place their products. That's one really good example of how Computer Vision, AI with OpenVINO is being used today.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">09:49</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">And that is a great example, because you're definitely spot on. It is invasive when you hand someone a Bluetooth device and say, "Please, keep this with you as you go throughout our store, our mall or throughout our hospital, wherever you might be." Now you mentioned another example earlier in the conversation which was related to worker safety. "Are they wearing a helmet?" I want to talk more about that concept in a real industrial setting, a manufacturing setting, where there might be a factory floor and there's certain requirements. Or better yet there's like a quality assurance requirement, let's say, when it comes to looking at a factory line. I've run that use case often with some of our customers. Can you talk more about those kinds of use cases?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">10:23</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">One of our partners, Robotron, we published a case study, I think last year, where they were working with BMW at one of their factories. And they do quality control inspection, but they're also doing things related to worker safety and analyzing. I use the safety hat example. There's a number of our ISVs and partners who have similar use cases and it comes down to, there's a few reasons that are motivating this and some are related to insurance. It's important to make sure that if you want to have your factory insured, that your workers are protecting themselves and wearing the gear regulatory compliance, you're being asked to properly protect from exposure to chemicals or potentially having something fall and hit someone on the head. So wearing a safety vest, wearing goggles, wearing a helmet, these are things that you need to do inside the factory and you can really easily automate and detect and sometimes without bias.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">11:21</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">I think that's one of the interesting things about the Robotron-BMW example is that they were also blurring, blacking out, so drawing a box to cover the face of the workers in the factory, so that somebody who was analyzing the video footage and getting the alerts saying that, "Bay 21 has a worker without a hat on," that it's not sending their face and in the alert and potentially invading or going against privacy laws or just the ethics of the company. They don't want to introduce bias or have people targeted because it's much better to blur the face and alert and have somebody take care of it on the floor. And then, if you ever need to audit that information later, they have a way to do it where people who need to be able to see who the employee was and look up their personal information, they can do that.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">12:17</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">But then just for the purposes of maintaining safety, they don't need to have access to that personal information, or biometric information. Because that's one thing that when you hear about Computer Vision or person tracking, object detection, there's a lot of concern, and rightfully so, about privacy being invaded and about tracking information, face re-identification, identifying people who may have committed crimes through video footage. And that's just not something that a lot of companies want to... They want to protect privacy and they don't want to be in a situation where they might be violating someone's rights.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">12:56</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">Well, privacy is certainly opening up Pandora's box. There's a lot to be explored in that area, especially in a digital world that we now live in. But for now, let's move on and explore a different area. I'm interested in how machines and computers offer advantages specifically in certain use cases like a quality control scenario. I asked Ryan to explain how a AI/ML and specifically machines, computers, could augment that capability.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">13:20</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">I can give a specific example where we have a partner that's doing defect detection, looking for anomalies in batteries. I'm sure you've heard there's a lot of interest right now in electric vehicles, a lot of batteries being produced. And so, if you go into one of these factories, they have images that they collect of every battery that's going through this assembly line. And through these images, people can look and see and visually inspect what their eyes and say, "This battery has a defect, send it back." And that's one step in the quality control process, there's other steps I'm sure, like running diagnostic tests and measuring voltage and doing other types of non-visual inspection. But for the visual inspection piece, where you can really easily identify some problems, it's much more efficient to introduce Computer Vision. And so, that's where we have this new library that we've introduced, called Anomalib.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">14:17</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">So OpenVINO, while we're focused on inference, we're also thinking about the pipeline, or the funnel, that gets these models to OpenVINO. And so, we've invested in this anomaly segmentation, anomaly detection library that we've recently open sourced and there's a great research paper about it, about Anomalib, but the idea is you can take just a few images and train a model and start detecting these defects. And so, for this battery example, that's a more advanced example, but to make it simpler, take some bolts and... Take 10 bolts. You have one that has a scratch on it, or one that is chipped, or has some damage to it, and you can easily get started in training to recognize the bolts that do not have an anomaly and the ones that do, which is a small data set. And I think that's really one of the most important things today.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">15:11</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Challenges, one is access to data, but the other is needing a massive amount of data to do something meaningful. And so we're starting to try to change that dynamic with Anomalib. You may not need a 100,000 images, you may need 100 images and you can start detecting anomalies in everything from batteries to bolts to, maybe even the wood varnish use case that you mentioned.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">15:37</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">That is a very key point because often in that data scientist process, that data engineering data scientist process, the one key thing is, can you gather the data that you need for the input for the model training? And we've often said, at least people I've worked with over the last couple years, "You need a lot of data, you need tens of thousands of correct images, so we can sort out the difference between dogs versus cats," let's say. Or you need dozens and dozens of situations where if it's a natural language processing scenario, a good customer interaction, a good customer conversation. And this case it sounds like what you're saying is, "Show us just the bad things, fewer images, fewer incorrect things, and then let us look for those kind of anomalies." Can you tell us more about that? Because that is very interesting. The concept that I can use a much smaller data set as my input, as opposed to gathering terabytes of data in some cases, to just simply get my model training underway.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">16:30</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Like you described, the idea is, if you have some good images and then you have some of the known defects, and you can just label, "Here's a set of good images and here's a few of the defects." And you can right away start detecting those specific defects that you've identified. And then, also be able to determine when it doesn't match the expected appearance of a non defective item. So if I have the undamaged screw and then I introduce one with some new anomaly that's never been seen before, I can say this one is not a valid screw. And so, that's the approach that we're taking and it's really important because so often you need to have subject matter experts. Take the battery example, there's these workers who are on the floor, in a factory and they're the ones who know best when they look at these images, which one's going to have an issue, which one's defective.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">17:31</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And then they also need to take that subject matter expertise and then use it to annotate data sets. And when you have these tens of thousands of images you need to annotate, it's asking those people to stop working on the factory floor so they can come annotate some images. That's a tough business call to make, right? But if you only need them to annotate a handful of images, it's a much easier ask to get the ball rolling and demonstrate value. And maybe over time you will want to annotate more and more images because you'll get even better accuracy in the model. Even better, even if it's just small incremental improvements, that's something that if it generates value for the business, it's something the business will invest in over time. But you have to convince the decision makers that it's worth the time of these subject matter experts to stop what they're doing and go and label some images of the things that they're working on in the factory.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">18:27</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">And that labeling process can be very labor intensive. If the annotation is basically saying what is correct, what's wrong, what is this, what is that. And therefore if we can minimize that timeframe to get the value quicker, then there's something that's useful for the business, useful for the organization, long before we necessarily go through a whole huge model training phase.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">18:49</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">So we talked about labeling and how that is labor intensive activity, but I love the idea of helping the human. And helping the human most specifically not get bored. Basically if the human is eyeballing a bunch of widgets flying by, over time they make mistakes, they get bored and they don't pay as close attention as they should. That's why the constant of AI/ML, and specifically Computer Vision augmenting that capability and really helping the human identify anomalies faster, more quickly, maybe with greater accuracy, could be a big win. We focused on manufacturing, but let's actually go into healthcare and learn how these tools can be used in that sector and that industry. Ryan talked me about how OpenVINO's run time can be incorporated into medical imaging equipment with Intel processors embedded in CT, MRI and ultrasound machines. While these inferences, this AI/ML workload, can be operating and executing right there in the same physical room as the patient.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">19:44</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">We did a presentation with GE last year, I think they said there's at least 80 countries that have their x-ray machines deployed. And they're doing things like helping doctors place breathing tubes in patients. So during COVID, during the pandemic, that was a really important tool to help with nurses and doctors who were intubating patients, sometimes in a parking lot or a hallway of a hospital. And when they had a statistic that GE said, I think one out of four breathing tubes gets placed incorrectly when you're doing it outside the operating room. Because when you're in an operating room it's much more controlled and there's someone who's an expert at placing the tubes, it's something you have more of a controlled environment. But when you're out, in a parking lot, in a tent, when the hospital's completely full and you're triaging patients with COVID, that's when they're more likely to make mistakes.And so, they had this endotracheal tube placement, ETT, model that they trained and it helped to use an x-ray and give an alert and say, "This tube is placed wrong, pull it out and do it again." And so, things like that help doctors so that they can avoid mistakes. And having a breathing tube placed incorrectly can cause collapsed lung and a number of other unwanted side effects. So it's really important to do it correctly. Another example is Samsung Medison. They actually are estimating fetal angle of progression. So this is analyzing ultrasound of pregnant women being able to help take measurements that are usually hard to calculate, but it can be done in an automated way. They're already taking an ultrasound scan and now they're executing this model that can take some of these measurements to help the doctor avoid potentially more intrusive alternative methods. So the patient wins, it makes their life better and the doctor is getting help from this AI model. And those are just a few examples.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">21:42</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">Those are some amazing examples when it comes to all these things, we're talking CT scans and x-rays, other examples of Computer Vision. One thing that's kind of interesting in this space, I think, whenever I get a chance to work on, let's say an object detection model, and one of our workshops, by the way, is actually putting that out in front of people to say, "Look, you can use your phone and it basically sends the image over to our OpenShift with our data science platform and then analyzes what you see." And even in my case, where I take a picture of my dog as an example, it can't really decide, is it a dog or a cat? I have a very funny looking dog.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">22:15</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">And so there's always a percentage outcome. In other words, "I think it's a dog, 52%." So I want to talk about that more. How important is it to get to that a hundred percent accuracy? How important is it to really, depending on the use case, to allow for the gray area if you will, where it's an 80% accuracy or a 70% accuracy, and what are the trade offs there associated with the application? Can you discuss that more?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">22:38</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Accuracy is definitely a touchy subject, because how you measure it makes a huge difference. I think what you were describing with the dog example, there's sort of a top five potential classes that might maybe be identified. So let's say you're doing object detection and you detect a region of interest, and it says 65% confidence this is a dog. Well, the next potential label that could be maybe 50% confidence or 20% confidence might be something similar to a dog. Or in the case of models that have been trained on the ImageNet dataset or on COCO dataset, they have actual breeds of dogs. If I want to look at the top five labels for a dog, for my dog for example, she's a mix, mostly a Labrador retriever, but I may look at the top five labels and it may say 65% confidence that she's a flat coated retriever.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">23:32</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And then confidence that she's a husky as 20%, and then 5% confidence that she's a greyhound or something. Those labels, all of them are dogs. So if I'm just trying to figure out, is this a dog? I could probably find all of the classes within the data set and say, "Well, these all, class ID 65, 132, 92 and 158, all belong to a group of dogs." So if I want to just write an application to tell me if this is a dog or not, I would probably use that to determine if it's a dog. But how you measure that as accuracy, well that's where it gets a little bit complicated. Because if you're being really strict about the definition and you're trying to validate against the data set of labeled images, and I have specific dog breeds or some specific detail and it doesn't match, well then, the accuracy's going to go down.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">24:25</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And that's especially important when we talk about things like compression and quantization, which historically, has been difficult to get adoption in some domains, like healthcare, where even the hint of accuracy going down implies that we're not going to be able to help. In some small case, maybe if it's even half a percent of the time, we won't detect that that tube is placed incorrectly or that that patient's lung has collapsed or something like that. And that's something that really prevents adoption of some of these methods that can really boost performance, like quantization. But if you take that example of... Different from the dog example, and you think about segmentation of kidneys. If I'm doing kidney segmentation, which is taking a CT scan and then trying to pick the pixels out of that scan that belong to a kidney, how I measure accuracy may be how many of those pixels I'm able to detect and how many did I miss?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">25:25</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Missing some of the pixels is maybe not a problem, depending on how you've built the application, because you still detect the kidney, and maybe you just need to apply padding around the region of interest, so that you don't miss any of the actual kidney when you compress the model and when you quantize the model. But that requires a data scientist, an ML engineer, somebody to really, they have to be able to go and apply that after the fact, after the inference happens, to make sure that you're not losing critical information. Because the next step from detecting the kidney, may be detecting a tumor.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">26:04</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And so, maybe you can use the more optimized model to detect the kidney, but then you can use a slower model to detect the tumor. But that also requires somebody to architect and make that decision or that trade off and say, "Well, I need to add padding," or, "I should only use the quantized model to detect the region of interest for the kidney." And then, use the model that takes longer to do the inference just to find the tumor, which is going to be on a smaller size. The dimensions are going to be much smaller once we crop to the region of interest. But all of those details, that's maybe not easy to explain in a few sentences and even the way I explained it is probably really confusing.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">26:45</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">I do love that use case, like you mentioned, the cropping, even in one scenario that we worked on for another project, we specifically decided to pixelate the image that we had taken, because we knew that we could get the outcome we wanted by even just using a smaller or having less resolution in our image. And therefore, as we transferred it from the mobile device, the edge device, up into the cloud, we wanted that smaller image just for transfer purposes. And still, we could get the accuracy we needed by a lot of testing.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">27:11</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">And one thing that's interesting about that, from my perspective, is, if you're doing image processing, sometimes it takes a while for this transaction to occur. I come from a traditional application background, where I'm reading and writing things from a database, or a message broker, or moving data from one place to another. Those things happen sub-second normally, even with great latency between your data centers, it's still sub-second in most cases. While a transaction like this one can actually take two seconds or four seconds, as it's doing its analysis and actually coming back with its, "I think it's a dog, I think it's a kidney, I think it's whatever." And providing me that accuracy statement. That concept of optimization is very important in the overall application architecture. Would you agree with that or how do you think about that concept?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">27:56</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Definitely. It depends too on the use case. So if you think about how important it is to reduce the latency and increase the number of frames per second that you can process when you're talking about a loss prevention model that's running at a grocery store. You want to keep the lines moving, you don't want every person who's at the self checkout to have to wait five seconds for every item they scan. You need it to happen as quickly as possible. And if sometimes the accuracy decreases slightly, or I'd say the accuracy of the whole pipeline, so not just looking at the individual model or the individual inference, but let's say that the whole pipeline is not as successful at detecting when somebody steals one item from the self checkout, it's not going to be a life threatening situation. Whereas being hooked up to the x-ray machine with the tube placement model, they might be willing to have the doctor or the nurse wait five seconds to get the result.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">28:55</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">They don't need it to happen in 500 milliseconds. Their threshold for waiting is a little bit higher. That, I think, also drives some of the decision. You want to keep people moving through the checkout line and you can afford to, potentially, if you lose a little bit of accuracy here and there, it's not going to cost the company that much money or it's not going to be life threatening. It's going to be worth the trade off of keeping the line moving and not having people leave the store and not check out at all, to say, "I'm not going to shop today because the line's too long."</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">29:30</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">There are so many trade-offs in enterprise AI/ML use cases, things like latency, accuracy and availability, and certainly complexities abound, especially in an obviously ever-evolving technological landscape where we are still very early in the adoption of AI/ML. And to navigate that complexity, that direct feedback from real world end users is essential to Ryan and his team at Intel. What would you say are some of the big hurdles or big outcomes, big opportunities in that space? And do you agree that we're still at the very beginning, in our infancy if you will, of adopting these technologies and discovering what they can do for us?</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">30:06</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Yeah, I think we're definitely in the infancy and I think that what we've seen is, our customers are evolving and the people who are deploying on Intel hardware, they're trying to run more complicated models. They're the models that are doing object detection or detecting defects and doing segmentation. In the past you could say, "Here's a generic model that will do face detection, or person detection, or vehicle detection, license plate detection." And those are general purpose models that you can just grab off the shelf and use them. But now we're moving into the Anomalib scenarios, where I've got my own data and I'm trying to do something very specific and I'm the only one that has access to this data. You don't have that public data set that you can go download that's under Creative Commons license for car batteries. It's just not something that's available.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">30:57</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And so, those use cases, the challenge with training those models and getting them optimized is the beginning of the pipeline. It's the data. You have to get the data, you have to annotate it and the tools have to exist for you to do that. And that's part of the problem that we're trying to help solve. And then, the models are getting more complex. So if you think, just from working with customers recently, they're no longer just trying to do image classification, "Is it a dog or a cat?" They've moved on to 3D point clouds and 3D segmentation models and things that are like the speech synthesis example. These GPT models that are generating... You put a text input and it generates an image for you. It's just becoming much more advanced, much more sophisticated and on larger images.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">31:50</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">And so things like running super resolution and enhancing images, upscaling images, instead of just trying to take that 200 by 200 pixel image and classifying if it's a cat, now we're talking about gigantic, huge images that we're processing and that all requires more resources or more optimized models. And every Computer Vision conference or AI conference, there's a new latest and greatest architecture, there's new research paper, and things are getting adopted much faster. The lead time for a NeurIPS paper, CVPR, for a company to actually adopt and put those into production, the time shortens every year.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">32:34</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">Well Ryan, I got to tell you, I could talk to you, literally, all day about these topics, the various use cases, the various ways models are being optimized, how to put models into a pipeline for average enterprise applications. I've enjoyed learning about OpenVINO and Anomalib. I'm fascinated by this, because I'll have a chance to go try this myself, taking advantage of Red Hat OpenShift and taking advantage of our data science platform. On top of that, I will definitely go be poking at this myself. Thank you so much for your time today.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">33:00</span> — <span slot="voice">Ryan Loney</span>
                <p slot="text">Thanks, Burr. This was a lot of fun. Thanks for having me.</p>
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">33:05</span> — <span slot="voice">Burr Sutter</span>
                <p slot="text">You can check out the full transcript of our conversation and more resources, like a link to a white paper on OpenVINO and Anomalib at redhat.com/codecommentspodcast. This episode was produced by Brent Simoneaux and Caroline Creaghead. Our sound designer is Christian Prohom. Our audio team includes Leigh Day, Stephanie Wonderlick, Mike Esser, Laura Barnes, Claire Allison, Nick Burns, Aaron Williamson, Karen King, Boo Boo Howse, Rachel Ertel, Mike Compton, Ocean Matthews, Laura Walters, Alex Traboulsi, and Victoria Lawton. I'm your host, Burr Sutter. Thank you for joining me today on Code Comments. I hope you enjoyed today's session and today's conversation, and I look forward to many more.</p>
            </rh-audio-player-cue>
        </rh-audio-player-transcript>
    </rh-audio-player>
</div>
<div>
    <h3>Language Localization</h3> 
    <rh-audio-player lang="es" poster="https://www.redhat.com/cms/managed-files/img-clh-s4e1-hero-455x539_0.png">
        <p slot="series">Temporada 4, Episodio 1</p>
        <h4 slot="title">Minicomputadoras: el alma de las máquinas de antes</h4>
        <audio crossorigin="anonymous" slot="media" control srclang="es" src="//cdn.simplecast.com/audio/ec894038-2e91-449e-9bff-e7ebc323c3e6/episodes/7507a7a1-7340-43f9-bde2-bb0645646ff6/audio/ab8ed5c7-9fdc-4a39-8384-141d66f02c46/default_tc.mp3"></audio>
        <rh-audio-player-about slot="about" label="Notas del podcast">
            <p>Sí, es cierto, las minicomputadoras no caben en tu bolsillo, pero en su momento representaron un avance importante porque redujeron el espacio que necesitaban sus antecesoras, las mainframes, que ocupaban habitaciones enteras. Además, abrieron la posibilidad de que las computadoras personales cupieran en una bolsa y de que, posteriormente, se convirtieran en el teléfono que traes en tu bolsillo. </p>
            <p>Las computadoras de 16 bits cambiaron el mundo de la tecnología de la información en los años 70. Gracias a ellas, las empresas tuvieron la posibilidad de darle a cada ingeniero su propia máquina. Pero los avances aún no eran suficientes; todavía faltaba que llegaran las versiones de 32 bits. </p>
            <p>Carl Alsing y Jim Guyer nos hablan del trabajo que realizaron en Data General para crear una nueva y revolucionaria máquina de 32 bits. Y aunque ahora esos esfuerzos son toda una leyenda, en su momento se realizaron en secreto. “Eagle” era el nombre clave de la computadora que diseñaron, cuyo primer propósito era competir con otra máquina que estaba desarrollando otro equipo de la misma empresa. Los ingenieros nos hablan de las políticas corporativas y nos explican todas las tramas necesarias para que el proyecto pudiera seguir su curso, e incluso nos dicen cómo lograron que las restricciones jugaran a su favor. Neal Firth nos cuenta cómo vivió un proyecto muy emocionante pero exigente, en que nuestros héroes trabajaron juntos por pura voluntad, sin ninguna expectativa de fama ni fortuna. Y los tres nos mencionan que la historia quedó inmortalizada en el libro clásico de ingeniería de Tracy Kidder, <em>El alma de una nueva máquina,</em> que se basa en hechos reales.</p>
        </rh-audio-player-about>
        <rh-audio-player-transcript slot="transcript" label="Transcripción">
            <rh-audio-player-cue>
                <span slot="start">00:03</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Corría el año de 1978 y en el sector de las minicomputadoras había una guerra a punto de estallar. Apenas un año antes, Digital Equipment Corporation, o DEC, había lanzado su computadora VAX 11 780 de 32 bits. Tenía una capacidad mucho mayor que las máquinas de 16 bits del mercado. Las ventas de la VAX pronto arrasaron con las de la competencia, que ofrecía computadoras más lentas. A Data General, la archienemiga de DEC, le urgía diseñar una nueva máquina capaz de competir con la VAX. Necesitaba su propia computadora de 32 bits y la necesitaba ya, pero la competencia entre Data General y DEC no era el único conflicto del momento. También había una disputa territorial en el interior de Data General, y el resultado de ambas guerras sería la creación de una computadora increíble, en circunstancias igual de increíbles. Una laptop de 13 pulgadas pesa como kilo y medio. Hoy en día damos por hecho la portabilidad y la practicidad de nuestras computadoras, pero en la década de 1970 la mayoría eran mainframes del tamaño de una habitación; eran aparatos que costaban millones de dólares y pesaban varias toneladas. Luego, cuando se desplomaron los costos del hardware, comenzó la carrera para desarrollar computadoras más pequeñas, más rápidas y más baratas. La minicomputadora abrió la posibilidad de que los ingenieros y los investigadores tuvieran su propia terminal, y nos trajo a donde estamos en la actualidad.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">01:37</span> - <span slot="voice">Presentadora</span>
                <p slot="text">En la temporada pasada de Command Line Heroes en español, analizamos un área clave para el desarrollo del software: el mundo de los lenguajes de programación. Hablamos de su historia, de los problemas que resolvieron y de su evolución a través del tiempo. Abordamos lenguajes como JavaScript, Python, C, Perl, COBOL y Go. En esta temporada, que es la cuarta, por si alguien lleva la cuenta, vamos a profundizar en el hardware en que se ejecuta nuestro software. Te vamos a contar siete historias maravillosas sobre las personas y los equipos que se atrevieron a cambiar las reglas del hardware. Piensa en la laptop que está en tu escritorio, o en el teléfono que traes en el bolsillo… Los héroes de la línea de comandos siempre dejan el alma en tu hardware; con su pasión por el diseño informático y su ingenio para que cada pieza se vuelva realidad, han revolucionado la forma en que programamos hoy en día.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:36</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Esto es Command Line Heroes en español, un podcast original de Red Hat.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">02:45</span> - <span slot="voice">Presentadora</span>
                <p slot="text">El primer episodio de esta temporada cuenta la carrera contrarreloj de un equipo de ingenieros que tenían que diseñar, depurar y entregar una computadora de vanguardia. Su trabajo se convirtió en el tema principal del bestseller El alma de una nueva máquina, de Tracy Kidder, que posteriormente se haría acreedor al premio Pulitzer y que habla de muchos de los invitados de este episodio.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">03:07</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Pero volvamos a Data General. El presidente de la compañía, Ed de Castro, había trazado un plan para competir con DEC. Dividió al departamento de ingeniería y trasladó a una parte del equipo de la sede de Westboro, Massachusetts, en Estados Unidos, a una nueva oficina que estaba en Carolina del Norte. ¿Su misión? Diseñar una computadora avanzada de 32 bits que hiciera trizas a la VAX. El proyecto se llamaba Fountainhead, y de Castro le dio apoyo y recursos casi ilimitados. Fountainhead iba a ser la salvación de la empresa. Los pocos ingenieros que se quedaron en Massachusetts se sintieron terriblemente menospreciados. Sabían que eran capaces de crear una computadora que destrozara a la VAX, y que probablemente sería mejor que la de Fountainhead, pero de Castro no les daba la oportunidad. Así que Tom West, que era el líder del grupo, decidió ocuparse personalmente del asunto. El ingeniero en Computación Tom West, de formación autodidacta, dirigía el departamento Eclipse de Data General. Eclipse era la gama de minicomputadoras de 16 bits más exitosa de Data General. Tom sabía fabricar y distribuir computadoras, y también sabía lo que quería el mercado. Después de poner en marcha el proyecto Fountainhead, de Castro les pidió a los demás ingenieros que siguieran mejorando la gama de productos del año anterior. Ni a Tom ni a los demás les convencía la idea.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">04:31</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">No nos hacía ninguna gracia. Algunos decidieron cambiar de empleo y otros estábamos deprimidos y preocupados por nuestras carreras; no nos sentíamos nada entusiasmados. Y nos imaginábamos que el otro grupo no lo iba a lograr.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">04:46</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Carl Alsing era el gerente del grupo de microprogramación de Data General. Era el segundo al mando, después de Tom. Así que los dos decidieron empezar su propio proyecto.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">04:56</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Iba a ser un diseño completamente nuevo, con las técnicas más avanzadas, para diseñar una computadora de 32 bits que superara a la VAX de DEC. Preparamos nuestra propuesta, se la presentamos al presidente, Ed de Castro, que nos dice: “No, para nada. El grupo de Carolina del Norte está en eso. No se preocupen”. Nos desanimamos, pero se nos ocurrió otra propuesta a la que le pusimos Víctor. Buscamos formas de mejorar el producto del año pasado. Le pusimos un pequeño interruptor, un bit de modo en el sistema; si lo encendías permitía que la computadora funcionara como una minicomputadora moderna de 32 bits, pero lenta. Se lo llevamos a Ed de Castro y se lo presentamos. Y total que nos dijo: “Eso es un bit de modo. No quiero ni ver diseños con bits de modo. La que se encarga de los nuevos diseños es Carolina del Norte”. Entonces otra vez nos desanimamos, y creo que en fue en ese momento que Tom West decidió hacer algo a escondidas.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">06:06</span> - <span slot="voice">Presentadora</span>
                <p slot="text">A Tom se le ocurrieron dos cosas. Una de ellas era para de Castro. Iban a mejorar la antigua línea de productos Eclipse: la harían un poco más rápida, le agregarían unos cuantos botones, le cambiarían el color. Tom lo presentó como una especie de plan b, en caso de que algo saliera mal en Carolina del Norte. De Castro lo aprobó. Pero a su equipo Tom le contó otra historia, una más interesante.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">06:32</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Tom West nos propuso diseñar una computadora moderna, muy buena, que fuera totalmente compatible con las anteriores y que pudiera manejar lo último en alta tecnología. Iba a tener memoria virtual, 32 bits, códigos de corrección de errores y esas cosas. Multitareas, multiprocesamiento, mucha memoria. “Oigan, vamos a diseñar una computadora nueva que se va a comer vivo al mercado”.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">07:04</span> - <span slot="voice">Presentadora</span>
                <p slot="text">El código de esta maravilla informática: la “Eagle”. Actualmente parece que no hay límites con lo que podemos hacer con la memoria integrada de nuestra computadora, pero en ese entonces, pasar de 16 a 32 bits era un paso enorme. De un día para otro, el espacio de direcciones había pasado de 65 mil bytes de información a más de 4 mil millones. Y con ese aumento, el software podía procesar mayores cantidades de datos. Esto generó dos grandes desafíos para las empresas de informática: obviamente había que pasar de 16 a 32 bits, pero además había que dejar conformes a los antiguos clientes, que todavía utilizaban el software anterior. Así que había que desarrollar una computadora que pudiera funcionar con el viejo software; una computadora de 32 bits que fuera compatible con lo anterior. La VAX tenía una gran potencia, pero no tenía ninguna solución elegante para el segundo problema. Tom estaba decidido a que su Eagle fuera la respuesta.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">08:14</span> - <span slot="voice">Presentadora</span>
                <p slot="text">La Eagle estaba escondida en el sótano del edificio Westborough, 14 AB. Tom le pidió a Carl que dirigiera la microcodificación. Carl nombró a Chuck Holland como gerente de los programadores, que se pusieron el nombre de Micro Kids. Mientras, Ed Rasala supervisaría el hardware. Y Ed designó a Ken Holberger para que dirigiera al equipo, al que llamaron, muy apropiadamente, los Hardy Boys. Tom encontró un aliado: el vicepresidente de Ingeniería, Carl Carman. Carman también tenía cuentas pendientes con De Castro, que se había negado a ponerlo a cargo del grupo de Carolina del Norte.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">08:51</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Carl Carman sabía en qué andábamos, pero no le dijo nada a su jefe. Él era el que nos financiaba; el problema es que necesitábamos ingenieros muy, pero muy buenos, pero teníamos que mantener bajos los salarios. Así que decidimos contratar estudiantes universitarios. Una de las ventajas es que no conocen tus límites. Creen que puedes hacer cualquier cosa.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">09:15</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Jim Guyer había egresado de la universidad dos años antes y trabajaba en Data General cuando lo pusieron a cargo de los Hardy Boys.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">09:21</span> - <span slot="voice">Jim Guyer</span>
                <p slot="text">La computadora que estaban desarrollando en Carolina del Norte tenía una tecnología informática mucho más avanzada, casi como una mainframe. Y bueno, digamos que en esa época no era cualquier cosa ponerse a competir con IBM y las demás empresas de mainframes. Creíamos que teníamos ventaja porque nuestro proyecto no era tan ambicioso y estábamos muy, muy concentrados en una implementación clara, sencilla y elegante, de bajo costo, pocos componentes... cosas así.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">09:51</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Bajo costo, diseño sencillo... Eso los hizo entender que tendrían que usar el firmware para controlar todo. Mientras más funciones lograran implementar en el firmware en vez del hardware, más barato y flexible sería el resultado</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">10:03</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Además, podrían hacer los cambios a medida que se necesitaran. Actualmente nos parece lógico porque así funcionan las computadoras, pero en 1978 era algo completamente nuevo. </p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">10:15</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">El diseño que estábamos haciendo era algo básico. Lo que queríamos era encontrar formas sencillas y directas de hacer las cosas, sin complicaciones, porque sabíamos que no podíamos terminar diseñando una computadora grande y cara. Necesitábamos usar pocas tarjetas, pocos circuitos, y de hecho eso nos ayudaba para que fuera rápida. No es lo mismo diseñar un producto seguro y sin riesgos, que diseñar un producto exitoso. Y no nos importaban los riesgos. Nos importaba el éxito. Queríamos que nuestra computadora fuera rápida y barata, y queríamos diseñarla rápido. Así que le pusimos unas tres o cuatro tarjetas, lo mínimo que podíamos de hardware, y lo compensamos con el firmware.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">11:06</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Pero el equipo de la Eagle se enfrentaba a varios obstáculos difíciles de superar. La VAX era la computadora de 32 bits con mejor rendimiento del mundo. La Eagle necesitaba estar a la altura. Pero además, tenía que ser compatible con la arquitectura anterior de 16 bits de Data General. Para lograr todo eso, pero con menos tiempo y dinero que los demás equipos, había que apostarle mucho a la Eagle. Pero el equipo de Tom West estaba dispuesto a jugárselo todo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">11:32</span> - <span slot="voice">Jim Guyer</span>
                <p slot="text">Había dos sistemas que funcionaban las 24 horas del día, los 7 días de la semana, y teníamos dos turnos de ingenieros que trabajaban en eso. Todos necesitábamos entender cómo funcionaba todo. Así que tuvimos que aprender qué hacía cada una de las piezas que construían los demás. Me costó mucho trabajo, pero al mismo tiempo aprendí muchísimo. Todos participábamos en el trabajo de los demás y pensábamos: “¿Cuál es el siguiente paso para resolver este problema? ¿En qué hay que fijarse?” Todos revisábamos los diagramas de circuitos y demás documentos para tratar de entender: “A ver, fíjate en esta señal, ve el estado de la computadora, revisa la secuencia de pasos del microcódigo. ¿Sí está haciendo lo que tiene que hacer? Uy, espérense, va para el otro lado. Ay, ¿pero por qué hizo eso?”</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">12:13</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Lo tomábamos muy en serio, era parte de la ética de trabajo. El ambiente era intenso. A veces había discusiones sobre la manera de hacer las cosas. Por ejemplo, tal vez había una forma un poco más cara y otra que era más barata pero no tan rápida o eficaz. Y había discusiones acaloradas y reuniones en las que teníamos que esforzarnos por llegar a un acuerdo. Pero al final lográbamos tomar una decisión. Y empezábamos a trabajar juntos.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">12:44</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Trabajábamos día y noche, nos repartíamos las horas que se necesitaban para diseñar el prototipo. Solo teníamos dos prototipos, y era muy importante que los dos equipos trabajaran en ellos. Algunos trabajaban en la noche, otros trabajaban en el día, y ya empezábamos a cansarnos. Pero estábamos muy motivados, así que sentíamos mucha satisfacción. Así que nadie se quejaba mucho de las condiciones laborales.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">13:11</span> - <span slot="voice">Presentadora</span>
                <p>Las condiciones laborales. Algunos relatos de esa época dicen que, para que el equipo funcionara, Tom West puso en práctica una cosa que se llama “la gestión de los hongos”: si les das de comer cualquier porquería y los mantienes en la oscuridad vas a verlos crecer. Estaban encerrados en un espacio de trabajo abarrotado y caluroso, así que las horas se hacían largas y los plazos eran poco realistas. Dicen que Tom era enigmático, frío, indiferente. Uno de los ingenieros incluso lo llamaba el “Príncipe de las Tinieblas”. ¿Pero a Tom West le importaba tanto lograr el éxito que se aprovechó de su equipo? ¿Sacrificó el bienestar de los Micro Kids y los Hardy Boys para diseñar la computadora perfecta?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">13:56</span> - <span slot="voice">Jim Guyer</span>
                <p slot="text">Era interesante trabajar con Tom. Porque tenía muchas expectativas, pero no te daba suficientes instrucciones. Esperaba que entendieras lo que tenías que hacer, y si no, pues qué pena, te sacaba del equipo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">14:10</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Los que daban instrucciones eran Carl y Ed, los gerentes de línea que trabajaban codo a codo con Jim y el resto del equipo. Pero estos jóvenes ingenieros también buscaban el éxito, y les gustaba tener la oportunidad de resolver las cosas ellos mismos.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">14:26</span> - <span slot="voice">Jim Guyer</span>
                <p slot="text">Yo me gané el primer lugar de los Micro Kids por aguantar toda la noche sin dormir. Quién sabe, a lo mejor éramos jóvenes, empezábamos nuestra vida profesional, éramos bravucones, muy seguros, y no entendíamos nada de nada. Confiábamos en nosotros mismos. Nos sentíamos muy inteligentes, creíamos que podíamos resolver todo, y yo supongo que el ego de los demás también nos... nos alimentaba, en cierto sentido. Yo me la pasaba muy bien. Yo creo que la mayoría de nosotros nos divertíamos mucho.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">14:56</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Carl no está de acuerdo con lo de la gestión de los hongos. En su opinión no estaban en la oscuridad, sino al contrario: todos sabían exactamente lo que estaba pasando y lo que se esperaba. Los directores eran los que no sabían. Al mismo tiempo, Tom West estaba bajo una enorme presión de varios frentes, y se la transmitía al grupo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">15:18</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Tom mantenía en secreto la verdadera finalidad del proyecto. Así que no hablaba mucho con los ingenieros, se mantenía a distancia y obviamente les decía que no hablaran del proyecto fuera del grupo, ni siquiera en su casa. Les decía que ni mencionaran la palabra Eagle. Así que también dejábamos muy claro que esto era muy urgente, que teníamos que lograrlo en un año, que la competencia ya estaba en el mercado, y si queríamos salir al mercado en medio del pico de ventas, teníamos que lograrlo ya. Estaban muy estresados, y se esperaba que trabajaran en la noche y los fines de semana; se esperaba que olvidaran los picnics con la familia; no había tiempo para nada que no fuera del trabajo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">16:06</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Como me daba curiosidad saber cómo era trabajar en las trincheras del Edificio 14 AB, me senté a conversar con Neal Firth, que era uno de los Micro Kids. Acababa de salir de la universidad cuando se incorporó al equipo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">16:20</span> - <span slot="voice">Presentadora</span>
                <p slot="text">¿Cómo era trabajar para Tom West? ¿Te comunicabas mucho con él?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">16:24</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">No tanto. Era como un fantasma. A veces lo veíamos por ahí. Intentaba no interferir para que hiciéramos lo que necesitábamos y alcanzáramos los objetivos. El proyecto era algo completamente nuevo en comparación con lo que hacía Data General, y Tom no quería imponernos nada forzoso respecto a la generación anterior de procesadores.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">16:49</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Suena intenso, suena a que había que trabajar sin descanso y a que siempre había algo que resolver. ¿Cómo te sentías de que no tuvieran el tiempo necesario para lograrlo?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">16:57</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">Sinceramente no nos preocupaba. En realidad la falta de tiempo no era problema. Nosotros nos tomábamos el tiempo que hiciera falta para lograr el resultado. Por eso necesitábamos que nuestras esposas nos apoyaran y fueran comprensivas, porque no siempre aceptaban todo. Era más o menos lo que sucedía con las personas de Silicon Valley de esa época, o con Jobs y Wozniak: “vamos a ponernos a hacer esto hasta terminarlo”. No vivíamos en el mismo departamento ni nos sentábamos en el piso a escribir código, pero teníamos mucho en común con ellos.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">17:35</span> - <span slot="voice">Presentadora</span>
                <p slot="text">¿Y qué te impulsaba a seguir adelante? ¿Por qué estabas tan motivado?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">17:39</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">La verdad solo era la posibilidad de resolver algún problema. Siempre me habían gustado los acertijos, los problemas que necesitaban solución. De hecho, así éramos casi todos. Todos compartíamos eso, y todos lo disfrutábamos. Nos motivaba resolver los problemas, solucionar esas cosas, descubrir una manera nueva de hacer algo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">18:01</span> - <span slot="voice">Presentadora</span>
                <p slot="text">¿Y cuál fue el momento del proyecto que no vas a poder olvidar?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">18:05</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">Fue... Ya llevábamos mucho tiempo con el proyecto, y estábamos ejecutando el simulador de microcódigo. Y resulta que lo que se estaba ejecutando era la propuesta del simulador de producción, que ya llevaba como 10 o 12 horas funcionando. Y de repente aparece la letra E en la consola… Nos esperamos un rato y de pronto aparece otra letra, y luego otra. Y entonces nos dimos cuenta de que lo que estábamos ejecutando como código de prueba era el diagnóstico que estábamos diseñando para que se ejecutara. Así que el simulador ejecutaba el microcódigo, y ya había empezado a imprimir letras como si realmente estuviera funcionando. Era mil veces más lento que en la vida real, o sea, era más lento que cuando se lanzó realmente, pero ese fue uno de los momentos que nunca voy a olvidar.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">19:02</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Y ahora que lo piensas, ¿te parece que te explotaron?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">19:07</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">No. O sea, yo sabía. Yo sabía lo que estaba pasando. Entonces… no. No me siento explotado. En realidad, mis expectativas... yo nunca hubiera esperado participar en un proyecto tan importante justo al salir de la universidad, ni tener la oportunidad de desempeñar un papel tan interesante en un proyecto así.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">19:31</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Me gustaría saber tu opinión sobre el sacrificio que requiere el inventar algo, porque cuando hacemos algo importante, en general hay que renunciar a algo para lograrlo, ¿no? Para lograr algo, hay que renunciar a algo, ¿no?¿Ese fue el caso? Y si sí, ¿a qué tuviste que renunciar?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">19:48</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">Yo no creo que haya tenido la conciencia de que iba a renunciar a algo. Más bien creo que lo que pasó fue que empecé a ser un poco más consciente de lo que estaba haciendo, y de que eso afectaba a los que me rodeaban.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">20:03</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">Pero para mí no... no era un sacrificio, y las personas que me rodeaban lo vivían como algo normal; así son las cosas y punto. A mí me han contado cosas horribles de lo que se vive hoy: amanece, te despiertas, te inyectas café, muerdes un pedazo de pizza o cualquier cosa... y empiezas a escribir código hasta que te quedas dormido encima del teclado. Y al día siguiente, igual.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">20:35</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">Nosotros no hacíamos tantos sacrificios. Digo, yo seguía casado, tenía amigos, los veía... Sí, no era un trabajo de nueve a cinco, pero me permitió obtener muchos logros personales y técnicos, y pude compartirlos con mi esposa, mi hermana, mi mamá, mi papá y mi suegro. O sea, mi familia lo apreciaba.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">20:59</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Sí. ¿Y cuál es el secreto para lograr algo maravilloso?</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">21:06</span> - <span slot="voice">Neal Firth</span>
                <p slot="text">¿Para lograr algo maravilloso? Qué interesante. Creo que la cosa es que quien participe lo haga porque quiere, no porque busca logros, fama o dinero. Porque esas son cosas muy fugaces y... casi nunca te dejan satisfecho. Pero si la idea es alcanzar un objetivo, y colaboras con muchas personas y lo logras, ahí vas a saber lo que es la satisfacción.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">21:42</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Neal Firth era uno de los Micro Kids del proyecto Eagle. Hoy en día es el presidente de VIZIM Worldwide, que es una empresa de software.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">21:57</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Como bien dice el libro de Tracy Kidder, la indiferencia y el distanciamiento de Tom West eran a propósito. Era un intento de mantener la cabeza despejada, por encima de toda la cháchara diaria, para conservar intacto el objetivo de la Eagle. Pero lo que más quería era proteger al equipo, aislarlo de la política y de los estira y afloja corporativos de su entorno. También protegió a los Micro Kids y a los Hardy Boys de las ideas preconcebidas de lo que se podía lograr.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">22:28</span> - <span slot="voice">Presentadora</span>
                <p slot="text">En 1980 se terminó el proyecto Eagle. Un año después de lo que Tom había prometido, pero se logró, a diferencia de Fountainhead. Y tal como pensaba el equipo sénior, el objetivo de Fountainhead no se logró, y el proyecto se quedó olvidado en algún cajón. Bill Foster, que en ese entonces era director de desarrollo de software, nos cuenta las dificultades de Fountainhead.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">22:50</span> - <span slot="voice">Bill Foster</span>
                <p slot="text">Creo que el mayor error fue que no se les puso ningún límite. Había que hacer la mejor computadora del mundo. “¿Pero para cuándo?” “Pues... la verdad no tenemos fecha.”. “¿Y cuánto debe costar?” “Eh… tampoco sabemos”. Y yo le atribuyo el fracaso a Edson. No les puso suficientes límites a los programadores ni a los ingenieros.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">23:15</span> - <span slot="voice">Bill Foster</span>
                <p slot="text">¿Y sabes qué pasa cuando no les pones límites? Diseñan algo tan amplio y complejo que simplemente no se puede concretar.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">23:26</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Pero a ver, vamos a hacer memoria. Tom y su equipo decidieron diseñar la Eagle a escondidas, y es lo que hicieron durante dos años. Y el presidente de la empresa nunca supo lo que estaba pasando. La computadora ahora se llamaba oficialmente Eclipse MV/8000, y cuando ya estaba lista para salir al mercado, el jefe de marketing fue a ver a Ed de Castro para que aprobara la campaña de publicidad. Vamos a escuchar a Carl Alsing.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">23:53</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">El jefe de marketing nos dijo: “Bueno, pues ya estamos listos para lanzar la Eagle, y vamos a necesitar varios miles de dólares. Vamos a hacer una conferencia de prensa en seis ciudades del mundo. Y después vamos a hacer una gira para visitar muchas ciudades, vamos a filmar una película y a mostrarla, y vamos a ser la sensación”.
                </p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">24:14</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Pero Ed de Castro contestó: “No entiendo. ¿Para qué quieren hacer eso?” Va a ser un peso más para la Eclipse. Es como hacerle cirugía estética: promocionarla por encimita. Pero el gerente de marketing le respondió: “No, es una computadora completamente nueva. Es una computadora de 32 bits. Tiene memoria virtual. Es compatible. Va a arrasar con la VAX. Tiene todo”.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">24:37</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Ed de Castro no entendía nada. Pensaba que nos habíamos equivocado en Carolina del Norte, y que ese era el fin de la empresa, pero en realidad le habíamos salvado el pellejo. Un día nos invitó a todos a almorzar. Había sándwiches y gaseosas, y de repente nos dice: “Bueno, pues felicidades por el trabajo que hicieron, estoy sorprendido. Yo no sabía que estaban con ese proyecto, pero vamos a lanzarlo, y tengo entendido que va a haber una película y varias giras, y ustedes van a participar en eso, así que gracias y buen provecho con los sándwiches”.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">25:19</span> - <span slot="voice">Presentadora</span>
                <p slot="text">La Eagle, que ahora se llamaba MV/8000, apareció en la portada de la revista Computer World. El lanzamiento con bombos y platillos en los medios de comunicación les dio cierta fama a aquellos empleados, que hasta entonces se habían escondido en el sótano. Habían salvado a Data General.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">25:38</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Pero todo lo bueno dura poco. Tom West ya no podía seguir protegiendo al grupo de la política interna de la empresa. Y el equipo no estaba preparado para los resentimientos que surgieron. En la compañía había gente que envidiaba sus logros y no podía creer que se hubieran salido con la suya durante tanto tiempo con un proyecto secreto.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">25:57</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Pronto, el nuevo vicepresidente de Ingeniería reemplazó a Carl Carman, que era el aliado del grupo. El recién llegado desarmó el grupo de Eagle y envió a Tom a la oficina de Data General de Japón antes de que se vendiera la primera MV/8000.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">26:13</span> - <span slot="voice">Jim Guyer</span>
                <p slot="text">Yo creía que habíamos hecho la mejor superminicomputadora de 32 bits que el dinero podía comprar, lo cual era excelente para Data General, y que durante un tiempo destronaríamos a Digital Equipment Corporation, no que ya habíamos acabado con ellos. La competencia era salvaje en esos tiempos, y no es fácil tener éxito en el sector de la alta tecnología, pero yo pensaba que lo que habíamos hecho valía la pena.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">26:42</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Sin duda, el lanzamiento de la Eagle salvó a Data General, pero habían perdido participación en el mercado frente a DEC durante tres años, así que la empresa nunca se recuperó realmente y la industria había seguido avanzando. Las minicomputadoras ya no eran lo más importante. La carrera de las microcomputadoras ya había comenzado, y le abrió camino a la revolución de las computadoras personales.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">27:04</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Data General siguió adelante, sacó nuevas versiones, las mejoró en los siguientes modelos y las vendió durante un tiempo, así que disfrutó de cierto éxito. Pero, bueno, las cosas cambian. El mercado cambió y... ellos se convirtieron en una empresa de software, y finalmente otra empresa los compró. Y ahora creo que lo único que queda de ellos es algún archivador en alguna empresa de Hopkinton, Massachusetts.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">27:36</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Un año después, muchos de los integrantes del grupo de la Eagle habían dejado Data General. Algunos estaban agotados. Otros ya querían diseñar alguna otra cosa. Otros se fueron al oeste, hacia Silicon Valley, y estaban ansiosos por encontrar la siguiente chispa creativa. Cualquiera que fuera el caso, no tenía mucho sentido quedarse en una empresa que no reconocía todo lo que habían hecho para salvarla. En ese mismo año, en 1981, se publicó El alma de una nueva máquina, de Tracy Kidder. Ahora el mundo sabría cómo se había diseñado la Eagle.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">28:14</span> - <span slot="voice">Carl Alsing</span>
                <p slot="text">Si me preguntas qué constituye el alma de una nueva máquina, yo diría que las personas y lo que les pasa a esas personas; los sacrificios que hacen, el esfuerzo y el entusiasmo que sienten, y las satisfacciones que esperan obtener. Tal vez lo logren, tal vez no, pero tienen un objetivo y luchan por él.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">28:35</span> - <span slot="voice">Jim Guyer</span>
                <p slot="text">En realidad, la computadora era un personaje secundario. El corazón del proyecto era la gente.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">28:47</span> - <span slot="voice">Presentadora</span>
                <p slot="text">En el próximo episodio de nuestra nueva temporada sobre el hardware, vamos a retroceder en el tiempo hasta la era de las computadoras mainframe, y te contaremos la historia de otro grupo de empleados rebeldes. La computadora que construyeron hizo surgir un lenguaje de programación que cambió el mundo.</p> 
            </rh-audio-player-cue>
            <rh-audio-player-cue>
                <span slot="start">29:04</span> - <span slot="voice">Presentadora</span>
                <p slot="text">Command Line Heroes en español es un podcast original de Red Hat. Para esta temporada, recopilamos excelentes materiales de investigación para que puedas saber más sobre la historia del hardware del que estamos hablando. Si quieres saber más sobre la Eagle y el equipo que la diseñó, visita redhat.com/commandlineheroes. Hasta la próxima, sigan programando.</p>
            </rh-audio-player-cue>
        </rh-audio-player-transcript>
    </rh-audio-player>
</div>
<div>
    <h3>Multiple players on a page</h3>
    <p>Pressing play on any rh-audio-player component will pause any the currently the currently playing rh-audio-player component.</p>
    <rh-audio-player mode="mini">
        <p slot="series">Code Comments</p>
        <h4 slot="title">Bringing Deep Learning to Enterprise Applications</h4>
        <audio crossorigin="anonymous" slot="media" controls>
            <source type="audio/mp3" srclang="en" src="https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3">
        </audio>
    </rh-audio-player>
    <rh-audio-player mode="mini">
        <p slot="series">Code Comments</p>
        <h4 slot="title">Rethinking Networks In Telecommunications</h4>
        <audio crossorigin="anonymous" slot="media" controls>
            <source type="audio/mp3" srclang="en" src="https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/32d79061-21f8-40a1-9ef8-d424e82a8326/audio/0a65ec45-a21a-4c31-94a6-61701a145b1d/default_tc.mp3">
        </audio>
    </rh-audio-player>
</div>
