WEBVTT

00:02.000 --> 00:03.000
<v Burr Sutter>I'm Burr Sutter.</v>

00:03.000 --> 00:08.000
<v Burr Sutter>I'm a Red Hatter who spends a lot of time talking to technologists about technologies.</v>

00:08.000 --> 00:10.000
<v Burr Sutter>We say this a lot at Red Hat.</v>

00:10.000 --> 00:14.000
<v Burr Sutter>No single technology provider holds the key to success, including us.</v>

00:14.000 --> 00:17.000
<v Burr Sutter>And I would say the same thing about myself.</v>

00:17.000 --> 00:22.000
<v Burr Sutter>I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners.</v>

00:22.000 --> 00:29.000
<v Burr Sutter>This is Code Comments, an original podcast from Red Hat.

00:29.000 --> 00:36.000
<v Burr Sutter>I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning.</v>

00:36.000 --> 00:41.000
<v Burr Sutter>I've been thinking about that for quite some time and I actually had the opportunity to work on a few successful projects, here at Red Hat, 

00:41.000 --> 00:45.000
<v Burr Sutter>using those technologies, actually enabling a data set,</v>

00:45.000 --> 00:49.000
<v Burr Sutter>gathering a data set, working with a data scientist and data engineering team,</v>

00:49.000 --> 00:45.000
<v Burr Sutter>and then training a model and putting that model into production runtime environment.</v>

00:52.000 --> 00:58.000
<v Burr Sutter>It was an exciting set of projects and you can see those on numerous YouTube videos that have published out there before.</v>

00:58.000 --> 01:04.000
<v Burr Sutter>But I want you to think about the problem space a little bit,</v>

00:58.000 --> 01:04.000
<v Burr Sutter>because there are some interesting challenges about a AI/ML.</v>

01:04.000 --> 01:05.000
<v Burr Sutter>One is simply just getting access to the data,</v>

01:05.000 --> 01:08.000
<v Burr Sutter>and while there are numerous publicly available data sets,</v>

01:08.000 --> 01:14.000
<v Burr Sutter>when it comes to your specific enterprise use case, you might not be to find publicly available data.</v></v>

01:14.000 --> 01:17.000
<v Burr Sutter>In many cases you cannot, even for our applications that we created,</v>

01:17.000 --> 01:22.000
<v Burr Sutter>we had to create our data set, capture our data set, explore the data set,</v>

01:22.000 --> 01:24.000
<v Burr Sutter>and of course, train a model accordingly.</v>

01:24.000 --> 01:28.000
<v Burr Sutter>And we also found there's another challenge to be overcome in this a AI/ML world,</v>

01:28.000 --> 01:31.000
<v Burr Sutter>and that is access to certain types of hardware.</v>

01:31.000 --> 01:36.000
<v Burr Sutter>If you think about an enterprise environment and the creation of an enterprise application specifically for a AI/ML,</v>

01:36.000 --> 01:39.000
<v Burr Sutter>end users need an inference engine to run on their hardware.</v>

01:39.000 --> 01:43.000
<v Burr Sutter>Hardware that's available to them, to be effective for their application.</v>

01:43.000 --> 01:49.000
<v Burr Sutter>Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor.</v>

01:49.000 --> 01:56.000
<v Burr Sutter>As those things are whizzing by on the factory line there, looking at them and trying to determine if there is an error or not.</v>

01:56.000 --> 2:01.000
<v Burr Sutter>Well, how do you actually make it run on your hardware, your accessible technology that you have today.</v>

02:01.000 --> 02:05.000
<v Burr Sutter>Well, there's a solution for this as an open toolkit called OpenVINO.</v>

02:05.000 --> 02:11.000
<v Burr Sutter>And you might be thinking, "Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning?"</v>

02:11.000 --> 02:11.000
<v Burr Sutter>Well, not according to Ryan Loney, product manager of OpenVINO Developer Tools at Intel.</v>

2:20.000 --> 02:22.000
<v Ryan Loney>I guess I'll start with trying to maybe dispel a myth.</v>

2:22.000 --> 02:26.000
<v Ryan Loney>I think that CPUs are widely used for inference today.</v>

2:26.000 --> 02:36.000
<v Ryan Loney>So if we look at the data center segment, about 70% of the AI inference is happening on Intel Xeon, on our data center CPUs.</v>

2:36.000 --> 02:43.000
<v Ryan Loney>And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO,</v>

2:43.000 --> 02:50.000
<v Ryan Loney>is that we're taking models that may have been trained on a GPU using deep learning frameworks like PyTorch or TensorFlow,</v>

2:50.000 --> 02:57.000
<v Ryan Loney>and then optimizing them to run on Intel hardware.