export const configure = project => project.config = {
  "files": {
    "demo/rhds-demo-base.css": {
      "contentType": "text/css",
      "content": "html,\nbody {\n  margin: 0;\n}\n\nhtml {\n  font-family: var(--rh-font-family-body-text, RedHatText, \"Red Hat Text\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Overpass, Helvetica, Arial, sans-serif, \"Overpass\", Overpass, Helvetica, Arial, sans-serif);\n  line-height: var(--rh-line-height-body-text, 1.5);\n  font-size: 16px;\n}\n\n*,\n*:before,\n*:after {\n  box-sizing: border-box;\n}\n\nh1,\nh2,\nh3,\nh4,\nh5,\nh6 {\n  font-weight: var(--rh-font-weight-heading-medium, 500);\n  font-family: var(--rh-font-family-heading, RedHatDisplay, \"Red Hat Display\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Overpass, Helvetica, Arial, sans-serif, \"Overpass\", Overpass, Helvetica, Arial, sans-serif);\n}\n",
      "hidden": true
    },
    "demo/index.html": {
      "contentType": "text/html",
      "selected": true,
      "content": "<rh-audio-player id=\"player\" layout=\"full\" poster=\"https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png\">\n  <p slot=\"series\">Code Comments</p>\n  <h3 slot=\"title\">Bringing Deep Learning to Enterprise Applications</h3>\n  <rh-audio-player-about slot=\"about\">\n    <h4 slot=\"heading\">About the episode</h4>\n    <p>\n      There are a lot of publicly available data sets out there. But when it\n      comes to specific enterprise use cases, you're not necessarily going to\n      able to find one to train your models. To realize the power of AI/ML in\n      enterprise environments, end users need an inference engine to run on\n      their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open\n      toolkits from Intel that do precisely that. He looks specifically at\n      anomaly detection in use cases as varied as medical imaging and\n      manufacturing.\n    </p>\n    <p>\n      Want to learn more about Anomalib? Check out the research paper that\n      introduces the deep learning library.\n    </p>\n    <rh-avatar slot=\"profile\" src=\"https://www.redhat.com/cms/managed-files/ryan-loney.png\">\n      Ryan Loney\n      <span slot=\"subtitle\">Product manager, OpenVINO Developer Tools, <em>IntelÂ®</em></span>\n    </rh-avatar>\n  </rh-audio-player-about>\n  <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n    <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3\">\n  </audio>\n  <rh-audio-player-subscribe slot=\"subscribe\">\n    <h4 slot=\"heading\">Subscribe</h4>\n    <p>Subscribe here:</p>\n    <a slot=\"link\" href=\"https://podcasts.apple.com/us/podcast/code-comments/id1649848507\" target=\"_blank\" title=\"Listen on Apple Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Apple Podcasts\" data-analytics-category=\"Hero|Listen on Apple Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg\" alt=\"Listen on Apple Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD\" target=\"_blank\" title=\"Listen on Spotify\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Spotify\" data-analytics-category=\"Hero|Listen on Spotify\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_spotify.svg\" alt=\"Listen on Spotify\">\n    </a>\n    <a slot=\"link\" href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw\" target=\"_blank\" title=\"Listen on Google Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Google Podcasts\" data-analytics-category=\"Hero|Listen on Google Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_google-podcast.svg\" alt=\"Listen on Google Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://feeds.pacific-content.com/codecomments\" target=\"_blank\" title=\"Subscribe via RSS Feed\" data-analytics-linktype=\"cta\" data-analytics-text=\"Subscribe via RSS Feed\" data-analytics-category=\"Hero|Subscribe via RSS Feed\">\n      <img class=\"img-fluid\" src=\"https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg\" alt=\"Subscribe via RSS Feed\">\n    </a>\n  </rh-audio-player-subscribe>\n  <rh-transcript id=\"regular\" slot=\"transcript\">\n    <h4 slot=\"heading\">Transcript</h4>\n    <rh-cue start=\"00:02\" voice=\"Burr Sutter\">\n      Hi, I'm Burr Sutter. I'm a Red Hatter who spends a lot of time talking to technologists about technologies. We say this a lot at Red Hat. No single technology provider holds the key to\n      success, including us. And I would say the same thing about myself. I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners. This is\n      Code Comments, an original podcast from Red Hat.\n    </rh-cue>\n    <rh-cue start=\"00:29\" voice=\"Burr Sutter\">\n      I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning. I've been thinking about that for quite some time and I actually had the\n      opportunity to work on a few successful projects, here at Red Hat, using those technologies, actually enabling a data set, gathering a data set, working with a data scientist and data\n      engineering team, and then training a model and putting that model into production runtime environment. It was an exciting set of projects and you can see those on numerous YouTube videos that\n      have published out there before. But I want you to think about the problem space a little bit, because there are some interesting challenges about a AI/ML. One is simply just getting access to\n      the data, and while there are numerous publicly available data sets, when it comes to your specific enterprise use case, you might not be to find publicly available data.\n    </rh-cue>\n    <rh-cue start=\"01:14\" voice=\"Burr Sutter\">\n      In many cases you cannot, even for our applications that we created, we had to create our data set, capture our data set, explore the data set, and of course, train a model accordingly. And\n      we also found there's another challenge to be overcome in this a AI/ML world, and that is access to certain types of hardware. If you think about an enterprise environment and the creation of\n      an enterprise application specifically for a AI/ML, end users need an inference engine to run on their hardware. Hardware that's available to them, to be effective for their application. Let's\n      say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor. As those things are whizzing by on the factory line there, looking at\n      them and trying to determine if there is an error or not.\n    </rh-cue>\n    <rh-cue start=\"01:56\" voice=\"Burr Sutter\">\n      Well, how do you actually make it run on your hardware, your accessible technology that you have today? Well, there's a solution for this as an open toolkit called OpenVINO. And you might be\n      thinking, \"Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning? Well, not according to Ryan Loney, product manager of OpenVINO\n      Developer Tools at Intel.\n    </rh-cue>\n    <rh-cue start=\"02:20\" voice=\"Ryan Loney\">\n      I guess I'll start with trying to maybe dispel a myth. I think that CPUs are widely used for inference today. So if we look at the data center segment, about 70% of the AI inference is\n      happening on Intel Xeon, on our data center CPUs. And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO, is that we're taking models that may\n      have been trained on a GPU using deep learning frameworks like PyTorch or TensorFlow, and then optimizing them to run on Intel hardware.\n    </rh-cue>\n    <rh-cue start=\"02:57\" voice=\"Burr Sutter\">\n      Ryan joined me to discuss AI/ML in the enterprise across various industries and exploring numerous use cases. Let's talk a little bit about the origin story behind OpenVINO. Tell us more\n      about it and how it came to be and why it came out of Intel.\n    </rh-cue>\n    <rh-cue start=\"03:12\" voice=\"Ryan Loney\">\n      Definitely. We had the first release of OpenVINO, was back in 2018, so still relatively new. And at that time, we were focused on Computer Vision and pretty tightly coupled with OpenCV, which\n      is another open source library with origins at Intel. It had its first release back in 1999, so it's been around a little bit longer. And many of the software engineers and architects at Intel\n      that were involved with and contributing to OpenCV are working on OpenVINO. So you can think of OpenVINO as complimentary software to OpenCV and we're providing an engine for executing\n      inferences as part of a Computer Vision pipeline, or at least that's how we started.\n    </rh-cue>\n    <rh-cue start=\"03:58\" voice=\"Ryan Loney\">\n      But since 2018, we've started to move beyond just Computer Vision inference. So when I say Computer Vision inference, I mean image classification, object detection, segmentation, and now\n      we're moving into natural language processing. Things like speech synthesis, speech recognition, knowledge graphs, time series forecasting and other use cases that don't involve Computer\n      Vision and don't involve inference on pixels. Our latest release, the 2022.1 that came out earlier this year, that was the most significant update that we've had to OpenVINO, since we started\n      in 2018. And the major focus of that release was optimizing for use cases that go beyond Computer Vision.\n    </rh-cue>\n    <rh-cue start=\"04:41\" voice=\"Burr Sutter\">\n      And I like that concept that you just mentioned right there, Computer Vision, and you said that you extended those use cases and went beyond that. Could you give us some more concrete\n      examples of Computer Vision?\n    </rh-cue>\n    <rh-cue start=\"04:50\" voice=\"Ryan Loney\">\n      Sure. When you think about manufacturing, quality control in factories, everything from arc welding, defect detection to inspecting BMW cars on assembly lines, they're using cameras or\n      sensors to collect data and usually it's cameras collecting images like RGB images that you and I can see and looks like something taken from a camera or video camera. But also, things like\n      infrared or computerized tomography scans used in healthcare, X-ray, different types of images where we can draw bounding boxes around regions of interest and say, \"This is a defect,\" or,\n      \"This is not a defect.\" And also, \"Is this worker wearing a safety hat or did they forget to put it on?\" And so, you can take this and integrate it into a pipeline where you're triggering an\n      alert if somebody forgets to wear their safety mask, or if there's a defect in a product on an assembly line, you can just use cameras and OpenVINO and OpenCV running these on Intel hardware\n      and help to analyze.\n    </rh-cue>\n    <rh-cue start=\"05:58\" voice=\"Ryan Loney\">\n      And that's what a lot of the partners that we work with are doing, so these independent software vendors. And there's other use cases for things like retail. You think about going to a store\n      and using an automated checkout system. Sometimes people use those automated checkouts and they slide a few extra items into their bag that they don't scan and it's a huge loss for the retail\n      outlets that are providing this way to check out realtime shelf monitoring. We have a Vispera, one of our ISVs that helps keep store shelves stocked by just analyzing the cameras in the\n      stores, detecting when objects are missing from the shelves so that they can be restocked. We have Vistry, another ISV that works with quick service restaurants. When you think about\n      automating the process of, when do I drop the fries into the fryer so that they're warm when the car gets to the drive through window, there's quite a bit of industrial healthcare retail\n      examples that we can walk through.\n    </rh-cue>\n    <rh-cue start=\"06:55\" voice=\"Burr Sutter\">\n      And we should dig into some more of those, but I got to tell you, I have a personal experience in this category that I want to share with and you can tell me how silly you might think at this\n      point in time it is. We actually built a keynote demonstration for the Red Hat big stage back in 2015. And I really want to illustrate the concept of asset tracking. So we actually gave\n      everybody in the conference a little Bluetooth token with a little battery, a little watch battery, and a little Bluetooth emitter. And we basically tracked those things around the conference.\n      We basically put a raspberry pi in each of the meeting rooms and up in the lunch room and you could see how the tokens moved from room to room to room.\n    </rh-cue>\n    <rh-cue start=\"07:28\" voice=\"Burr Sutter\">\n      It was a relatively simple application, but it occurred to me, after we figured out how to do that with Bluetooth and triangulating Bluetooth signals by looking at relative signal strength\n      from one radio to another and putting that through an Apache Spark application at the time, we then realized, \"You know what? This is easier done with cameras.\" And just simply looking at a\n      camera and having some form of a AI/ML model, a machine learning model, that would say, \"There are people here now,\" or, \"There are no people here now.\" What do you think about that?\n    </rh-cue>\n    <rh-cue start=\"07:56\" voice=\"Ryan Loney\">\n      What you just described is exactly the product that Pathr, one of our partners is offering, but they're doing it with Computer Vision and cameras. So when Pathr tries to help retail stores\n      analyze the foot traffic and understand, with heat maps, where are people spending the most time in stores, how many people are coming in, what size groups are coming into the store and trying\n      to help understand if there was a successful transaction from the people who entered the store and left the store, to help with the retail analytics and marketing sales and positioning of\n      products. And so, they're doing that in a way that also protects privacy. And that's something that's really important. So when you talked about those Bluetooth beacons, probably if everyone\n      who walked into a grocery store was asked to put a tracking device in their cart or on their person and say, \"You're going to be tracked around the store,\" they probably wouldn't want to do\n      that.\n    </rh-cue>\n    <rh-cue start=\"08:53\" voice=\"Ryan Loney\">\n      The way that you can do this with cameras, is you can detect people as they enter and remove their face. So you can ignore any biometric information and just track the person based on pixels\n      that are present in the detected region of interest. So they're able to analyze... Say a family walks in the door and they can group those people together with object detection and then they\n      can track their movement throughout the store without keeping track of their face, or any biometric, or any personal identifiable information, to avoid things like bias and to make sure that\n      they're protecting the privacy of the shoppers in the store, while still getting that really useful marketing analytics data. So that they can make better decisions about where to place their\n      products. That's one really good example of how Computer Vision, AI with OpenVINO is being used today.\n    </rh-cue>\n    <rh-cue start=\"09:49\" voice=\"Burr Sutter\">\n      And that is a great example, because you're definitely spot on. It is invasive when you hand someone a Bluetooth device and say, \"Please, keep this with you as you go throughout our store,\n      our mall or throughout our hospital, wherever you might be.\" Now you mentioned another example earlier in the conversation which was related to worker safety. \"Are they wearing a helmet?\" I\n      want to talk more about that concept in a real industrial setting, a manufacturing setting, where there might be a factory floor and there's certain requirements. Or better yet there's like a\n      quality assurance requirement, let's say, when it comes to looking at a factory line. I've run that use case often with some of our customers. Can you talk more about those kinds of use cases?\n    </rh-cue>\n    <rh-cue start=\"10:23\" voice=\"Ryan Loney\">\n      One of our partners, Robotron, we published a case study, I think last year, where they were working with BMW at one of their factories. And they do quality control inspection, but they're\n      also doing things related to worker safety and analyzing. I use the safety hat example. There's a number of our ISVs and partners who have similar use cases and it comes down to, there's a few\n      reasons that are motivating this and some are related to insurance. It's important to make sure that if you want to have your factory insured, that your workers are protecting themselves and\n      wearing the gear regulatory compliance, you're being asked to properly protect from exposure to chemicals or potentially having something fall and hit someone on the head. So wearing a safety\n      vest, wearing goggles, wearing a helmet, these are things that you need to do inside the factory and you can really easily automate and detect and sometimes without bias.\n    </rh-cue>\n    <rh-cue start=\"11:21\" voice=\"Ryan Loney\">\n      I think that's one of the interesting things about the Robotron-BMW example is that they were also blurring, blacking out, so drawing a box to cover the face of the workers in the factory, so\n      that somebody who was analyzing the video footage and getting the alerts saying that, \"Bay 21 has a worker without a hat on,\" that it's not sending their face and in the alert and potentially\n      invading or going against privacy laws or just the ethics of the company. They don't want to introduce bias or have people targeted because it's much better to blur the face and alert and have\n      somebody take care of it on the floor. And then, if you ever need to audit that information later, they have a way to do it where people who need to be able to see who the employee was and\n      look up their personal information, they can do that.\n    </rh-cue>\n    <rh-cue start=\"12:17\" voice=\"Ryan Loney\">\n      But then just for the purposes of maintaining safety, they don't need to have access to that personal information, or biometric information. Because that's one thing that when you hear about\n      Computer Vision or person tracking, object detection, there's a lot of concern, and rightfully so, about privacy being invaded and about tracking information, face re-identification,\n      identifying people who may have committed crimes through video footage. And that's just not something that a lot of companies want to... They want to protect privacy and they don't want to be\n      in a situation where they might be violating someone's rights.\n    </rh-cue>\n    <rh-cue start=\"12:56\" voice=\"Burr Sutter\">\n      Well, privacy is certainly opening up Pandora's box. There's a lot to be explored in that area, especially in a digital world that we now live in. But for now, let's move on and explore a\n      different area. I'm interested in how machines and computers offer advantages specifically in certain use cases like a quality control scenario. I asked Ryan to explain how a AI/ML and\n      specifically machines, computers, could augment that capability.\n    </rh-cue>\n    <rh-cue start=\"13:20\" voice=\"Ryan Loney\">\n      I can give a specific example where we have a partner that's doing defect detection, looking for anomalies in batteries. I'm sure you've heard there's a lot of interest right now in electric\n      vehicles, a lot of batteries being produced. And so, if you go into one of these factories, they have images that they collect of every battery that's going through this assembly line. And\n      through these images, people can look and see and visually inspect what their eyes and say, \"This battery has a defect, send it back.\" And that's one step in the quality control process,\n      there's other steps I'm sure, like running diagnostic tests and measuring voltage and doing other types of non-visual inspection. But for the visual inspection piece, where you can really\n      easily identify some problems, it's much more efficient to introduce Computer Vision. And so, that's where we have this new library that we've introduced, called Anomalib.\n    </rh-cue>\n    <rh-cue start=\"14:17\" voice=\"Ryan Loney\">\n      So OpenVINO, while we're focused on inference, we're also thinking about the pipeline, or the funnel, that gets these models to OpenVINO. And so, we've invested in this anomaly segmentation,\n      anomaly detection library that we've recently open sourced and there's a great research paper about it, about Anomalib, but the idea is you can take just a few images and train a model and\n      start detecting these defects. And so, for this battery example, that's a more advanced example, but to make it simpler, take some bolts and... Take 10 bolts. You have one that has a scratch\n      on it, or one that is chipped, or has some damage to it, and you can easily get started in training to recognize the bolts that do not have an anomaly and the ones that do, which is a small\n      data set. And I think that's really one of the most important things today.\n    </rh-cue>\n    <rh-cue start=\"15:11\" voice=\"Ryan Loney\">\n      Challenges, one is access to data, but the other is needing a massive amount of data to do something meaningful. And so we're starting to try to change that dynamic with Anomalib. You may not\n      need a 100,000 images, you may need 100 images and you can start detecting anomalies in everything from batteries to bolts to, maybe even the wood varnish use case that you mentioned.\n    </rh-cue>\n    <rh-cue start=\"15:37\" voice=\"Burr Sutter\">\n      That is a very key point because often in that data scientist process, that data engineering data scientist process, the one key thing is, can you gather the data that you need for the input\n      for the model training? And we've often said, at least people I've worked with over the last couple years, \"You need a lot of data, you need tens of thousands of correct images, so we can sort\n      out the difference between dogs versus cats,\" let's say. Or you need dozens and dozens of situations where if it's a natural language processing scenario, a good customer interaction, a good\n      customer conversation. And this case it sounds like what you're saying is, \"Show us just the bad things, fewer images, fewer incorrect things, and then let us look for those kind of\n      anomalies.\" Can you tell us more about that? Because that is very interesting. The concept that I can use a much smaller data set as my input, as opposed to gathering terabytes of data in some\n      cases, to just simply get my model training underway.\n    </rh-cue>\n    <rh-cue start=\"16:30\" voice=\"Ryan Loney\">\n      Like you described, the idea is, if you have some good images and then you have some of the known defects, and you can just label, \"Here's a set of good images and here's a few of the\n      defects.\" And you can right away start detecting those specific defects that you've identified. And then, also be able to determine when it doesn't match the expected appearance of a non\n      defective item. So if I have the undamaged screw and then I introduce one with some new anomaly that's never been seen before, I can say this one is not a valid screw. And so, that's the\n      approach that we're taking and it's really important because so often you need to have subject matter experts. Take the battery example, there's these workers who are on the floor, in a\n      factory and they're the ones who know best when they look at these images, which one's going to have an issue, which one's defective.\n    </rh-cue>\n    <rh-cue start=\"17:31\" voice=\"Ryan Loney\">\n      And then they also need to take that subject matter expertise and then use it to annotate data sets. And when you have these tens of thousands of images you need to annotate, it's asking\n      those people to stop working on the factory floor so they can come annotate some images. That's a tough business call to make, right? But if you only need them to annotate a handful of images,\n      it's a much easier ask to get the ball rolling and demonstrate value. And maybe over time you will want to annotate more and more images because you'll get even better accuracy in the model.\n      Even better, even if it's just small incremental improvements, that's something that if it generates value for the business, it's something the business will invest in over time. But you have\n      to convince the decision makers that it's worth the time of these subject matter experts to stop what they're doing and go and label some images of the things that they're working on in the\n      factory.\n    </rh-cue>\n    <rh-cue start=\"18:27\" voice=\"Burr Sutter\">\n      And that labeling process can be very labor intensive. If the annotation is basically saying what is correct, what's wrong, what is this, what is that. And therefore if we can minimize that\n      timeframe to get the value quicker, then there's something that's useful for the business, useful for the organization, long before we necessarily go through a whole huge model training phase.\n\n    </rh-cue>\n    <rh-cue start=\"18:49\" voice=\"Burr Sutter\">\n      So we talked about labeling and how that is labor intensive activity, but I love the idea of helping the human. And helping the human most specifically not get bored. Basically if the human\n      is eyeballing a bunch of widgets flying by, over time they make mistakes, they get bored and they don't pay as close attention as they should. That's why the constant of AI/ML, and\n      specifically Computer Vision augmenting that capability and really helping the human identify anomalies faster, more quickly, maybe with greater accuracy, could be a big win. We focused on\n      manufacturing, but let's actually go into healthcare and learn how these tools can be used in that sector and that industry. Ryan talked me about how OpenVINO's run time can be incorporated\n      into medical imaging equipment with Intel processors embedded in CT, MRI and ultrasound machines. While these inferences, this AI/ML workload, can be operating and executing right there in the\n      same physical room as the patient.\n    </rh-cue>\n    <rh-cue start=\"19:44\" voice=\"Ryan Loney\">\n      We did a presentation with GE last year, I think they said there's at least 80 countries that have their x-ray machines deployed. And they're doing things like helping doctors place breathing\n      tubes in patients. So during COVID, during the pandemic, that was a really important tool to help with nurses and doctors who were intubating patients, sometimes in a parking lot or a hallway\n      of a hospital. And when they had a statistic that GE said, I think one out of four breathing tubes gets placed incorrectly when you're doing it outside the operating room. Because when you're\n      in an operating room it's much more controlled and there's someone who's an expert at placing the tubes, it's something you have more of a controlled environment. But when you're out, in a\n      parking lot, in a tent, when the hospital's completely full and you're triaging patients with COVID, that's when they're more likely to make mistakes.And so, they had this endotracheal tube\n      placement, ETT, model that they trained and it helped to use an x-ray and give an alert and say, \"This tube is placed wrong, pull it out and do it again.\" And so, things like that help doctors\n      so that they can avoid mistakes. And having a breathing tube placed incorrectly can cause collapsed lung and a number of other unwanted side effects. So it's really important to do it\n      correctly. Another example is Samsung Medison. They actually are estimating fetal angle of progression. So this is analyzing ultrasound of pregnant women being able to help take measurements\n      that are usually hard to calculate, but it can be done in an automated way. They're already taking an ultrasound scan and now they're executing this model that can take some of these\n      measurements to help the doctor avoid potentially more intrusive alternative methods. So the patient wins, it makes their life better and the doctor is getting help from this AI model. And\n      those are just a few examples.\n    </rh-cue>\n    <rh-cue start=\"21:42\" voice=\"Burr Sutter\">\n      Those are some amazing examples when it comes to all these things, we're talking CT scans and x-rays, other examples of Computer Vision. One thing that's kind of interesting in this space, I\n      think, whenever I get a chance to work on, let's say an object detection model, and one of our workshops, by the way, is actually putting that out in front of people to say, \"Look, you can use\n      your phone and it basically sends the image over to our OpenShift with our data science platform and then analyzes what you see.\" And even in my case, where I take a picture of my dog as an\n      example, it can't really decide, is it a dog or a cat? I have a very funny looking dog.\n    </rh-cue>\n    <rh-cue start=\"22:15\" voice=\"Burr Sutter\">\n      And so there's always a percentage outcome. In other words, \"I think it's a dog, 52%.\" So I want to talk about that more. How important is it to get to that a hundred percent accuracy? How\n      important is it to really, depending on the use case, to allow for the gray area if you will, where it's an 80% accuracy or a 70% accuracy, and what are the trade offs there associated with\n      the application? Can you discuss that more?\n    </rh-cue>\n    <rh-cue start=\"22:38\" voice=\"Ryan Loney\">\n      Accuracy is definitely a touchy subject, because how you measure it makes a huge difference. I think what you were describing with the dog example, there's sort of a top five potential\n      classes that might maybe be identified. So let's say you're doing object detection and you detect a region of interest, and it says 65% confidence this is a dog. Well, the next potential label\n      that could be maybe 50% confidence or 20% confidence might be something similar to a dog. Or in the case of models that have been trained on the ImageNet dataset or on COCO dataset, they have\n      actual breeds of dogs. If I want to look at the top five labels for a dog, for my dog for example, she's a mix, mostly a Labrador retriever, but I may look at the top five labels and it may\n      say 65% confidence that she's a flat coated retriever.\n    </rh-cue>\n    <rh-cue start=\"23:32\" voice=\"Ryan Loney\">\n      And then confidence that she's a husky as 20%, and then 5% confidence that she's a greyhound or something. Those labels, all of them are dogs. So if I'm just trying to figure out, is this a\n      dog? I could probably find all of the classes within the data set and say, \"Well, these all, class ID 65, 132, 92 and 158, all belong to a group of dogs.\" So if I want to just write an\n      application to tell me if this is a dog or not, I would probably use that to determine if it's a dog. But how you measure that as accuracy, well that's where it gets a little bit complicated.\n      Because if you're being really strict about the definition and you're trying to validate against the data set of labeled images, and I have specific dog breeds or some specific detail and it\n      doesn't match, well then, the accuracy's going to go down.\n    </rh-cue>\n    <rh-cue start=\"24:25\" voice=\"Ryan Loney\">\n      And that's especially important when we talk about things like compression and quantization, which historically, has been difficult to get adoption in some domains, like healthcare, where\n      even the hint of accuracy going down implies that we're not going to be able to help. In some small case, maybe if it's even half a percent of the time, we won't detect that that tube is\n      placed incorrectly or that that patient's lung has collapsed or something like that. And that's something that really prevents adoption of some of these methods that can really boost\n      performance, like quantization. But if you take that example of... Different from the dog example, and you think about segmentation of kidneys. If I'm doing kidney segmentation, which is\n      taking a CT scan and then trying to pick the pixels out of that scan that belong to a kidney, how I measure accuracy may be how many of those pixels I'm able to detect and how many did I miss?\n    </rh-cue>\n    <rh-cue start=\"25:25\" voice=\"Ryan Loney\">\n      Missing some of the pixels is maybe not a problem, depending on how you've built the application, because you still detect the kidney, and maybe you just need to apply padding around the\n      region of interest, so that you don't miss any of the actual kidney when you compress the model and when you quantize the model. But that requires a data scientist, an ML engineer, somebody to\n      really, they have to be able to go and apply that after the fact, after the inference happens, to make sure that you're not losing critical information. Because the next step from detecting\n      the kidney, may be detecting a tumor.\n    </rh-cue>\n    <rh-cue start=\"26:04\" voice=\"Ryan Loney\">\n      And so, maybe you can use the more optimized model to detect the kidney, but then you can use a slower model to detect the tumor. But that also requires somebody to architect and make that\n      decision or that trade off and say, \"Well, I need to add padding,\" or, \"I should only use the quantized model to detect the region of interest for the kidney.\" And then, use the model that\n      takes longer to do the inference just to find the tumor, which is going to be on a smaller size. The dimensions are going to be much smaller once we crop to the region of interest. But all of\n      those details, that's maybe not easy to explain in a few sentences and even the way I explained it is probably really confusing.\n    </rh-cue>\n    <rh-cue start=\"26:45\" voice=\"Burr Sutter\">\n      I do love that use case, like you mentioned, the cropping, even in one scenario that we worked on for another project, we specifically decided to pixelate the image that we had taken, because\n      we knew that we could get the outcome we wanted by even just using a smaller or having less resolution in our image. And therefore, as we transferred it from the mobile device, the edge\n      device, up into the cloud, we wanted that smaller image just for transfer purposes. And still, we could get the accuracy we needed by a lot of testing.\n    </rh-cue>\n    <rh-cue start=\"27:11\" voice=\"Burr Sutter\">\n      And one thing that's interesting about that, from my perspective, is, if you're doing image processing, sometimes it takes a while for this transaction to occur. I come from a traditional\n      application background, where I'm reading and writing things from a database, or a message broker, or moving data from one place to another. Those things happen sub-second normally, even with\n      great latency between your data centers, it's still sub-second in most cases. While a transaction like this one can actually take two seconds or four seconds, as it's doing its analysis and\n      actually coming back with its, \"I think it's a dog, I think it's a kidney, I think it's whatever.\" And providing me that accuracy statement. That concept of optimization is very important in\n      the overall application architecture. Would you agree with that or how do you think about that concept?\n    </rh-cue>\n    <rh-cue start=\"27:56\" voice=\"Ryan Loney\">\n      Definitely. It depends too on the use case. So if you think about how important it is to reduce the latency and increase the number of frames per second that you can process when you're\n      talking about a loss prevention model that's running at a grocery store. You want to keep the lines moving, you don't want every person who's at the self checkout to have to wait five seconds\n      for every item they scan. You need it to happen as quickly as possible. And if sometimes the accuracy decreases slightly, or I'd say the accuracy of the whole pipeline, so not just looking at\n      the individual model or the individual inference, but let's say that the whole pipeline is not as successful at detecting when somebody steals one item from the self checkout, it's not going\n      to be a life threatening situation. Whereas being hooked up to the x-ray machine with the tube placement model, they might be willing to have the doctor or the nurse wait five seconds to get\n      the result.\n    </rh-cue>\n    <rh-cue start=\"28:55\" voice=\"Ryan Loney\">\n      They don't need it to happen in 500 milliseconds. Their threshold for waiting is a little bit higher. That, I think, also drives some of the decision. You want to keep people moving through\n      the checkout line and you can afford to, potentially, if you lose a little bit of accuracy here and there, it's not going to cost the company that much money or it's not going to be life\n      threatening. It's going to be worth the trade off of keeping the line moving and not having people leave the store and not check out at all, to say, \"I'm not going to shop today because the\n      line's too long.\"\n    </rh-cue>\n    <rh-cue start=\"29:30\" voice=\"Burr Sutter\">\n      There are so many trade-offs in enterprise AI/ML use cases, things like latency, accuracy and availability, and certainly complexities abound, especially in an obviously ever-evolving\n      technological landscape where we are still very early in the adoption of AI/ML. And to navigate that complexity, that direct feedback from real world end users is essential to Ryan and his\n      team at Intel. What would you say are some of the big hurdles or big outcomes, big opportunities in that space? And do you agree that we're still at the very beginning, in our infancy if you\n      will, of adopting these technologies and discovering what they can do for us?\n    </rh-cue>\n    <rh-cue start=\"30:06\" voice=\"Ryan Loney\">\n      Yeah, I think we're definitely in the infancy and I think that what we've seen is, our customers are evolving and the people who are deploying on Intel hardware, they're trying to run more\n      complicated models. They're the models that are doing object detection or detecting defects and doing segmentation. In the past you could say, \"Here's a generic model that will do face\n      detection, or person detection, or vehicle detection, license plate detection.\" And those are general purpose models that you can just grab off the shelf and use them. But now we're moving\n      into the Anomalib scenarios, where I've got my own data and I'm trying to do something very specific and I'm the only one that has access to this data. You don't have that public data set that\n      you can go download that's under Creative Commons license for car batteries. It's just not something that's available.\n    </rh-cue>\n    <rh-cue start=\"30:57\" voice=\"Ryan Loney\">\n      And so, those use cases, the challenge with training those models and getting them optimized is the beginning of the pipeline. It's the data. You have to get the data, you have to annotate it\n      and the tools have to exist for you to do that. And that's part of the problem that we're trying to help solve. And then, the models are getting more complex. So if you think, just from\n      working with customers recently, they're no longer just trying to do image classification, \"Is it a dog or a cat?\" They've moved on to 3D point clouds and 3D segmentation models and things\n      that are like the speech synthesis example. These GPT models that are generating... You put a text input and it generates an image for you. It's just becoming much more advanced, much more\n      sophisticated and on larger images.\n    </rh-cue>\n    <rh-cue start=\"31:50\" voice=\"Ryan Loney\">\n      And so things like running super resolution and enhancing images, upscaling images, instead of just trying to take that 200 by 200 pixel image and classifying if it's a cat, now we're talking\n      about gigantic, huge images that we're processing and that all requires more resources or more optimized models. And every Computer Vision conference or AI conference, there's a new latest and\n      greatest architecture, there's new research paper, and things are getting adopted much faster. The lead time for a NeurIPS paper, CVPR, for a company to actually adopt and put those into\n      production, the time shortens every year.\n    </rh-cue>\n    <rh-cue start=\"32:34\" voice=\"Burr Sutter\">\n      Well Ryan, I got to tell you, I could talk to you, literally, all day about these topics, the various use cases, the various ways models are being optimized, how to put models into a pipeline\n      for average enterprise applications. I've enjoyed learning about OpenVINO and Anomalib. I'm fascinated by this, because I'll have a chance to go try this myself, taking advantage of Red Hat\n      OpenShift and taking advantage of our data science platform. On top of that, I will definitely go be poking at this myself. Thank you so much for your time today.\n    </rh-cue>\n    <rh-cue start=\"33:00\" voice=\"Ryan Loney\">\n      Thanks, Burr. This was a lot of fun. Thanks for having me.\n    </rh-cue>\n    <rh-cue start=\"33:05\" voice=\"Burr Sutter\">\n      You can check out the full transcript of our conversation and more resources, like a link to a white paper on OpenVINO and Anomalib at redhat.com/codecommentspodcast. This episode was\n      produced by Brent Simoneaux and Caroline Creaghead. Our sound designer is Christian Prohom. Our audio team includes Leigh Day, Stephanie Wonderlick, Mike Esser, Laura Barnes, Claire Allison,\n      Nick Burns, Aaron Williamson, Karen King, Boo Boo Howse, Rachel Ertel, Mike Compton, Ocean Matthews, Laura Walters, Alex Traboulsi, and Victoria Lawton. I'm your host, Burr Sutter. Thank you\n      for joining me today on Code Comments. I hope you enjoyed today's session and today's conversation, and I look forward to many more.\n    </rh-cue>\n  </rh-transcript>\n</rh-audio-player>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\">\n  import '@rhds/elements/rh-audio-player/rh-audio-player.js';\n</script>\n\n<!--playground-fold--><link rel=\"stylesheet\" href=\"rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Audio Player"
    },
    "demo/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "rh-audio-player-lightdom.css": {
      "content": "*[hidden] {\n  display: none !important;\n}\n\nrh-audio-player {\n  line-height: var(--rh-line-height-body-text, 1.5);\n  font-weight: var(--rh-font-weight-body-regular, 400);\n  font-size: var(--rh-font-size-code-md, 1rem);\n  font-family: var(--rh-font-family-body-text, RedHatText, \"Red Hat Text\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Helvetica, Arial, sans-serif);\n  color: var(--rh-audio-player-text-color, var(--rh-color-text-primary-on-light, #151515));\n}\n\nrh-audio-player:not([expanded]) rh-transcript:not(:defined) {\n  display: none;\n}\n\nrh-cue[start]:not(:defined):before,\nrh-cue[voice]:not(:defined):before {\n  font-family: var(--rh-font-family-heading, RedHatDisplay, \"Red Hat Display\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Helvetica, Arial, sans-serif);\n  font-size: var(--rh-font-size-body-text-md, 1rem);\n  font-weight: var(--rh-font-weight-heading-bold, 700);\n}\n\nrh-cue[voice]:not(:defined):before {\n  content: attr(voice);\n}\n\nrh-cue[start]:not(:defined):before {\n  content: attr(start);\n}\n\nrh-cue[voice][start]:not(:defined):before {\n  content: attr(start) \" - \" attr(voice);\n}\n\nrh-cue[voice][start]:empty {\n  display: block;\n  margin-top: var(--rh-space-lg, 16px);\n}\n\nrh-audio-player-about,\nrh-audio-player-subscribe {\n  display: block;\n}\n\nrh-audio-player[color-palette=\"dark\"] {\n  color: var(--rh-audio-player-text-color, var(--rh-color-text-primary-on-dark, #ffffff));\n}\n\nrh-audio-player[color-palette=\"dark\"] > * {\n  background-color:\n    var(--rh-audio-player-background-color,\n      var(--rh-color-surface-darkest, #151515));\n}\n\nrh-audio-player-about,\nrh-audio-player-subscribe,\nrh-transcript {\n  background-color:\n    var(--rh-audio-player-background-color,\n      var(--rh-color-surface-lightest, #ffffff));\n}\n\nrh-audio-player > [slot=\"series\"] {\n  letter-spacing: var(--rh-font-letter-spacing-body-text, 0.0125rem);\n  font-size: var(--rh-font-size-body-text-xs, 0.75rem);\n  font-weight: var(--rh-font-weight-heading-medium, 500);\n  margin: 0 0 var(--rh-space-md, 8px);\n  padding: 0;\n}\n\nrh-audio-player > [slot=\"title\"],\nrh-audio-player [slot=\"heading\"] {\n  font-size: var(--rh-font-size-heading-xs, 1.25rem);\n  font-family: var(--rh-font-family-heading, RedHatDisplay, \"Red Hat Display\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Helvetica, Arial, sans-serif);\n  font-weight: var(--rh-font-weight-heading-medium, 500);\n  line-height: var(--rh-line-height-heading, 1.3);\n  margin: 0 0 var(--rh-space-lg, 16px);\n  padding: 0;\n}\n\nrh-audio-player [slot=\"heading\"] {\n  font-size: var(--rh-font-size-body-text-md, 1rem);\n}\n\n",
      "hidden": true
    },
    "demo/customization/index.html": {
      "contentType": "text/html",
      "selected": false,
      "content": "<h2>rh-audio-player</h2>\n\n<!-- Options for demo -->\n<form>\n  <p>Options:</p>\n  <ul>\n    <li><label>Poster: <input name=\"poster\" type=\"checkbox\" checked=\"\"></label></li>\n    <li><label>Color Palette:\n        <select name=\"palette\">\n          <option value=\"light\" selected=\"\">Light</option>\n          <option value=\"dark\">Dark</option>\n          <option value=\"purple\">Purple</option>\n          <option value=\"purple img\">Purple with Image</option>\n          <option value=\"cyan\">Cyan </option>\n        </select>\n      </label></li>\n    <li>\n      <label>Layout:\n        <select name=\"layout\">\n          <option value=\"full\" selected=\"\">Full</option>\n          <option value=\"compact-wide\">Compact Wide</option>\n          <option value=\"compact\">Compact</option>\n          <option value=\"\">Mini</option>\n        </select>\n      </label>\n    </li>\n  </ul>\n</form>\n\n<!-- Audio player demo will update via form -->\n<rh-audio-player id=\"player\" layout=\"full\" poster=\"https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png\">\n  <p slot=\"series\">Code Comments</p>\n  <h3 slot=\"title\">Bringing Deep Learning to Enterprise Applications</h3>\n  <rh-audio-player-about slot=\"about\">\n    <h4 slot=\"heading\">About the episode</h4>\n    <p>\n      There are a lot of publicly available data sets out there. But when it\n      comes to specific enterprise use cases, you're not necessarily going to\n      able to find one to train your models. To realize the power of AI/ML in\n      enterprise environments, end users need an inference engine to run on\n      their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open\n      toolkits from Intel that do precisely that. He looks specifically at\n      anomaly detection in use cases as varied as medical imaging and\n      manufacturing.\n    </p>\n    <p>\n      Want to learn more about Anomalib? Check out the research paper that\n      introduces the deep learning library.\n    </p>\n    <rh-avatar slot=\"profile\" src=\"https://www.redhat.com/cms/managed-files/ryan-loney.png\">\n      Ryan Loney\n      <span slot=\"subtitle\">Product manager, OpenVINO Developer Tools, <em>IntelÂ®</em></span>\n    </rh-avatar>\n  </rh-audio-player-about>\n  <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n    <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3\">\n  </audio>\n  <rh-audio-player-subscribe slot=\"subscribe\">\n    <h4 slot=\"heading\">Subscribe</h4>\n    <p>Subscribe here:</p>\n    <a slot=\"link\" href=\"https://podcasts.apple.com/us/podcast/code-comments/id1649848507\" target=\"_blank\" title=\"Listen on Apple Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Apple Podcasts\" data-analytics-category=\"Hero|Listen on Apple Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg\" alt=\"Listen on Apple Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD\" target=\"_blank\" title=\"Listen on Spotify\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Spotify\" data-analytics-category=\"Hero|Listen on Spotify\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_spotify.svg\" alt=\"Listen on Spotify\">\n    </a>\n    <a slot=\"link\" href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw\" target=\"_blank\" title=\"Listen on Google Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Google Podcasts\" data-analytics-category=\"Hero|Listen on Google Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_google-podcast.svg\" alt=\"Listen on Google Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://feeds.pacific-content.com/codecomments\" target=\"_blank\" title=\"Subscribe via RSS Feed\" data-analytics-linktype=\"cta\" data-analytics-text=\"Subscribe via RSS Feed\" data-analytics-category=\"Hero|Subscribe via RSS Feed\">\n      <img class=\"img-fluid\" src=\"https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg\" alt=\"Subscribe via RSS Feed\">\n    </a>\n  </rh-audio-player-subscribe>\n  <rh-transcript id=\"regular\" slot=\"transcript\">\n    <h4 slot=\"heading\">Transcript</h4>\n    <rh-cue start=\"00:02\" voice=\"Burr Sutter\">\n      Hi, I'm Burr Sutter. I'm a Red Hatter who spends a lot of time talking to technologists about technologies. We say this a lot at Red Hat. No single technology provider holds the key to success,\n      including us. And I would say the same thing about myself. I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners. This is Code\n      Comments, an original podcast from Red Hat.\n    </rh-cue>\n    <rh-cue start=\"00:29\" voice=\"Burr Sutter\">\n      I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning. I've been thinking about that for quite some time and I actually had the\n      opportunity to work on a few successful projects, here at Red Hat, using those technologies, actually enabling a data set, gathering a data set, working with a data scientist and data\n      engineering team, and then training a model and putting that model into production runtime environment. It was an exciting set of projects and you can see those on numerous YouTube videos that\n      have published out there before. But I want you to think about the problem space a little bit, because there are some interesting challenges about a AI/ML. One is simply just getting access to\n      the data, and while there are numerous publicly available data sets, when it comes to your specific enterprise use case, you might not be to find publicly available data.\n    </rh-cue>\n    <rh-cue start=\"01:14\" voice=\"Burr Sutter\">\n      In many cases you cannot, even for our applications that we created, we had to create our data set, capture our data set, explore the data set, and of course, train a model accordingly. And we\n      also found there's another challenge to be overcome in this a AI/ML world, and that is access to certain types of hardware. If you think about an enterprise environment and the creation of an\n      enterprise application specifically for a AI/ML, end users need an inference engine to run on their hardware. Hardware that's available to them, to be effective for their application. Let's say\n      an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor. As those things are whizzing by on the factory line there, looking at them and\n      trying to determine if there is an error or not.\n    </rh-cue>\n    <rh-cue start=\"01:56\" voice=\"Burr Sutter\">\n      Well, how do you actually make it run on your hardware, your accessible technology that you have today? Well, there's a solution for this as an open toolkit called OpenVINO. And you might be\n      thinking, \"Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning? Well, not according to Ryan Loney, product manager of OpenVINO\n      Developer Tools at Intel.\n    </rh-cue>\n    <rh-cue start=\"02:20\" voice=\"Ryan Loney\">\n      I guess I'll start with trying to maybe dispel a myth. I think that CPUs are widely used for inference today. So if we look at the data center segment, about 70% of the AI inference is happening\n      on Intel Xeon, on our data center CPUs. And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO, is that we're taking models that may have been\n      trained on a GPU using deep learning frameworks like PyTorch or TensorFlow, and then optimizing them to run on Intel hardware.\n    </rh-cue>\n    <rh-cue start=\"02:57\" voice=\"Burr Sutter\">\n      Ryan joined me to discuss AI/ML in the enterprise across various industries and exploring numerous use cases. Let's talk a little bit about the origin story behind OpenVINO. Tell us more about\n      it and how it came to be and why it came out of Intel.\n    </rh-cue>\n    <rh-cue start=\"03:12\" voice=\"Ryan Loney\">\n      Definitely. We had the first release of OpenVINO, was back in 2018, so still relatively new. And at that time, we were focused on Computer Vision and pretty tightly coupled with OpenCV, which is\n      another open source library with origins at Intel. It had its first release back in 1999, so it's been around a little bit longer. And many of the software engineers and architects at Intel that\n      were involved with and contributing to OpenCV are working on OpenVINO. So you can think of OpenVINO as complimentary software to OpenCV and we're providing an engine for executing inferences as\n      part of a Computer Vision pipeline, or at least that's how we started.\n    </rh-cue>\n    <rh-cue start=\"03:58\" voice=\"Ryan Loney\">\n      But since 2018, we've started to move beyond just Computer Vision inference. So when I say Computer Vision inference, I mean image classification, object detection, segmentation, and now we're\n      moving into natural language processing. Things like speech synthesis, speech recognition, knowledge graphs, time series forecasting and other use cases that don't involve Computer Vision and\n      don't involve inference on pixels. Our latest release, the 2022.1 that came out earlier this year, that was the most significant update that we've had to OpenVINO, since we started in 2018. And\n      the major focus of that release was optimizing for use cases that go beyond Computer Vision.\n    </rh-cue>\n    <rh-cue start=\"04:41\" voice=\"Burr Sutter\">\n      And I like that concept that you just mentioned right there, Computer Vision, and you said that you extended those use cases and went beyond that. Could you give us some more concrete examples\n      of Computer Vision?\n    </rh-cue>\n    <rh-cue start=\"04:50\" voice=\"Ryan Loney\">\n      Sure. When you think about manufacturing, quality control in factories, everything from arc welding, defect detection to inspecting BMW cars on assembly lines, they're using cameras or sensors\n      to collect data and usually it's cameras collecting images like RGB images that you and I can see and looks like something taken from a camera or video camera. But also, things like infrared or\n      computerized tomography scans used in healthcare, X-ray, different types of images where we can draw bounding boxes around regions of interest and say, \"This is a defect,\" or, \"This is not a\n      defect.\" And also, \"Is this worker wearing a safety hat or did they forget to put it on?\" And so, you can take this and integrate it into a pipeline where you're triggering an alert if somebody\n      forgets to wear their safety mask, or if there's a defect in a product on an assembly line, you can just use cameras and OpenVINO and OpenCV running these on Intel hardware and help to analyze.\n    </rh-cue>\n    <rh-cue start=\"05:58\" voice=\"Ryan Loney\">\n      And that's what a lot of the partners that we work with are doing, so these independent software vendors. And there's other use cases for things like retail. You think about going to a store and\n      using an automated checkout system. Sometimes people use those automated checkouts and they slide a few extra items into their bag that they don't scan and it's a huge loss for the retail\n      outlets that are providing this way to check out realtime shelf monitoring. We have a Vispera, one of our ISVs that helps keep store shelves stocked by just analyzing the cameras in the stores,\n      detecting when objects are missing from the shelves so that they can be restocked. We have Vistry, another ISV that works with quick service restaurants. When you think about automating the\n      process of, when do I drop the fries into the fryer so that they're warm when the car gets to the drive through window, there's quite a bit of industrial healthcare retail examples that we can\n      walk through.\n    </rh-cue>\n    <rh-cue start=\"06:55\" voice=\"Burr Sutter\">\n      And we should dig into some more of those, but I got to tell you, I have a personal experience in this category that I want to share with and you can tell me how silly you might think at this\n      point in time it is. We actually built a keynote demonstration for the Red Hat big stage back in 2015. And I really want to illustrate the concept of asset tracking. So we actually gave\n      everybody in the conference a little Bluetooth token with a little battery, a little watch battery, and a little Bluetooth emitter. And we basically tracked those things around the conference.\n      We basically put a raspberry pi in each of the meeting rooms and up in the lunch room and you could see how the tokens moved from room to room to room.\n    </rh-cue>\n    <rh-cue start=\"07:28\" voice=\"Burr Sutter\">\n      It was a relatively simple application, but it occurred to me, after we figured out how to do that with Bluetooth and triangulating Bluetooth signals by looking at relative signal strength from\n      one radio to another and putting that through an Apache Spark application at the time, we then realized, \"You know what? This is easier done with cameras.\" And just simply looking at a camera\n      and having some form of a AI/ML model, a machine learning model, that would say, \"There are people here now,\" or, \"There are no people here now.\" What do you think about that?\n    </rh-cue>\n    <rh-cue start=\"07:56\" voice=\"Ryan Loney\">\n      What you just described is exactly the product that Pathr, one of our partners is offering, but they're doing it with Computer Vision and cameras. So when Pathr tries to help retail stores\n      analyze the foot traffic and understand, with heat maps, where are people spending the most time in stores, how many people are coming in, what size groups are coming into the store and trying\n      to help understand if there was a successful transaction from the people who entered the store and left the store, to help with the retail analytics and marketing sales and positioning of\n      products. And so, they're doing that in a way that also protects privacy. And that's something that's really important. So when you talked about those Bluetooth beacons, probably if everyone who walked into a\n      grocery store was asked to put a tracking device in their cart or on their person and say, \"You're going to be tracked around the store,\" they probably wouldn't want to do that.\n    </rh-cue>\n    <rh-cue start=\"08:53\" voice=\"Ryan Loney\">\n      The way that you can do this with cameras, is you can detect people as they enter and remove their face. So you can ignore any biometric information and just track the person based on pixels\n      that are present in the detected region of interest. So they're able to analyze... Say a family walks in the door and they can group those people together with object detection and then they can\n      track their movement throughout the store without keeping track of their face, or any biometric, or any personal identifiable information, to avoid things like bias and to make sure that they're\n      protecting the privacy of the shoppers in the store, while still getting that really useful marketing analytics data. So that they can make better decisions about where to place their products.\n      That's one really good example of how Computer Vision, AI with OpenVINO is being used today.\n    </rh-cue>\n    <rh-cue start=\"09:49\" voice=\"Burr Sutter\">\n      And that is a great example, because you're definitely spot on. It is invasive when you hand someone a Bluetooth device and say, \"Please, keep this with you as you go throughout our store, our\n      mall or throughout our hospital, wherever you might be.\" Now you mentioned another example earlier in the conversation which was related to worker safety. \"Are they wearing a helmet?\" I want to\n      talk more about that concept in a real industrial setting, a manufacturing setting, where there might be a factory floor and there's certain requirements. Or better yet there's like a quality\n      assurance requirement, let's say, when it comes to looking at a factory line. I've run that use case often with some of our customers. Can you talk more about those kinds of use cases?\n    </rh-cue>\n    <rh-cue start=\"10:23\" voice=\"Ryan Loney\">\n      One of our partners, Robotron, we published a case study, I think last year, where they were working with BMW at one of their factories. And they do quality control inspection, but they're also\n      doing things related to worker safety and analyzing. I use the safety hat example. There's a number of our ISVs and partners who have similar use cases and it comes down to, there's a few\n      reasons that are motivating this and some are related to insurance. It's important to make sure that if you want to have your factory insured, that your workers are protecting themselves and\n      wearing the gear regulatory compliance, you're being asked to properly protect from exposure to chemicals or potentially having something fall and hit someone on the head. So wearing a safety\n      vest, wearing goggles, wearing a helmet, these are things that you need to do inside the factory and you can really easily automate and detect and sometimes without bias.\n    </rh-cue>\n    <rh-cue start=\"11:21\" voice=\"Ryan Loney\">\n      I think that's one of the interesting things about the Robotron-BMW example is that they were also blurring, blacking out, so drawing a box to cover the face of the workers in the factory, so\n      that somebody who was analyzing the video footage and getting the alerts saying that, \"Bay 21 has a worker without a hat on,\" that it's not sending their face and in the alert and potentially\n      invading or going against privacy laws or just the ethics of the company. They don't want to introduce bias or have people targeted because it's much better to blur the face and alert and have\n      somebody take care of it on the floor. And then, if you ever need to audit that information later, they have a way to do it where people who need to be able to see who the employee was and look\n      up their personal information, they can do that.\n    </rh-cue>\n    <rh-cue start=\"12:17\" voice=\"Ryan Loney\">\n      But then just for the purposes of maintaining safety, they don't need to have access to that personal information, or biometric information. Because that's one thing that when you hear about\n      Computer Vision or person tracking, object detection, there's a lot of concern, and rightfully so, about privacy being invaded and about tracking information, face re-identification, identifying\n      people who may have committed crimes through video footage. And that's just not something that a lot of companies want to... They want to protect privacy and they don't want to be in a situation\n      where they might be violating someone's rights.\n    </rh-cue>\n    <rh-cue start=\"12:56\" voice=\"Burr Sutter\">\n      Well, privacy is certainly opening up Pandora's box. There's a lot to be explored in that area, especially in a digital world that we now live in. But for now, let's move on and explore a\n      different area. I'm interested in how machines and computers offer advantages specifically in certain use cases like a quality control scenario. I asked Ryan to explain how a AI/ML and\n      specifically machines, computers, could augment that capability.\n    </rh-cue>\n    <rh-cue start=\"13:20\" voice=\"Ryan Loney\">\n      I can give a specific example where we have a partner that's doing defect detection, looking for anomalies in batteries. I'm sure you've heard there's a lot of interest right now in electric\n      vehicles, a lot of batteries being produced. And so, if you go into one of these factories, they have images that they collect of every battery that's going through this assembly line. And\n      through these images, people can look and see and visually inspect what their eyes and say, \"This battery has a defect, send it back.\" And that's one step in the quality control process, there's\n      other steps I'm sure, like running diagnostic tests and measuring voltage and doing other types of non-visual inspection. But for the visual inspection piece, where you can really easily\n      identify some problems, it's much more efficient to introduce Computer Vision. And so, that's where we have this new library that we've introduced, called Anomalib.\n    </rh-cue>\n    <rh-cue start=\"14:17\" voice=\"Ryan Loney\">\n      So OpenVINO, while we're focused on inference, we're also thinking about the pipeline, or the funnel, that gets these models to OpenVINO. And so, we've invested in this anomaly segmentation,\n      anomaly detection library that we've recently open sourced and there's a great research paper about it, about Anomalib, but the idea is you can take just a few images and train a model and start\n      detecting these defects. And so, for this battery example, that's a more advanced example, but to make it simpler, take some bolts and... Take 10 bolts. You have one that has a scratch on it, or\n      one that is chipped, or has some damage to it, and you can easily get started in training to recognize the bolts that do not have an anomaly and the ones that do, which is a small data set. And\n      I think that's really one of the most important things today.\n    </rh-cue>\n    <rh-cue start=\"15:11\" voice=\"Ryan Loney\">\n      Challenges, one is access to data, but the other is needing a massive amount of data to do something meaningful. And so we're starting to try to change that dynamic with Anomalib. You may not\n      need a 100,000 images, you may need 100 images and you can start detecting anomalies in everything from batteries to bolts to, maybe even the wood varnish use case that you mentioned.\n    </rh-cue>\n    <rh-cue start=\"15:37\" voice=\"Burr Sutter\">\n      That is a very key point because often in that data scientist process, that data engineering data scientist process, the one key thing is, can you gather the data that you need for the input for\n      the model training? And we've often said, at least people I've worked with over the last couple years, \"You need a lot of data, you need tens of thousands of correct images, so we can sort out\n      the difference between dogs versus cats,\" let's say. Or you need dozens and dozens of situations where if it's a natural language processing scenario, a good customer interaction, a good\n      customer conversation. And this case it sounds like what you're saying is, \"Show us just the bad things, fewer images, fewer incorrect things, and then let us look for those kind of anomalies.\"\n      Can you tell us more about that? Because that is very interesting. The concept that I can use a much smaller data set as my input, as opposed to gathering terabytes of data in some cases, to just simply\n      get my model training underway.\n    </rh-cue>\n    <rh-cue start=\"16:30\" voice=\"Ryan Loney\">\n      Like you described, the idea is, if you have some good images and then you have some of the known defects, and you can just label, \"Here's a set of good images and here's a few of the defects.\"\n      And you can right away start detecting those specific defects that you've identified. And then, also be able to determine when it doesn't match the expected appearance of a non defective item.\n      So if I have the undamaged screw and then I introduce one with some new anomaly that's never been seen before, I can say this one is not a valid screw. And so, that's the approach that we're\n      taking and it's really important because so often you need to have subject matter experts. Take the battery example, there's these workers who are on the floor, in a factory and they're the ones\n      who know best when they look at these images, which one's going to have an issue, which one's defective.\n    </rh-cue>\n    <rh-cue start=\"17:31\" voice=\"Ryan Loney\">\n      And then they also need to take that subject matter expertise and then use it to annotate data sets. And when you have these tens of thousands of images you need to annotate, it's asking those\n      people to stop working on the factory floor so they can come annotate some images. That's a tough business call to make, right? But if you only need them to annotate a handful of images, it's a\n      much easier ask to get the ball rolling and demonstrate value. And maybe over time you will want to annotate more and more images because you'll get even better accuracy in the model. Even\n      better, even if it's just small incremental improvements, that's something that if it generates value for the business, it's something the business will invest in over time. But you have to\n      convince the decision makers that it's worth the time of these subject matter experts to stop what they're doing and go and label some images of the things that they're working on in the\n      factory.\n    </rh-cue>\n    <rh-cue start=\"18:27\" voice=\"Burr Sutter\">\n      And that labeling process can be very labor intensive. If the annotation is basically saying what is correct, what's wrong, what is this, what is that. And therefore if we can minimize that\n      timeframe to get the value quicker, then there's something that's useful for the business, useful for the organization, long before we necessarily go through a whole huge model training phase.\n    </rh-cue>\n    <rh-cue start=\"18:49\" voice=\"Burr Sutter\">\n      So we talked about labeling and how that is labor intensive activity, but I love the idea of helping the human. And helping the human most specifically not get bored. Basically if the human is\n      eyeballing a bunch of widgets flying by, over time they make mistakes, they get bored and they don't pay as close attention as they should. That's why the constant of AI/ML, and specifically\n      Computer Vision augmenting that capability and really helping the human identify anomalies faster, more quickly, maybe with greater accuracy, could be a big win. We focused on manufacturing, but\n      let's actually go into healthcare and learn how these tools can be used in that sector and that industry. Ryan talked me about how OpenVINO's run time can be incorporated into medical imaging\n      equipment with Intel processors embedded in CT, MRI and ultrasound machines. While these inferences, this AI/ML workload, can be operating and executing right there in the same physical room as\n      the patient.\n    </rh-cue>\n    <rh-cue start=\"19:44\" voice=\"Ryan Loney\">\n      We did a presentation with GE last year, I think they said there's at least 80 countries that have their x-ray machines deployed. And they're doing things like helping doctors place breathing\n      tubes in patients. So during COVID, during the pandemic, that was a really important tool to help with nurses and doctors who were intubating patients, sometimes in a parking lot or a hallway of\n      a hospital. And when they had a statistic that GE said, I think one out of four breathing tubes gets placed incorrectly when you're doing it outside the operating room. Because when you're in an\n      operating room it's much more controlled and there's someone who's an expert at placing the tubes, it's something you have more of a controlled environment. But when you're out, in a parking\n      lot, in a tent, when the hospital's completely full and you're triaging patients with COVID, that's when they're more likely to make mistakes.And so, they had this endotracheal tube placement,\n      ETT, model that they trained and it helped to use an x-ray and give an alert and say, \"This tube is placed wrong, pull it out and do it again.\" And so, things like that help doctors so that they\n      can avoid mistakes. And having a breathing tube placed incorrectly can cause collapsed lung and a number of other unwanted side effects. So it's really important to do it correctly. Another example\n      is Samsung Medison. They actually are estimating fetal angle of progression. So this is analyzing ultrasound of pregnant women being able to help take measurements that are usually hard to\n      calculate, but it can be done in an automated way. They're already taking an ultrasound scan and now they're executing this model that can take some of these measurements to help the doctor\n      avoid potentially more intrusive alternative methods. So the patient wins, it makes their life better and the doctor is getting help from this AI model. And those are just a few examples.\n    </rh-cue>\n    <rh-cue start=\"21:42\" voice=\"Burr Sutter\">\n      Those are some amazing examples when it comes to all these things, we're talking CT scans and x-rays, other examples of Computer Vision. One thing that's kind of interesting in this space, I\n      think, whenever I get a chance to work on, let's say an object detection model, and one of our workshops, by the way, is actually putting that out in front of people to say, \"Look, you can use\n      your phone and it basically sends the image over to our OpenShift with our data science platform and then analyzes what you see.\" And even in my case, where I take a picture of my dog as an\n      example, it can't really decide, is it a dog or a cat? I have a very funny looking dog.\n    </rh-cue>\n    <rh-cue start=\"22:15\" voice=\"Burr Sutter\">\n      And so there's always a percentage outcome. In other words, \"I think it's a dog, 52%.\" So I want to talk about that more. How important is it to get to that a hundred percent accuracy? How\n      important is it to really, depending on the use case, to allow for the gray area if you will, where it's an 80% accuracy or a 70% accuracy, and what are the trade offs there associated with the\n      application? Can you discuss that more?\n    </rh-cue>\n    <rh-cue start=\"22:38\" voice=\"Ryan Loney\">\n      Accuracy is definitely a touchy subject, because how you measure it makes a huge difference. I think what you were describing with the dog example, there's sort of a top five potential classes\n      that might maybe be identified. So let's say you're doing object detection and you detect a region of interest, and it says 65% confidence this is a dog. Well, the next potential label that\n      could be maybe 50% confidence or 20% confidence might be something similar to a dog. Or in the case of models that have been trained on the ImageNet dataset or on COCO dataset, they have actual\n      breeds of dogs. If I want to look at the top five labels for a dog, for my dog for example, she's a mix, mostly a Labrador retriever, but I may look at the top five labels and it may say 65%\n      confidence that she's a flat coated retriever.\n    </rh-cue>\n    <rh-cue start=\"23:32\" voice=\"Ryan Loney\">\n      And then confidence that she's a husky as 20%, and then 5% confidence that she's a greyhound or something. Those labels, all of them are dogs. So if I'm just trying to figure out, is this a dog?\n      I could probably find all of the classes within the data set and say, \"Well, these all, class ID 65, 132, 92 and 158, all belong to a group of dogs.\" So if I want to just write an application to\n      tell me if this is a dog or not, I would probably use that to determine if it's a dog. But how you measure that as accuracy, well that's where it gets a little bit complicated. Because if you're\n      being really strict about the definition and you're trying to validate against the data set of labeled images, and I have specific dog breeds or some specific detail and it doesn't match, well\n      then, the accuracy's going to go down.\n    </rh-cue>\n    <rh-cue start=\"24:25\" voice=\"Ryan Loney\">\n      And that's especially important when we talk about things like compression and quantization, which historically, has been difficult to get adoption in some domains, like healthcare, where even\n      the hint of accuracy going down implies that we're not going to be able to help. In some small case, maybe if it's even half a percent of the time, we won't detect that that tube is placed\n      incorrectly or that that patient's lung has collapsed or something like that. And that's something that really prevents adoption of some of these methods that can really boost performance, like\n      quantization. But if you take that example of... Different from the dog example, and you think about segmentation of kidneys. If I'm doing kidney segmentation, which is taking a CT scan and then\n      trying to pick the pixels out of that scan that belong to a kidney, how I measure accuracy may be how many of those pixels I'm able to detect and how many did I miss?\n    </rh-cue>\n    <rh-cue start=\"25:25\" voice=\"Ryan Loney\">\n      Missing some of the pixels is maybe not a problem, depending on how you've built the application, because you still detect the kidney, and maybe you just need to apply padding around the region\n      of interest, so that you don't miss any of the actual kidney when you compress the model and when you quantize the model. But that requires a data scientist, an ML engineer, somebody to really,\n      they have to be able to go and apply that after the fact, after the inference happens, to make sure that you're not losing critical information. Because the next step from detecting the kidney,\n      may be detecting a tumor.\n    </rh-cue>\n    <rh-cue start=\"26:04\" voice=\"Ryan Loney\">\n      And so, maybe you can use the more optimized model to detect the kidney, but then you can use a slower model to detect the tumor. But that also requires somebody to architect and make that\n      decision or that trade off and say, \"Well, I need to add padding,\" or, \"I should only use the quantized model to detect the region of interest for the kidney.\" And then, use the model that takes\n      longer to do the inference just to find the tumor, which is going to be on a smaller size. The dimensions are going to be much smaller once we crop to the region of interest. But all of those\n      details, that's maybe not easy to explain in a few sentences and even the way I explained it is probably really confusing.\n    </rh-cue>\n    <rh-cue start=\"26:45\" voice=\"Burr Sutter\">\n      I do love that use case, like you mentioned, the cropping, even in one scenario that we worked on for another project, we specifically decided to pixelate the image that we had taken, because we\n      knew that we could get the outcome we wanted by even just using a smaller or having less resolution in our image. And therefore, as we transferred it from the mobile device, the edge device, up\n      into the cloud, we wanted that smaller image just for transfer purposes. And still, we could get the accuracy we needed by a lot of testing.\n    </rh-cue>\n    <rh-cue start=\"27:11\" voice=\"Burr Sutter\">\n      And one thing that's interesting about that, from my perspective, is, if you're doing image processing, sometimes it takes a while for this transaction to occur. I come from a traditional\n      application background, where I'm reading and writing things from a database, or a message broker, or moving data from one place to another. Those things happen sub-second normally, even with\n      great latency between your data centers, it's still sub-second in most cases. While a transaction like this one can actually take two seconds or four seconds, as it's doing its analysis and\n      actually coming back with its, \"I think it's a dog, I think it's a kidney, I think it's whatever.\" And providing me that accuracy statement. That concept of optimization is very important in the\n      overall application architecture. Would you agree with that or how do you think about that concept?\n    </rh-cue>\n    <rh-cue start=\"27:56\" voice=\"Ryan Loney\">\n      Definitely. It depends too on the use case. So if you think about how important it is to reduce the latency and increase the number of frames per second that you can process when you're talking\n      about a loss prevention model that's running at a grocery store. You want to keep the lines moving, you don't want every person who's at the self checkout to have to wait five seconds for every\n      item they scan. You need it to happen as quickly as possible. And if sometimes the accuracy decreases slightly, or I'd say the accuracy of the whole pipeline, so not just looking at the\n      individual model or the individual inference, but let's say that the whole pipeline is not as successful at detecting when somebody steals one item from the self checkout, it's not going to be a\n      life threatening situation. Whereas being hooked up to the x-ray machine with the tube placement model, they might be willing to have the doctor or the nurse wait five seconds to get the result.\n    </rh-cue>\n    <rh-cue start=\"28:55\" voice=\"Ryan Loney\">\n      They don't need it to happen in 500 milliseconds. Their threshold for waiting is a little bit higher. That, I think, also drives some of the decision. You want to keep people moving through the\n      checkout line and you can afford to, potentially, if you lose a little bit of accuracy here and there, it's not going to cost the company that much money or it's not going to be life\n      threatening. It's going to be worth the trade off of keeping the line moving and not having people leave the store and not check out at all, to say, \"I'm not going to shop today because the\n      line's too long.\"\n    </rh-cue>\n    <rh-cue start=\"29:30\" voice=\"Burr Sutter\">\n      There are so many trade-offs in enterprise AI/ML use cases, things like latency, accuracy and availability, and certainly complexities abound, especially in an obviously ever-evolving\n      technological landscape where we are still very early in the adoption of AI/ML. And to navigate that complexity, that direct feedback from real world end users is essential to Ryan and his team\n      at Intel. What would you say are some of the big hurdles or big outcomes, big opportunities in that space? And do you agree that we're still at the very beginning, in our infancy if you will, of\n      adopting these technologies and discovering what they can do for us?\n    </rh-cue>\n    <rh-cue start=\"30:06\" voice=\"Ryan Loney\">\n      Yeah, I think we're definitely in the infancy and I think that what we've seen is, our customers are evolving and the people who are deploying on Intel hardware, they're trying to run more\n      complicated models. They're the models that are doing object detection or detecting defects and doing segmentation. In the past you could say, \"Here's a generic model that will do face\n      detection, or person detection, or vehicle detection, license plate detection.\" And those are general purpose models that you can just grab off the shelf and use them. But now we're moving into\n      the Anomalib scenarios, where I've got my own data and I'm trying to do something very specific and I'm the only one that has access to this data. You don't have that public data set that you can go\n      download that's under Creative Commons license for car batteries. It's just not something that's available.\n    </rh-cue>\n    <rh-cue start=\"30:57\" voice=\"Ryan Loney\">\n      And so, those use cases, the challenge with training those models and getting them optimized is the beginning of the pipeline. It's the data. You have to get the data, you have to annotate it\n      and the tools have to exist for you to do that. And that's part of the problem that we're trying to help solve. And then, the models are getting more complex. So if you think, just from working\n      with customers recently, they're no longer just trying to do image classification, \"Is it a dog or a cat?\" They've moved on to 3D point clouds and 3D segmentation models and things that are like the\n      speech synthesis example. These GPT models that are generating... You put a text input and it generates an image for you. It's just becoming much more advanced, much more sophisticated and on\n      larger images.\n    </rh-cue>\n    <rh-cue start=\"31:50\" voice=\"Ryan Loney\">\n      And so things like running super resolution and enhancing images, upscaling images, instead of just trying to take that 200 by 200 pixel image and classifying if it's a cat, now we're talking\n      about gigantic, huge images that we're processing and that all requires more resources or more optimized models. And every Computer Vision conference or AI conference, there's a new latest and\n      greatest architecture, there's new research paper, and things are getting adopted much faster. The lead time for a NeurIPS paper, CVPR, for a company to actually adopt and put those into\n      production, the time shortens every year.\n    </rh-cue>\n    <rh-cue start=\"32:34\" voice=\"Burr Sutter\">\n      Well Ryan, I got to tell you, I could talk to you, literally, all day about these topics, the various use cases, the various ways models are being optimized, how to put models into a pipeline\n      for average enterprise applications. I've enjoyed learning about OpenVINO and Anomalib. I'm fascinated by this, because I'll have a chance to go try this myself, taking advantage of Red Hat\n      OpenShift and taking advantage of our data science platform. On top of that, I will definitely go be poking at this myself. Thank you so much for your time today.\n    </rh-cue>\n    <rh-cue start=\"33:00\" voice=\"Ryan Loney\">\n      Thanks, Burr. This was a lot of fun. Thanks for having me.\n    </rh-cue>\n    <rh-cue start=\"33:05\" voice=\"Burr Sutter\">\n      You can check out the full transcript of our conversation and more resources, like a link to a white paper on OpenVINO and Anomalib at redhat.com/codecommentspodcast. This episode was produced\n      by Brent Simoneaux and Caroline Creaghead. Our sound designer is Christian Prohom. Our audio team includes Leigh Day, Stephanie Wonderlick, Mike Esser, Laura Barnes, Claire Allison, Nick Burns,\n      Aaron Williamson, Karen King, Boo Boo Howse, Rachel Ertel, Mike Compton, Ocean Matthews, Laura Walters, Alex Traboulsi, and Victoria Lawton. I'm your host, Burr Sutter. Thank you for joining me\n      today on Code Comments. I hope you enjoyed today's session and today's conversation, and I look forward to many more.\n    </rh-cue>\n  </rh-transcript>\n</rh-audio-player>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\" src=\"customization.js\"></script>\n<!--playground-fold--><link rel=\"stylesheet\" href=\"../rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Customization"
    },
    "demo/customization/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "demo/rh-audio-player-lightdom.css": {
      "content": "*[hidden] {\n  display: none !important;\n}\n\nrh-audio-player {\n  line-height: var(--rh-line-height-body-text, 1.5);\n  font-weight: var(--rh-font-weight-body-regular, 400);\n  font-size: var(--rh-font-size-code-md, 1rem);\n  font-family: var(--rh-font-family-body-text, RedHatText, \"Red Hat Text\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Helvetica, Arial, sans-serif);\n  color: var(--rh-audio-player-text-color, var(--rh-color-text-primary-on-light, #151515));\n}\n\nrh-audio-player:not([expanded]) rh-transcript:not(:defined) {\n  display: none;\n}\n\nrh-cue[start]:not(:defined):before,\nrh-cue[voice]:not(:defined):before {\n  font-family: var(--rh-font-family-heading, RedHatDisplay, \"Red Hat Display\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Helvetica, Arial, sans-serif);\n  font-size: var(--rh-font-size-body-text-md, 1rem);\n  font-weight: var(--rh-font-weight-heading-bold, 700);\n}\n\nrh-cue[voice]:not(:defined):before {\n  content: attr(voice);\n}\n\nrh-cue[start]:not(:defined):before {\n  content: attr(start);\n}\n\nrh-cue[voice][start]:not(:defined):before {\n  content: attr(start) \" - \" attr(voice);\n}\n\nrh-cue[voice][start]:empty {\n  display: block;\n  margin-top: var(--rh-space-lg, 16px);\n}\n\nrh-audio-player-about,\nrh-audio-player-subscribe {\n  display: block;\n}\n\nrh-audio-player[color-palette=\"dark\"] {\n  color: var(--rh-audio-player-text-color, var(--rh-color-text-primary-on-dark, #ffffff));\n}\n\nrh-audio-player[color-palette=\"dark\"] > * {\n  background-color:\n    var(--rh-audio-player-background-color,\n      var(--rh-color-surface-darkest, #151515));\n}\n\nrh-audio-player-about,\nrh-audio-player-subscribe,\nrh-transcript {\n  background-color:\n    var(--rh-audio-player-background-color,\n      var(--rh-color-surface-lightest, #ffffff));\n}\n\nrh-audio-player > [slot=\"series\"] {\n  letter-spacing: var(--rh-font-letter-spacing-body-text, 0.0125rem);\n  font-size: var(--rh-font-size-body-text-xs, 0.75rem);\n  font-weight: var(--rh-font-weight-heading-medium, 500);\n  margin: 0 0 var(--rh-space-md, 8px);\n  padding: 0;\n}\n\nrh-audio-player > [slot=\"title\"],\nrh-audio-player [slot=\"heading\"] {\n  font-size: var(--rh-font-size-heading-xs, 1.25rem);\n  font-family: var(--rh-font-family-heading, RedHatDisplay, \"Red Hat Display\", \"Noto Sans Arabic\", \"Noto Sans Hebrew\", \"Noto Sans JP\", \"Noto Sans KR\", \"Noto Sans Malayalam\", \"Noto Sans SC\", \"Noto Sans TC\", \"Noto Sans Thai\", Helvetica, Arial, sans-serif);\n  font-weight: var(--rh-font-weight-heading-medium, 500);\n  line-height: var(--rh-line-height-heading, 1.3);\n  margin: 0 0 var(--rh-space-lg, 16px);\n  padding: 0;\n}\n\nrh-audio-player [slot=\"heading\"] {\n  font-size: var(--rh-font-size-body-text-md, 1rem);\n}\n\n",
      "hidden": true
    },
    "demo/customization/customization.js": {
      "content": "import '@rhds/elements/rh-audio-player/rh-audio-player.js';\nconst form = document.querySelector('form');\nconst player = document.querySelector('rh-audio-player');\nconst { poster } = player;\n\n/**\n * update audio player demo based on form selections\n */\nfunction updateDemo() {\n  const colorPalette = ['cyan', 'light'].includes(form.palette.value) ?\n    'light' : 'dark';\n  const colorClass =\n    ['cyan', 'purple', 'purple img'].includes(form.palette.value) ? form.palette.value : '';\n  player.poster = !form.poster.checked || form.palette.value === 'purple img' ? undefined : poster;\n  player.layout = form.layout.value !== '' ? form.layout.value : undefined;\n  if (colorPalette === player.colorPalette) {\n    const oldOn = player.colorPalette;\n    player.colorPalette = oldOn === 'dark' ? 'light' : 'dark';\n  }\n  player.setAttribute('class', colorClass);\n  player.colorPalette = colorPalette;\n  player.hasAccentColor = ['cyan', 'purple', 'purple img'].includes(form.palette.value);\n  player.requestUpdate();\n}\n\nif (form) {\n  form.addEventListener('input', updateDemo);\n  updateDemo();\n}\n",
      "hidden": true
    },
    "demo/detailed-transcript/index.html": {
      "contentType": "text/html",
      "selected": false,
      "content": "<h2>rh-audio-player: Detailed Transcript</h2>\n\n<!-- Options for demo -->\n<form>\n  <p>Options:</p>\n  <ul>\n    <li><label>Poster: <input name=\"poster\" type=\"checkbox\" checked=\"\"></label></li>\n    <li><label>Color Palette:\n        <select name=\"palette\">\n          <option value=\"light\" selected=\"\">Light</option>\n          <option value=\"dark\">Dark</option>\n          <option value=\"purple\">Purple</option>\n          <option value=\"purple-img\">Purple with Image</option>\n          <option value=\"cyan\">Cyan </option>\n        </select>\n      </label></li>\n    <li>\n      <label>Layout:\n        <select name=\"layout\">\n          <option value=\"full\" selected=\"\">Full</option>\n          <option value=\"compact-wide\">Compact Wide</option>\n          <option value=\"compact\">Compact</option>\n          <option value=\"\">Mini</option>\n        </select>\n      </label>\n    </li>\n  </ul>\n</form>\n\n<!-- Audio player demo will update via form -->\n<rh-audio-player layout=\"full\" poster=\"https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png\">\n  <p slot=\"series\">Code Comments</p>\n  <h3 slot=\"title\">Bringing Deep Learning to Enterprise Applications</h3>\n  <rh-audio-player-about slot=\"about\">\n    <h4 slot=\"heading\">About the episode</h4>\n    <p>\n      There are a lot of publicly available data sets out there. But when it\n      comes to specific enterprise use cases, you're not necessarily going to\n      able to find one to train your models. To realize the power of AI/ML in\n      enterprise environments, end users need an inference engine to run on\n      their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open\n      toolkits from Intel that do precisely that. He looks specifically at\n      anomaly detection in use cases as varied as medical imaging and\n      manufacturing.\n    </p>\n    <p>\n      Want to learn more about Anomalib? Check out the research paper that\n      introduces the deep learning library.\n    </p>\n    <rh-avatar slot=\"profile\" src=\"https://www.redhat.com/cms/managed-files/ryan-loney.png\">\n      Ryan Loney\n      <span slot=\"subtitle\">Product manager, OpenVINO Developer Tools, <em>IntelÂ®</em></span>\n    </rh-avatar>\n  </rh-audio-player-about>\n  <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n    <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3\">\n  </audio>\n  <rh-audio-player-subscribe slot=\"subscribe\">\n    <h4 slot=\"heading\">Subscribe</h4>\n    <p>Subscribe here:</p>\n    <a slot=\"link\" href=\"https://podcasts.apple.com/us/podcast/code-comments/id1649848507\" target=\"_blank\" title=\"Listen on Apple Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Apple Podcasts\" data-analytics-category=\"Hero|Listen on Apple Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg\" alt=\"Listen on Apple Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD\" target=\"_blank\" title=\"Listen on Spotify\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Spotify\" data-analytics-category=\"Hero|Listen on Spotify\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_spotify.svg\" alt=\"Listen on Spotify\">\n    </a>\n    <a slot=\"link\" href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw\" target=\"_blank\" title=\"Listen on Google Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Google Podcasts\" data-analytics-category=\"Hero|Listen on Google Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_google-podcast.svg\" alt=\"Listen on Google Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://feeds.pacific-content.com/codecomments\" target=\"_blank\" title=\"Subscribe via RSS Feed\" data-analytics-linktype=\"cta\" data-analytics-text=\"Subscribe via RSS Feed\" data-analytics-category=\"Hero|Subscribe via RSS Feed\">\n      <img class=\"img-fluid\" src=\"https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg\" alt=\"Subscribe via RSS Feed\">\n    </a>\n  </rh-audio-player-subscribe>\n  <rh-transcript slot=\"transcript\">\n    <rh-cue start=\"00:02\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"00:02\" end=\"00:04\">Hi, I'm Burr Sutter.</rh-cue>\n    <rh-cue start=\"00:04\" end=\"00:05\">I'm a Red Hatter</rh-cue>\n    <rh-cue start=\"00:05\" end=\"00:08\">who spends a lot of time talking to technologists about technologies.</rh-cue>\n    <rh-cue start=\"00:09\" end=\"00:10\">We say this a lot of Red Hat.</rh-cue>\n    <rh-cue start=\"00:10\" end=\"00:14\">No single technology provider holds the key to success, including us.</rh-cue>\n    <rh-cue start=\"00:15\" end=\"00:17\">And I would say the same thing about myself.</rh-cue>\n    <rh-cue start=\"00:17\" end=\"00:18\">I love to share ideas.</rh-cue>\n    <rh-cue start=\"00:18\" end=\"00:19\">So I thought it'd be awesome</rh-cue>\n    <rh-cue start=\"00:19\" end=\"00:22\">to talk to some brilliant technologists at Red Hat Partners.</rh-cue>\n    <rh-cue start=\"00:23\" end=\"00:26\">This is Code Comments, an original podcast</rh-cue>\n    <rh-cue start=\"00:26\" end=\"00:29\">from Red Hat.</rh-cue>\n    <rh-cue start=\"00:29\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"00:29\" end=\"00:32\">I'm sure, like many of you here, you have been thinking about</rh-cue>\n    <rh-cue start=\"00:32\" end=\"00:36\">AI, ML, artificial intelligence and machine learning.</rh-cue>\n    <rh-cue start=\"00:36\" end=\"00:38\">I've been thinking about that for quite some time</rh-cue>\n    <rh-cue start=\"00:38\" end=\"00:39\">and actually had the opportunity</rh-cue>\n    <rh-cue start=\"00:39\" end=\"00:43\">to work on a few successful projects here at Red Hat using those technologies,</rh-cue>\n    <rh-cue start=\"00:43\" end=\"00:46\">actually enabling a dataset, gathering a dataset,</rh-cue>\n    <rh-cue start=\"00:46\" end=\"00:49\">working with data scientists and data engineering team,</rh-cue>\n    <rh-cue start=\"00:49\" end=\"00:51\">and then training a model and putting that model into production</rh-cue>\n    <rh-cue start=\"00:51\" end=\"00:53\">runtime environment.</rh-cue>\n    <rh-cue start=\"00:53\" end=\"00:55\">It was an exciting set of projects and you can kind of see</rh-cue>\n    <rh-cue start=\"00:55\" end=\"00:58\">those on numerous YouTube videos I have published out there before.</rh-cue>\n    <rh-cue start=\"00:59\" end=\"01:01\">But I want you to think about the problem space a little bit</rh-cue>\n    <rh-cue start=\"01:01\" end=\"01:04\">because there are some interesting challenges about AI/ML.</rh-cue>\n    <rh-cue start=\"01:04\" end=\"01:06\">One is simply just getting access to the data,</rh-cue>\n    <rh-cue start=\"01:06\" end=\"01:09\">and while there are numerous publicly available datasets</rh-cue>\n    <rh-cue start=\"01:09\" end=\"01:12\">when it comes to your specific enterprise use case, you might not be to find</rh-cue>\n    <rh-cue start=\"01:12\" end=\"01:14\">publicly available data.</rh-cue>\n    <rh-cue start=\"01:14\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"01:14\" end=\"01:17\">In many cases, you cannot, even for our applications that we created,</rh-cue>\n    <rh-cue start=\"01:17\" end=\"01:20\">we had to create our dataset, capture our dataset,</rh-cue>\n    <rh-cue start=\"01:21\" end=\"01:24\">explore the dataset, and of course train a model accordingly.</rh-cue>\n    <rh-cue start=\"01:24\" end=\"01:27\">And we also found there's another challenge to be overcome</rh-cue>\n    <rh-cue start=\"01:27\" end=\"01:30\">in this AML world, and that is access to certain types of hardware.</rh-cue>\n    <rh-cue start=\"01:31\" end=\"01:33\">If you think about the enterprise environment</rh-cue>\n    <rh-cue start=\"01:33\" end=\"01:36\">and the creation of an enterprise application specifically for AML</rh-cue>\n    <rh-cue start=\"01:37\" end=\"01:40\">and users need an inference engine to run on their hardware,</rh-cue>\n    <rh-cue start=\"01:40\" end=\"01:43\">hardware that's available to them to be effective for their application.</rh-cue>\n    <rh-cue start=\"01:43\" end=\"01:45\">Let's say an application like computer vision,</rh-cue>\n    <rh-cue start=\"01:45\" end=\"01:49\">one that can detect anomalies in medical imaging or maybe on a factory floor,</rh-cue>\n    <rh-cue start=\"01:49\" end=\"01:52\">You know, those things are whizzing by on the factory line.</rh-cue>\n    <rh-cue start=\"01:52\" end=\"01:55\">They're looking at them and trying to determine if there is an error or not.</rh-cue>\n    <rh-cue start=\"01:56\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"01:56\" end=\"01:58\">Well, how do you actually make it run on your hardware,</rh-cue>\n    <rh-cue start=\"01:58\" end=\"02:01\">your accessible technology that you have today?</rh-cue>\n    <rh-cue start=\"02:01\" end=\"02:05\">Well, there's a solution for this as an open toolkit called Open vino.</rh-cue>\n    <rh-cue start=\"02:05\" end=\"02:07\">And you might be thinking, hey, wait a minute,</rh-cue>\n    <rh-cue start=\"02:07\" end=\"02:10\">don't you need a GPU for a I inferencing a GPU</rh-cue>\n    <rh-cue start=\"02:10\" end=\"02:12\">for artificial intelligence machine learning?</rh-cue>\n    <rh-cue start=\"02:12\" end=\"02:15\">Well, not according to Ryan Loney, product manager of Open Vino Developer</rh-cue>\n    <rh-cue start=\"02:15\" end=\"02:16\">Tools at Intel.</rh-cue>\n    <rh-cue start=\"02:20\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"02:20\" end=\"02:20\">I guess we'll</rh-cue>\n    <rh-cue start=\"02:20\" end=\"02:23\">start with trying to maybe dispel the myths, right?</rh-cue>\n    <rh-cue start=\"02:23\" end=\"02:27\">I think that CPUs are widely used for inference today.</rh-cue>\n    <rh-cue start=\"02:27\" end=\"02:32\">So and if we look at the data center segment, you know, about 70% of the A.I.</rh-cue>\n    <rh-cue start=\"02:32\" end=\"02:36\">inference is happening on Intel Xeon on our data center CPUs.</rh-cue>\n    <rh-cue start=\"02:36\" end=\"02:40\">And so you don't needed a GPU, especially for running inference.</rh-cue>\n    <rh-cue start=\"02:40\" end=\"02:43\">And that's part of the value of open vino, is that we're you know,</rh-cue>\n    <rh-cue start=\"02:43\" end=\"02:47\">we're taking models that may have been trained on a GPU</rh-cue>\n    <rh-cue start=\"02:47\" end=\"02:50\">using deep learning frameworks like PyTorch or TensorFlow</rh-cue>\n    <rh-cue start=\"02:51\" end=\"02:54\">and then optimizing them to run on Intel hardware.</rh-cue>\n    <rh-cue start=\"02:57\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"02:56\" end=\"03:00\">Ryan joined me to discuss AI/ML and the enterprise</rh-cue>\n    <rh-cue start=\"03:00\" end=\"03:03\">across various industries and exploring numerous use cases.</rh-cue>\n    <rh-cue start=\"03:05\" end=\"03:08\">Let's talk a little bit about the origin story behind Open Vino.</rh-cue>\n    <rh-cue start=\"03:08\" end=\"03:10\">Tell us more about it and how it came to be</rh-cue>\n    <rh-cue start=\"03:10\" end=\"03:13\">and why it came out of Intel.</rh-cue>\n    <rh-cue start=\"03:12\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"03:12\" end=\"03:16\">Definitely. So we had the first release of Open Vino</rh-cue>\n    <rh-cue start=\"03:16\" end=\"03:20\">was back in 2018, so still relatively new.</rh-cue>\n    <rh-cue start=\"03:20\" end=\"03:25\">And at that time we were focused on computer vision and pretty tightly coupled</rh-cue>\n    <rh-cue start=\"03:25\" end=\"03:31\">with open CV, which is another open source library with origins at Intel.</rh-cue>\n    <rh-cue start=\"03:31\" end=\"03:31\">You know, it</rh-cue>\n    <rh-cue start=\"03:31\" end=\"03:36\">had its first release back in 1999, so it's been around a little bit longer.</rh-cue>\n    <rh-cue start=\"03:36\" end=\"03:40\">And many of the software engineers and architects at Intel</rh-cue>\n    <rh-cue start=\"03:40\" end=\"03:45\">that were involved with and contributing to open CV are working on open Vino.</rh-cue>\n    <rh-cue start=\"03:45\" end=\"03:49\">So you can think of open vino as complementary software to open CV.</rh-cue>\n    <rh-cue start=\"03:50\" end=\"03:53\">And we're providing like an engine for executing inference</rh-cue>\n    <rh-cue start=\"03:53\" end=\"03:57\">as part of a computer vision pipeline, or at least that's how we started.</rh-cue>\n    <rh-cue start=\"03:58\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"03:58\" end=\"04:01\">But since 2018, we've we've started to move beyond</rh-cue>\n    <rh-cue start=\"04:01\" end=\"04:02\">just computer vision inference.</rh-cue>\n    <rh-cue start=\"04:02\" end=\"04:05\">So when I say computer vision inference, I mean like image</rh-cue>\n    <rh-cue start=\"04:05\" end=\"04:08\">classification, object detection, segmentation.</rh-cue>\n    <rh-cue start=\"04:09\" end=\"04:12\">And now we're moving into natural language processing, things</rh-cue>\n    <rh-cue start=\"04:12\" end=\"04:16\">like speech synthesis, speech recognition, knowledge, graphs,</rh-cue>\n    <rh-cue start=\"04:17\" end=\"04:21\">time series forecasting, and other use cases that don't involve</rh-cue>\n    <rh-cue start=\"04:21\" end=\"04:24\">computer vision and don't involve inference on pixels.</rh-cue>\n    <rh-cue start=\"04:25\" end=\"04:28\">Our latest release, the 20 22.1 that came out earlier this year,</rh-cue>\n    <rh-cue start=\"04:29\" end=\"04:32\">there was a most significant update that we've had to open vino</rh-cue>\n    <rh-cue start=\"04:32\" end=\"04:36\">since we started in 2018, and the major focus of that release</rh-cue>\n    <rh-cue start=\"04:36\" end=\"04:40\">was optimizing for use cases that go beyond computer vision.</rh-cue>\n    <rh-cue start=\"04:41\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"04:41\" end=\"04:44\">And I like that concept that you just mentioned right there, computer vision.</rh-cue>\n    <rh-cue start=\"04:44\" end=\"04:47\">And you said that you extended those use cases and went beyond that.</rh-cue>\n    <rh-cue start=\"04:47\" end=\"04:50\">So could you give us more concrete examples of computer vision?</rh-cue>\n    <rh-cue start=\"04:50\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"04:50\" end=\"04:50\">Yeah, sure.</rh-cue>\n    <rh-cue start=\"04:50\" end=\"04:55\">So when you think about manufacturing quality control in factories, everything</rh-cue>\n    <rh-cue start=\"04:55\" end=\"05:01\">from ARC welding, defect detection to inspecting BMW cars on assembly lines,</rh-cue>\n    <rh-cue start=\"05:02\" end=\"05:05\">they're using cameras or sensors to collect data.</rh-cue>\n    <rh-cue start=\"05:05\" end=\"05:11\">And usually it's cameras collecting images like RGV images that you and I can see.</rh-cue>\n    <rh-cue start=\"05:11\" end=\"05:14\">And looks like something taken from a camera or video camera,</rh-cue>\n    <rh-cue start=\"05:15\" end=\"05:19\">but also things like infrared or computerized tomography</rh-cue>\n    <rh-cue start=\"05:19\" end=\"05:24\">scans used in health care, X-ray, different types of images where we can</rh-cue>\n    <rh-cue start=\"05:25\" end=\"05:28\">draw bounding boxes around regions of interest</rh-cue>\n    <rh-cue start=\"05:29\" end=\"05:32\">and say, you know, this is a defect or this is not a defect.</rh-cue>\n    <rh-cue start=\"05:32\" end=\"05:37\">And also, is this worker wearing a safety hat or did they forget to put it on?</rh-cue>\n    <rh-cue start=\"05:37\" end=\"05:41\">And so you can take this and integrate it into a pipeline</rh-cue>\n    <rh-cue start=\"05:41\" end=\"05:44\">where you're triggering an alert if somebody forgets</rh-cue>\n    <rh-cue start=\"05:44\" end=\"05:49\">to wear their safety mask or if there's a defect in a product</rh-cue>\n    <rh-cue start=\"05:49\" end=\"05:53\">on an assembly line, you can just use cameras and open</rh-cue>\n    <rh-cue start=\"05:53\" end=\"05:58\">vino and open CV running these on Intel hardware and help to analyze.</rh-cue>\n    <rh-cue start=\"05:58\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"05:58\" end=\"06:01\">And that's what a lot of the partners that we work with are doing.</rh-cue>\n    <rh-cue start=\"06:01\" end=\"06:03\">So these independent software vendors</rh-cue>\n    <rh-cue start=\"06:03\" end=\"06:06\">and there's other use cases for things like retail.</rh-cue>\n    <rh-cue start=\"06:06\" end=\"06:10\">You think about going to a store and using an automated checkout system.</rh-cue>\n    <rh-cue start=\"06:11\" end=\"06:13\">You know, sometimes people use those automated checkouts</rh-cue>\n    <rh-cue start=\"06:13\" end=\"06:17\">and they they slide a few extra items into their bag that they don't scan.</rh-cue>\n    <rh-cue start=\"06:17\" end=\"06:21\">And it's a huge loss for the retail outlets</rh-cue>\n    <rh-cue start=\"06:21\" end=\"06:25\">that are providing this way to check out real time shelf monitoring.</rh-cue>\n    <rh-cue start=\"06:25\" end=\"06:29\">We have this bear on one of our is fees that helps keep store shelves</rh-cue>\n    <rh-cue start=\"06:29\" end=\"06:33\">stocked by just analyzing the cameras in the stores, detecting</rh-cue>\n    <rh-cue start=\"06:33\" end=\"06:37\">when objects are missing from the shelves so that they can be restocked.</rh-cue>\n    <rh-cue start=\"06:37\" end=\"06:41\">We have Vistry, another ISP that works with quick service restaurants.</rh-cue>\n    <rh-cue start=\"06:41\" end=\"06:44\">So when you think about automating the process of</rh-cue>\n    <rh-cue start=\"06:44\" end=\"06:48\">when do I drop the fries into the fryer so that they're warm</rh-cue>\n    <rh-cue start=\"06:48\" end=\"06:50\">when the car gets to the drive thru window,</rh-cue>\n    <rh-cue start=\"06:50\" end=\"06:54\">you know, there's quite a bit of industrial health care retail examples</rh-cue>\n    <rh-cue start=\"06:54\" end=\"06:57\">that we can walk through and we should dig into some more of those.</rh-cue>\n    <rh-cue start=\"06:57\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"06:57\" end=\"06:59\">But I got to tell you, I have I have a personal experience</rh-cue>\n    <rh-cue start=\"06:59\" end=\"06:59\">in this category</rh-cue>\n    <rh-cue start=\"06:59\" end=\"07:01\">that I want to share with, and you can tell me how</rh-cue>\n    <rh-cue start=\"07:01\" end=\"07:03\">how silly you might think at this point in time.</rh-cue>\n    <rh-cue start=\"07:03\" end=\"07:04\">It is.</rh-cue>\n    <rh-cue start=\"07:04\" end=\"07:08\">We actually built an AI keynote demonstration for the Red Hat big stage</rh-cue>\n    <rh-cue start=\"07:08\" end=\"07:12\">back in 2015, and I really want to illustrate the concept of asset tracking.</rh-cue>\n    <rh-cue start=\"07:12\" end=\"07:15\">So we actually gave everybody in the conference a little Bluetooth token,</rh-cue>\n    <rh-cue start=\"07:16\" end=\"07:19\">but a little battery, a little watch battery and a little Bluetooth emitter.</rh-cue>\n    <rh-cue start=\"07:19\" end=\"07:22\">And we basically tracked those things around the conference.</rh-cue>\n    <rh-cue start=\"07:22\" end=\"07:24\">We basically put a Raspberry Pi in each of the meeting rooms</rh-cue>\n    <rh-cue start=\"07:24\" end=\"07:26\">and up in the lunch room, and you could see how the tokens</rh-cue>\n    <rh-cue start=\"07:26\" end=\"07:30\">moved from room to room to room as a relatively simple application.</rh-cue>\n    <rh-cue start=\"07:30\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"07:30\" end=\"07:32\">But it occurred to me after we figured out,</rh-cue>\n    <rh-cue start=\"07:32\" end=\"07:34\">okay, how to do that with Bluetooth and triangulating</rh-cue>\n    <rh-cue start=\"07:34\" end=\"07:39\">Bluetooth signals by looking at relative signal strength from one radio to another</rh-cue>\n    <rh-cue start=\"07:39\" end=\"07:42\">and putting that through an Apache Spark application at the time,</rh-cue>\n    <rh-cue start=\"07:42\" end=\"07:45\">we then realized, you know what, this is easier done with cameras</rh-cue>\n    <rh-cue start=\"07:45\" end=\"07:49\">and just simply looking at a camera and having some form of animal</rh-cue>\n    <rh-cue start=\"07:49\" end=\"07:51\">or machine learning model that would say, Oh,</rh-cue>\n    <rh-cue start=\"07:51\" end=\"07:53\">there are people here now are there are no people here now.</rh-cue>\n    <rh-cue start=\"07:53\" end=\"07:55\">What do you think about that?</rh-cue>\n    <rh-cue start=\"07:55\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"07:55\" end=\"07:59\">Yeah, I mean, what you just described is sort of exactly that the product</rh-cue>\n    <rh-cue start=\"07:59\" end=\"08:02\">that either one of our partners is offering,</rh-cue>\n    <rh-cue start=\"08:02\" end=\"08:04\">you know, that they're doing it with computer vision and cameras.</rh-cue>\n    <rh-cue start=\"08:04\" end=\"08:08\">So when partner tries to help retail stores</rh-cue>\n    <rh-cue start=\"08:08\" end=\"08:12\">analyze the foot traffic and understand with Heatmaps,</rh-cue>\n    <rh-cue start=\"08:12\" end=\"08:16\">where people are spending the most time in stores, how many people are coming</rh-cue>\n    <rh-cue start=\"08:16\" end=\"08:19\">in, what size groups are coming into the store,</rh-cue>\n    <rh-cue start=\"08:19\" end=\"08:23\">you know, and trying to help understand if there was a successful transaction</rh-cue>\n    <rh-cue start=\"08:23\" end=\"08:27\">from the people who entered the store and left the store so that you can,</rh-cue>\n    <rh-cue start=\"08:27\" end=\"08:30\">you know, to help with the, you know, retail analytics</rh-cue>\n    <rh-cue start=\"08:30\" end=\"08:33\">and marketing sales and positioning of products.</rh-cue>\n    <rh-cue start=\"08:34\" end=\"08:37\">And so they're doing that in a way that also protects privacy.</rh-cue>\n    <rh-cue start=\"08:37\" end=\"08:38\">And that's something that's really important.</rh-cue>\n    <rh-cue start=\"08:38\" end=\"08:41\">So when you talked about those Bluetooth beacons, probably,</rh-cue>\n    <rh-cue start=\"08:41\" end=\"08:44\">\n      you know, if everyone who walked into a grocery store was asked\n    </rh-cue>\n    <rh-cue start=\"08:44\" end=\"08:49\">\n      to put a tracking device in their cart or on their person and say, you know,\n    </rh-cue>\n    <rh-cue start=\"08:49\" end=\"08:50\">\n      you're going\n    </rh-cue>\n    <rh-cue start=\"08:50\" end=\"08:53\">\n      to be tracked around the store, they probably wouldn't want to do that.\n    </rh-cue>\n    <rh-cue start=\"08:53\" end=\"08:56\">\n      The way that you can do this with cameras is you can,\n    </rh-cue>\n    <rh-cue start=\"08:53\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"08:56\" end=\"09:01\">\n      you know, detect people as they enter and, you know, remove their face.\n    </rh-cue>\n    <rh-cue start=\"09:01\" end=\"09:01\">\n      Right.\n    </rh-cue>\n    <rh-cue start=\"09:01\" end=\"09:05\">\n      So you can ignore any biometric information\n    </rh-cue>\n    <rh-cue start=\"09:05\" end=\"09:08\">\n      and and just track the person based on pixels\n    </rh-cue>\n    <rh-cue start=\"09:09\" end=\"09:12\">\n      that are present in the detected region of interest.\n    </rh-cue>\n    <rh-cue start=\"09:12\" end=\"09:15\">\n      So they're able to analyze, say, a family walks in the door\n    </rh-cue>\n    <rh-cue start=\"09:16\" end=\"09:20\">\n      and they can group those people together with object detection\n    </rh-cue>\n    <rh-cue start=\"09:20\" end=\"09:23\">\n      and then they can track their movement throughout the store\n    </rh-cue>\n    <rh-cue start=\"09:23\" end=\"09:26\">\n      without keeping track of their face or any biometric\n    </rh-cue>\n    <rh-cue start=\"09:26\" end=\"09:30\">\n      or any personal identifiable information to avoid things like bias\n    </rh-cue>\n    <rh-cue start=\"09:30\" end=\"09:35\">\n      and to make sure that they're protecting the privacy of the shoppers in the store\n    </rh-cue>\n    <rh-cue start=\"09:35\" end=\"09:39\">\n      while still getting that really useful marketing analytics data rate\n    </rh-cue>\n    <rh-cue start=\"09:39\" end=\"09:42\">\n      so that they can make better decisions about where to place their products.\n    </rh-cue>\n    <rh-cue start=\"09:42\" end=\"09:45\">\n      So that's one really good example of how\n    </rh-cue>\n    <rh-cue start=\"09:45\" end=\"09:48\">\n      computer vision AI with open vino is being used today.\n    </rh-cue>\n    <rh-cue start=\"09:48\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"09:48\" end=\"09:51\">\n      And that is a great example because you're definitely spot on.\n    </rh-cue>\n    <rh-cue start=\"09:51\" end=\"09:53\">\n      It is invasive when you hand someone to Bluetooth devices,\n    </rh-cue>\n    <rh-cue start=\"09:53\" end=\"09:56\">\n      say, please keep this with you as you go throughout our our store\n    </rh-cue>\n    <rh-cue start=\"09:56\" end=\"09:59\">\n      or our mall or throughout our hospital, wherever you might be.\n    </rh-cue>\n    <rh-cue start=\"09:59\" end=\"10:01\">\n      Now, you mentioned another example earlier\n    </rh-cue>\n    <rh-cue start=\"10:01\" end=\"10:03\">\n      in the conversation which was related to like worker safety.\n    </rh-cue>\n    <rh-cue start=\"10:03\" end=\"10:05\">\n      Are they wearing a helmet?\n    </rh-cue>\n    <rh-cue start=\"10:05\" end=\"10:08\">\n      I want to talk more about that concept in a real industrial setting,\n    </rh-cue>\n    <rh-cue start=\"10:08\" end=\"10:11\">\n      a manufacturing setting where there might be a factory floor\n    </rh-cue>\n    <rh-cue start=\"10:11\" end=\"10:13\">\n      and there are certain requirements, or better yet, there's like a\n    </rh-cue>\n    <rh-cue start=\"10:13\" end=\"10:16\">\n      a quality assurance requirement, let's say, when it comes to looking\n    </rh-cue>\n    <rh-cue start=\"10:16\" end=\"10:20\">\n      at a factory line, I run to that use case often what some of our customers.\n    </rh-cue>\n    <rh-cue start=\"10:20\" end=\"10:22\">\n      Can you talk more about those kinds of use cases? Yeah.\n    </rh-cue>\n    <rh-cue start=\"10:22\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"10:22\" end=\"10:27\">\n      So one of our partners, Robuchon and we you know, published a case study\n    </rh-cue>\n    <rh-cue start=\"10:27\" end=\"10:31\">\n      I think last year where they're working with BMW at one of their factories\n    </rh-cue>\n    <rh-cue start=\"10:32\" end=\"10:35\">\n      and they do quality control inspection, but they're also doing\n    </rh-cue>\n    <rh-cue start=\"10:35\" end=\"10:38\">\n      things related to worker safety and analyzing.\n    </rh-cue>\n    <rh-cue start=\"10:38\" end=\"10:40\">\n      You know, I used the safety had example.\n    </rh-cue>\n    <rh-cue start=\"10:40\" end=\"10:45\">\n      There's a number of of our ISP's and partners who have similar use cases.\n    </rh-cue>\n    <rh-cue start=\"10:45\" end=\"10:48\">\n      And it comes down to there's a few reasons\n    </rh-cue>\n    <rh-cue start=\"10:48\" end=\"10:51\">\n      that are motivating this and some are related to like insurance, right?\n    </rh-cue>\n    <rh-cue start=\"10:51\" end=\"10:53\">\n      It's important to make sure that\n    </rh-cue>\n    <rh-cue start=\"10:53\" end=\"10:56\">\n      if you want to have your factory insured and that your workers\n    </rh-cue>\n    <rh-cue start=\"10:56\" end=\"10:58\">\n      are protecting themselves and wearing the gear.\n    </rh-cue>\n    <rh-cue start=\"10:58\" end=\"11:00\">\n      Regulatory compliance. Right.\n    </rh-cue>\n    <rh-cue start=\"11:00\" end=\"11:05\">\n      You're you're being asked to properly protect from exposure to chemicals or,\n    </rh-cue>\n    <rh-cue start=\"11:05\" end=\"11:09\">\n      you know, potentially having something fall and and hit someone on the head.\n    </rh-cue>\n    <rh-cue start=\"11:09\" end=\"11:13\">\n      So wearing a safety vest, wearing goggles, wearing a helmet,\n    </rh-cue>\n    <rh-cue start=\"11:14\" end=\"11:17\">\n      these are things that you need to do inside the factory.\n    </rh-cue>\n    <rh-cue start=\"11:17\" end=\"11:21\">\n      And you can really easily automate and detect and sometimes without bias.\n    </rh-cue>\n    <rh-cue start=\"11:21\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"11:21\" end=\"11:26\">\n      I think that's one of the interesting things about the robots on BMW example\n    </rh-cue>\n    <rh-cue start=\"11:26\" end=\"11:31\">\n      is that they were also blurring sort of blocking out and so drawing a box\n    </rh-cue>\n    <rh-cue start=\"11:31\" end=\"11:35\">\n      to cover the face of the workers in the factory\n    </rh-cue>\n    <rh-cue start=\"11:35\" end=\"11:38\">\n      so that somebody who was analyzing the video footage\n    </rh-cue>\n    <rh-cue start=\"11:38\" end=\"11:43\">\n      and getting the alerts saying that, hey, you know, Bay 21 has a worker\n    </rh-cue>\n    <rh-cue start=\"11:43\" end=\"11:47\">\n      without a hat on, that it's not sending their face\n    </rh-cue>\n    <rh-cue start=\"11:47\" end=\"11:50\">\n      and in the alert and potentially, you know, invading\n    </rh-cue>\n    <rh-cue start=\"11:50\" end=\"11:54\">\n      or going against privacy laws or just the ethics of the company.\n    </rh-cue>\n    <rh-cue start=\"11:54\" end=\"11:54\">\n      Right.\n    </rh-cue>\n    <rh-cue start=\"11:54\" end=\"11:58\">\n      They don't want to introduce bias or have people targeted because\n    </rh-cue>\n    <rh-cue start=\"11:58\" end=\"12:02\">\n      it's much better to to have it be, you know, blur the face\n    </rh-cue>\n    <rh-cue start=\"12:02\" end=\"12:06\">\n      and alert and have somebody take care of it on the floor.\n    </rh-cue>\n    <rh-cue start=\"12:06\" end=\"12:09\">\n      And then if you ever need to audit that information later,\n    </rh-cue>\n    <rh-cue start=\"12:09\" end=\"12:12\">\n      they have a way to do it where people who need to be able to see\n    </rh-cue>\n    <rh-cue start=\"12:12\" end=\"12:17\">\n      who the employee was and look up their personal information, they can do that.\n    </rh-cue>\n    <rh-cue start=\"12:17\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"12:17\" end=\"12:20\">\n      But then just for the purposes of maintaining safety,\n    </rh-cue>\n    <rh-cue start=\"12:20\" end=\"12:21\">\n      they don't need to have access\n    </rh-cue>\n    <rh-cue start=\"12:21\" end=\"12:24\">\n      to that personal information or biometric information,\n    </rh-cue>\n    <rh-cue start=\"12:25\" end=\"12:28\">\n      because that's one thing that when you hear about computer vision\n    </rh-cue>\n    <rh-cue start=\"12:28\" end=\"12:31\">\n      or object person tracking, object detection,\n    </rh-cue>\n    <rh-cue start=\"12:32\" end=\"12:36\">\n      there's a lot of concern, and rightfully so, about privacy\n    </rh-cue>\n    <rh-cue start=\"12:36\" end=\"12:40\">\n      being invaded and about tracking information, face ID,\n    </rh-cue>\n    <rh-cue start=\"12:41\" end=\"12:45\">\n      identifying people who may have committed crimes through video footage.\n    </rh-cue>\n    <rh-cue start=\"12:45\" end=\"12:48\">\n      And that's just not something that a lot of companies want to\n    </rh-cue>\n    <rh-cue start=\"12:49\" end=\"12:51\">\n      you know, they want to protect privacy\n    </rh-cue>\n    <rh-cue start=\"12:51\" end=\"12:52\">\n      and they don't they don't want to be in a situation\n    </rh-cue>\n    <rh-cue start=\"12:52\" end=\"12:55\">\n      where they might be violating someone's rights.\n    </rh-cue>\n    <rh-cue start=\"12:56\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"12:56\" end=\"12:58\">\n      Well, privacy is certainly opening up Pandora's box.\n    </rh-cue>\n    <rh-cue start=\"12:58\" end=\"13:00\">\n      There's a lot to be explored in that area,\n    </rh-cue>\n    <rh-cue start=\"13:00\" end=\"13:02\">\n      especially in a digital world that we now live in.\n    </rh-cue>\n    <rh-cue start=\"13:02\" end=\"13:05\">\n      But for now, let's move on and explore different area.\n    </rh-cue>\n    <rh-cue start=\"13:05\" end=\"13:08\">\n      I'm interested in how machines and computers offer advantages,\n    </rh-cue>\n    <rh-cue start=\"13:08\" end=\"13:12\">\n      specifically in certain use cases like a quality control scenario.\n    </rh-cue>\n    <rh-cue start=\"13:12\" end=\"13:15\">\n      I asked Ryan to explain how AML and specifically machines\n    </rh-cue>\n    <rh-cue start=\"13:15\" end=\"13:18\">\n      computers can augment that capability.\n    </rh-cue>\n    <rh-cue start=\"13:19\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"13:19\" end=\"13:22\">\n      I can give a specific example where we have a partner\n    </rh-cue>\n    <rh-cue start=\"13:22\" end=\"13:25\">\n      that's there doing defect detection with\n    </rh-cue>\n    <rh-cue start=\"13:25\" end=\"13:28\">\n      and looking for anomalies in batteries.\n    </rh-cue>\n    <rh-cue start=\"13:28\" end=\"13:31\">\n      So, you know, sure, you've heard there's a lot of interest right now\n    </rh-cue>\n    <rh-cue start=\"13:31\" end=\"13:34\">\n      in electric vehicles, a lot of batteries being produced.\n    </rh-cue>\n    <rh-cue start=\"13:34\" end=\"13:36\">\n      And so if you go into one of these factories,\n    </rh-cue>\n    <rh-cue start=\"13:36\" end=\"13:40\">\n      they have images that they collect of every battery that's going through this\n    </rh-cue>\n    <rh-cue start=\"13:40\" end=\"13:44\">\n      assembly line and through these images, people\n    </rh-cue>\n    <rh-cue start=\"13:44\" end=\"13:47\">\n      can look and see and visually inspect with their eyes and say,\n    </rh-cue>\n    <rh-cue start=\"13:48\" end=\"13:50\">\n      this battery has a defect, send it back.\n    </rh-cue>\n    <rh-cue start=\"13:50\" end=\"13:53\">\n      And that's one step in the quality control process.\n    </rh-cue>\n    <rh-cue start=\"13:53\" end=\"13:58\">\n      And there's other steps, I'm sure, like running diagnostic tests and, you know,\n    </rh-cue>\n    <rh-cue start=\"13:58\" end=\"14:02\">\n      measuring voltage and doing other types of non-visual inspection.\n    </rh-cue>\n    <rh-cue start=\"14:02\" end=\"14:06\">\n      But for the visual inspection piece where you can really easily identify\n    </rh-cue>\n    <rh-cue start=\"14:06\" end=\"14:10\">\n      some problems, it's much more efficient to introduce computer vision.\n    </rh-cue>\n    <rh-cue start=\"14:11\" end=\"14:14\">\n      And so that's where we have this new library that we've introduced\n    </rh-cue>\n    <rh-cue start=\"14:14\" end=\"14:17\">\n      called Anomali, that's open vino.\n    </rh-cue>\n    <rh-cue start=\"14:17\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"14:17\" end=\"14:20\">\n      While we're focused on inference, you know, we're also thinking\n    </rh-cue>\n    <rh-cue start=\"14:20\" end=\"14:25\">\n      about the pipeline or the funnel that gets these models to open vino.\n    </rh-cue>\n    <rh-cue start=\"14:25\" end=\"14:28\">\n      And so we've we've invested in this anomaly segmentation,\n    </rh-cue>\n    <rh-cue start=\"14:28\" end=\"14:32\">\n      anomaly detection library and that we've recently open source\n    </rh-cue>\n    <rh-cue start=\"14:32\" end=\"14:35\">\n      and there's a great research paper about it about Anomali.\n    </rh-cue>\n    <rh-cue start=\"14:36\" end=\"14:39\">\n      But the idea is you can take just a few images\n    </rh-cue>\n    <rh-cue start=\"14:39\" end=\"14:43\">\n      and train a model and start detecting these defects.\n    </rh-cue>\n    <rh-cue start=\"14:43\" end=\"14:46\">\n      And so for this battery example, that's a more advanced example.\n    </rh-cue>\n    <rh-cue start=\"14:46\" end=\"14:52\">\n      But to make it simpler, you know, take some bolts and, you know, take ten bolts.\n    </rh-cue>\n    <rh-cue start=\"14:52\" end=\"14:55\">\n      You have one that has a scratch on it or one that is chipped\n    </rh-cue>\n    <rh-cue start=\"14:56\" end=\"14:58\">\n      or has some damage to it.\n    </rh-cue>\n    <rh-cue start=\"14:58\" end=\"15:00\">\n      And you can easily get started in training\n    </rh-cue>\n    <rh-cue start=\"15:00\" end=\"15:03\">\n      to recognize the bolts that do not have an anomaly.\n    </rh-cue>\n    <rh-cue start=\"15:04\" end=\"15:06\">\n      And the ones that do, which is a small data set\n    </rh-cue>\n    <rh-cue start=\"15:06\" end=\"15:10\">\n      and I think that's really one of the most important things today.\n    </rh-cue>\n    <rh-cue start=\"15:11\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"15:11\" end=\"15:14\">\n      Challenges is one is access to data, but the other is\n    </rh-cue>\n    <rh-cue start=\"15:14\" end=\"15:17\">\n      is needing a massive amount of data to do something meaningful.\n    </rh-cue>\n    <rh-cue start=\"15:18\" end=\"15:22\">\n      And so we're starting to try to change that dynamic with Anomali.\n    </rh-cue>\n    <rh-cue start=\"15:22\" end=\"15:27\">\n      So you may not need 100,000 images, you may need 100 images,\n    </rh-cue>\n    <rh-cue start=\"15:27\" end=\"15:33\">\n      and you can start detecting anomalies in everything from batteries to bolts to,\n    </rh-cue>\n    <rh-cue start=\"15:33\" end=\"15:37\">\n      you know, maybe even the wood varnish use case that you mentioned.\n    </rh-cue>\n    <rh-cue start=\"15:37\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"15:37\" end=\"15:40\">\n      That is very key point because often in that data scientist\n    </rh-cue>\n    <rh-cue start=\"15:40\" end=\"15:43\">\n      process, that data engineer and data scientist process, right.\n    </rh-cue>\n    <rh-cue start=\"15:43\" end=\"15:44\">\n      The one key thing is can you gather\n    </rh-cue>\n    <rh-cue start=\"15:44\" end=\"15:47\">\n      the data that you need for the input for the model training?\n    </rh-cue>\n    <rh-cue start=\"15:47\" end=\"15:49\">\n      And we've often sat at least people I've worked\n    </rh-cue>\n    <rh-cue start=\"15:49\" end=\"15:52\">\n      with over the last couple of years, you know, you need a lot of data.\n    </rh-cue>\n    <rh-cue start=\"15:52\" end=\"15:55\">\n      You need tens of thousands of correct images\n    </rh-cue>\n    <rh-cue start=\"15:55\" end=\"15:58\">\n      so we can sort out the difference between dogs versus cats, let's say,\n    </rh-cue>\n    <rh-cue start=\"15:58\" end=\"16:01\">\n      or you need dozens and dozens of situations\n    </rh-cue>\n    <rh-cue start=\"16:01\" end=\"16:03\">\n      where if it's a natural language processing scenario,\n    </rh-cue>\n    <rh-cue start=\"16:03\" end=\"16:06\">\n      you know, a good customer interaction, a good customer conversation,\n    </rh-cue>\n    <rh-cue start=\"16:06\" end=\"16:07\">\n      and in this case,\n    </rh-cue>\n    <rh-cue start=\"16:07\" end=\"16:11\">\n      it sounds like what you're saying is show us just the bad things, right?\n    </rh-cue>\n    <rh-cue start=\"16:11\" end=\"16:14\">\n      Fewer images, fewer incorrect things,\n    </rh-cue>\n    <rh-cue start=\"16:14\" end=\"16:17\">\n      and then let us look for those kind of anomalies.\n    </rh-cue>\n    <rh-cue start=\"16:18\" end=\"16:20\">\n      Can tell us more about that because that is very interesting.\n    </rh-cue>\n    <rh-cue start=\"16:20\" end=\"16:23\">\n      The concept that I can use a much smaller dataset as my input\n    </rh-cue>\n    <rh-cue start=\"16:23\" end=\"16:26\">\n      as opposed to gathering terabytes of data in some cases\n    </rh-cue>\n    <rh-cue start=\"16:26\" end=\"16:29\">\n      to just simply get my model training underway.\n    </rh-cue>\n    <rh-cue start=\"16:30\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"16:30\" end=\"16:34\">\n      You know, like you described, the idea is if you have some good images\n    </rh-cue>\n    <rh-cue start=\"16:34\" end=\"16:37\">\n      and then you have some of the the known defects\n    </rh-cue>\n    <rh-cue start=\"16:38\" end=\"16:41\">\n      and you can just label here's a set of good images\n    </rh-cue>\n    <rh-cue start=\"16:41\" end=\"16:44\">\n      and here's a few of the defects and you can right away\n    </rh-cue>\n    <rh-cue start=\"16:44\" end=\"16:48\">\n      start detecting those specific defects that you've identified.\n    </rh-cue>\n    <rh-cue start=\"16:48\" end=\"16:49\">\n      And then also, you know, be able to\n    </rh-cue>\n    <rh-cue start=\"16:50\" end=\"16:53\">\n      determine when it doesn't match\n    </rh-cue>\n    <rh-cue start=\"16:53\" end=\"16:57\">\n      the expected appearance of a non defective item.\n    </rh-cue>\n    <rh-cue start=\"16:57\" end=\"17:00\">\n      So if I have the undamaged screw and then I introduce\n    </rh-cue>\n    <rh-cue start=\"17:00\" end=\"17:03\">\n      one with some new anomaly that's never been seen before,\n    </rh-cue>\n    <rh-cue start=\"17:04\" end=\"17:07\">\n      I can say, you know, this one is not a valid screw.\n    </rh-cue>\n    <rh-cue start=\"17:07\" end=\"17:11\">\n      And so that's sort of the the approach that we're taking.\n    </rh-cue>\n    <rh-cue start=\"17:11\" end=\"17:15\">\n      And it's really important because so often you need to have\n    </rh-cue>\n    <rh-cue start=\"17:15\" end=\"17:19\">\n      subject matter experts often like if you think the take the battery example,\n    </rh-cue>\n    <rh-cue start=\"17:20\" end=\"17:23\">\n      there's these workers who are on the floor\n    </rh-cue>\n    <rh-cue start=\"17:23\" end=\"17:27\">\n      in a factory and they're the ones who know best when they look at these images,\n    </rh-cue>\n    <rh-cue start=\"17:28\" end=\"17:31\">\n      which one's going to have an issue, which one's defective?\n    </rh-cue>\n    <rh-cue start=\"17:31\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"17:31\" end=\"17:34\">\n      And then they also need to take that subject matter, expertise\n    </rh-cue>\n    <rh-cue start=\"17:35\" end=\"17:38\">\n      and then use it to annotate data sets.\n    </rh-cue>\n    <rh-cue start=\"17:38\" end=\"17:39\">\n      And when you have these, you know,\n    </rh-cue>\n    <rh-cue start=\"17:39\" end=\"17:43\">\n      tens of thousands of images you need to annotate, it's asking those people\n    </rh-cue>\n    <rh-cue start=\"17:43\" end=\"17:47\">\n      to stop working on the factory floor so they can come annotate some images.\n    </rh-cue>\n    <rh-cue start=\"17:47\" end=\"17:49\">\n      That's a tough business call to make, right?\n    </rh-cue>\n    <rh-cue start=\"17:49\" end=\"17:53\">\n      But if you only need them to annotate a handful of images, it's a much easier\n    </rh-cue>\n    <rh-cue start=\"17:53\" end=\"17:56\">\n      ask to get the ball rolling and demonstrate value.\n    </rh-cue>\n    <rh-cue start=\"17:56\" end=\"17:59\">\n      And maybe over time you will want to annotate more\n    </rh-cue>\n    <rh-cue start=\"17:59\" end=\"18:03\">\n      and more images because you'll get even better accuracy in the model.\n    </rh-cue>\n    <rh-cue start=\"18:03\" end=\"18:07\">\n      Even better, even if it's just small incremental improvements.\n    </rh-cue>\n    <rh-cue start=\"18:08\" end=\"18:11\">\n      You know, that's something that if it generates value for the business,\n    </rh-cue>\n    <rh-cue start=\"18:11\" end=\"18:14\">\n      it's something the business will invest in over time.\n    </rh-cue>\n    <rh-cue start=\"18:14\" end=\"18:17\">\n      But you have to convince the decision makers that it's worth\n    </rh-cue>\n    <rh-cue start=\"18:17\" end=\"18:22\">\n      the time of these subject matter experts to stop what they're doing\n    </rh-cue>\n    <rh-cue start=\"18:22\" end=\"18:26\">\n      and go and label some images of the things that they're working on in the factory.\n    </rh-cue>\n    <rh-cue start=\"18:26\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"18:26\" end=\"18:30\">\n      And that labeling process can be very labor intensive of the annotations,\n    </rh-cue>\n    <rh-cue start=\"18:30\" end=\"18:33\">\n      basically saying what is correct, what's wrong, what is this, what is that?\n    </rh-cue>\n    <rh-cue start=\"18:33\" end=\"18:36\">\n      And therefore, if we can minimize that time frame to get the value quicker,\n    </rh-cue>\n    <rh-cue start=\"18:36\" end=\"18:40\">\n      then there's something that's useful for the business, useful for the organization\n    </rh-cue>\n    <rh-cue start=\"18:40\" end=\"18:41\">\n      long before we necessarily good.\n    </rh-cue>\n    <rh-cue start=\"18:41\" end=\"18:43\">\n      There are huge model training based,\n    </rh-cue>\n    <rh-cue start=\"18:49\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"18:49\" end=\"18:52\">\n      so we talk about labeling and how that is labor intensive activity.\n    </rh-cue>\n    <rh-cue start=\"18:52\" end=\"18:54\">\n      But I love the idea of helping the human\n    </rh-cue>\n    <rh-cue start=\"18:54\" end=\"18:57\">\n      and helping the human models specifically not get bored.\n    </rh-cue>\n    <rh-cue start=\"18:57\" end=\"19:01\">\n      Basically, if the human is eyeballing a bunch of widgets flying by over time,\n    </rh-cue>\n    <rh-cue start=\"19:01\" end=\"19:03\">\n      they make mistakes, they get bored\n    </rh-cue>\n    <rh-cue start=\"19:03\" end=\"19:06\">\n      and they don't pay as close attention as they should.\n    </rh-cue>\n    <rh-cue start=\"19:07\" end=\"19:10\">\n      That's why the concept of Amazon specifically computer vision, augmenting\n    </rh-cue>\n    <rh-cue start=\"19:10\" end=\"19:14\">\n      that capability and really helping the human identify anomalies faster,\n    </rh-cue>\n    <rh-cue start=\"19:14\" end=\"19:17\">\n      more quickly, maybe with greater accuracy could be a big win.\n    </rh-cue>\n    <rh-cue start=\"19:18\" end=\"19:21\">\n      We focused on manufacturing, but let's actually go into health care\n    </rh-cue>\n    <rh-cue start=\"19:21\" end=\"19:24\">\n      and learn how these tools can be used in that sector and that industry.\n    </rh-cue>\n    <rh-cue start=\"19:24\" end=\"19:28\">\n      Ryan talked to me about how Open Windows runtime can be incorporated into medical\n    </rh-cue>\n    <rh-cue start=\"19:28\" end=\"19:32\">\n      imaging equipment with intel processors and better than c.T.\n    </rh-cue>\n    <rh-cue start=\"19:32\" end=\"19:34\">\n      MRI and ultrasound machines.\n    </rh-cue>\n    <rh-cue start=\"19:34\" end=\"19:37\">\n      Well, these inferences, this AML workload can be operating\n    </rh-cue>\n    <rh-cue start=\"19:37\" end=\"19:41\">\n      and executing right there in the same physical room as the patient.\n    </rh-cue>\n    <rh-cue start=\"19:44\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"19:44\" end=\"19:46\">\n      We did a presentation when she last year.\n    </rh-cue>\n    <rh-cue start=\"19:46\" end=\"19:47\">\n      I think they said\n    </rh-cue>\n    <rh-cue start=\"19:47\" end=\"19:50\">\n      there's at least 80 countries that have their X-ray machines deployed\n    </rh-cue>\n    <rh-cue start=\"19:51\" end=\"19:56\">\n      and they're doing things like helping doctors place breathing tubes in patients.\n    </rh-cue>\n    <rh-cue start=\"19:56\" end=\"20:01\">\n      So during COVID, during the pandemic, that was a really important tool\n    </rh-cue>\n    <rh-cue start=\"20:01\" end=\"20:05\">\n      to help with nurses and doctors who were intubating patients\n    </rh-cue>\n    <rh-cue start=\"20:05\" end=\"20:09\">\n      sometimes like in a parking lot or a hallway of the hospital.\n    </rh-cue>\n    <rh-cue start=\"20:09\" end=\"20:14\">\n      And, you know, when they had a statistic that you said, I think one out of four\n    </rh-cue>\n    <rh-cue start=\"20:14\" end=\"20:17\">\n      breathing tubes gets placed incorrectly\n    </rh-cue>\n    <rh-cue start=\"20:17\" end=\"20:19\">\n      when you're doing it outside the operating room,\n    </rh-cue>\n    <rh-cue start=\"20:19\" end=\"20:22\">\n      because when you're in an operating room, it's much more controlled\n    </rh-cue>\n    <rh-cue start=\"20:22\" end=\"20:24\">\n      and there's someone who's an expert at placing the tubes.\n    </rh-cue>\n    <rh-cue start=\"20:24\" end=\"20:28\">\n      It's something you have more of a controlled environment\n    </rh-cue>\n    <rh-cue start=\"20:28\" end=\"20:31\">\n      than when you're out in a parking lot, in a tent.\n    </rh-cue>\n    <rh-cue start=\"20:31\" end=\"20:34\">\n      You know, when the hospital's completely full and you're triaging patients\n    </rh-cue>\n    <rh-cue start=\"20:34\" end=\"20:37\">\n      with COVID, that's when they're more likely to make mistakes.\n    </rh-cue>\n    <rh-cue start=\"20:37\" end=\"20:40\">\n      And so they had this endotracheal tube placement\n    </rh-cue>\n    <rh-cue start=\"20:42\" end=\"20:43\">\n      model that they trained,\n    </rh-cue>\n    <rh-cue start=\"20:43\" end=\"20:47\">\n      and it helped to use an x ray and give an alert and say, hey,\n    </rh-cue>\n    <rh-cue start=\"20:47\" end=\"20:50\">\n      this tube is placed wrong, pull it out and do it again.\n    </rh-cue>\n    <rh-cue start=\"20:50\" end=\"20:53\">\n      And so things like that help doctors so that they can avoid mistakes.\n    </rh-cue>\n    <rh-cue start=\"20:54\" end=\"20:57\">\n      And, you know, having a breathing tube placed incorrectly\n    </rh-cue>\n    <rh-cue start=\"20:57\" end=\"21:01\">\n      can cause collapsed lung and a number of other unwanted side effects.\n    </rh-cue>\n    <rh-cue start=\"21:01\" end=\"21:03\">\n      So it's really important to do it correctly.\n    </rh-cue>\n    <rh-cue start=\"21:03\" end=\"21:06\">\n      Another example is Samsung Medicine.\n    </rh-cue>\n    <rh-cue start=\"21:06\" end=\"21:10\">\n      They actually are doing estimating fetal angle of progression.\n    </rh-cue>\n    <rh-cue start=\"21:10\" end=\"21:13\">\n      So this is analyzing ultrasound\n    </rh-cue>\n    <rh-cue start=\"21:14\" end=\"21:18\">\n      of pregnant women with that, being able to to help take measurements\n    </rh-cue>\n    <rh-cue start=\"21:18\" end=\"21:22\">\n      that are usually hard to calculate that can be done in an automated way.\n    </rh-cue>\n    <rh-cue start=\"21:22\" end=\"21:26\">\n      They're already taking the ultrasound scan and now they're executing this model.\n    </rh-cue>\n    <rh-cue start=\"21:26\" end=\"21:31\">\n      They can take some of these measurements to help the doctor avoid potentially more\n    </rh-cue>\n    <rh-cue start=\"21:31\" end=\"21:34\">\n      intrusive alternative methods so the patient wins.\n    </rh-cue>\n    <rh-cue start=\"21:35\" end=\"21:36\">\n      It makes their life better.\n    </rh-cue>\n    <rh-cue start=\"21:36\" end=\"21:39\">\n      And the doctors is getting help from this A.I.\n    </rh-cue>\n    <rh-cue start=\"21:39\" end=\"21:40\">\n      model.\n    </rh-cue>\n    <rh-cue start=\"21:40\" end=\"21:42\">\n      And those are, you know, just a few examples.\n    </rh-cue>\n    <rh-cue start=\"21:42\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"21:42\" end=\"21:45\">\n      Those are some amazing examples when it comes to all these things.\n    </rh-cue>\n    <rh-cue start=\"21:45\" end=\"21:49\">\n      We're talking like CT scans, right, and x rays, other examples of computer vision.\n    </rh-cue>\n    <rh-cue start=\"21:49\" end=\"21:52\">\n      One thing that's kind of interesting in the space, I think\n    </rh-cue>\n    <rh-cue start=\"21:52\" end=\"21:56\">\n      whenever I get a chance to work on, let's say an object traction model\n    </rh-cue>\n    <rh-cue start=\"21:56\" end=\"21:58\">\n      and one of our workshops, by the way, is actually putting that out\n    </rh-cue>\n    <rh-cue start=\"21:58\" end=\"22:01\">\n      in front of people to say, Hey, look, you can use your phone.\n    </rh-cue>\n    <rh-cue start=\"22:01\" end=\"22:04\">\n      And it basically sends the image over to our OpenShift, right,\n    </rh-cue>\n    <rh-cue start=\"22:04\" end=\"22:07\">\n      with our data science platform and then analyzes what you see.\n    </rh-cue>\n    <rh-cue start=\"22:08\" end=\"22:09\">\n      And even in my case, where I take a picture of my dog\n    </rh-cue>\n    <rh-cue start=\"22:09\" end=\"22:13\">\n      as an example, it can't really decide is it a dog or a cat?\n    </rh-cue>\n    <rh-cue start=\"22:13\" end=\"22:15\">\n      I have a very funny looking dog,\n    </rh-cue>\n    <rh-cue start=\"22:15\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"22:15\" end=\"22:18\">\n      and so there's always a percentage outcome, you know?\n    </rh-cue>\n    <rh-cue start=\"22:18\" end=\"22:21\">\n      In other words, I think it's a dog 52%.\n    </rh-cue>\n    <rh-cue start=\"22:21\" end=\"22:22\">\n      So I want to talk about that more.\n    </rh-cue>\n    <rh-cue start=\"22:22\" end=\"22:25\">\n      What how important is it to get to 100% accuracy?\n    </rh-cue>\n    <rh-cue start=\"22:25\" end=\"22:29\">\n      How important is it to really, depending on the use case, to allow\n    </rh-cue>\n    <rh-cue start=\"22:29\" end=\"22:34\">\n      for the gray area, if you will, where it's an 80% accuracy or 70% accuracy?\n    </rh-cue>\n    <rh-cue start=\"22:34\" end=\"22:36\">\n      And where are the trade offs there associated with the application?\n    </rh-cue>\n    <rh-cue start=\"22:36\" end=\"22:38\">\n      Can you can you discuss that more?\n    </rh-cue>\n    <rh-cue start=\"22:38\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"22:38\" end=\"22:40\">\n      Accuracy is definitely, you know, a touchy subject\n    </rh-cue>\n    <rh-cue start=\"22:40\" end=\"22:43\">\n      because how you measure it makes a huge difference.\n    </rh-cue>\n    <rh-cue start=\"22:43\" end=\"22:46\">\n      And then I think with like what you were describing with the dog example, there's\n    </rh-cue>\n    <rh-cue start=\"22:46\" end=\"22:51\">\n      sort of a top five potential classes that might may be identified.\n    </rh-cue>\n    <rh-cue start=\"22:51\" end=\"22:55\">\n      So let's say you're doing object detection and you detect a region of interest\n    </rh-cue>\n    <rh-cue start=\"22:55\" end=\"22:57\">\n      and it says 65% confidence.\n    </rh-cue>\n    <rh-cue start=\"22:57\" end=\"22:58\">\n      This is a dog.\n    </rh-cue>\n    <rh-cue start=\"22:58\" end=\"23:03\">\n      Well, the next potential label that could be maybe 50% confidence\n    </rh-cue>\n    <rh-cue start=\"23:03\" end=\"23:08\">\n      or 20% confidence might be something similar to a dog or in the case of models\n    </rh-cue>\n    <rh-cue start=\"23:08\" end=\"23:11\">\n      that have been trained on like the image net dataset\n    </rh-cue>\n    <rh-cue start=\"23:11\" end=\"23:15\">\n      or on cocoa data set, they have like actual breeds of dogs.\n    </rh-cue>\n    <rh-cue start=\"23:15\" end=\"23:20\">\n      So if I want to look at the top five labels for a dog,\n    </rh-cue>\n    <rh-cue start=\"23:20\" end=\"23:24\">\n      for my dog, for example, she's a mixed mostly Labrador retriever.\n    </rh-cue>\n    <rh-cue start=\"23:24\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"23:24\" end=\"23:29\">\n      But I may look at the top five labels and it may say 65% confidence that she's\n    </rh-cue>\n    <rh-cue start=\"23:29\" end=\"23:34\">\n      a flat coated retriever and then confidence that she's a husky,\n    </rh-cue>\n    <rh-cue start=\"23:34\" end=\"23:39\">\n      as you know, 20% and then 5% confidence that she's a Greyhound or something.\n    </rh-cue>\n    <rh-cue start=\"23:40\" end=\"23:42\">\n      Those labels, all of them are dogs.\n    </rh-cue>\n    <rh-cue start=\"23:42\" end=\"23:45\">\n      So if I'm just trying to figure out is, is this a dog,\n    </rh-cue>\n    <rh-cue start=\"23:45\" end=\"23:50\">\n      I could probably find all of the, you know, classes within the data set\n    </rh-cue>\n    <rh-cue start=\"23:50\" end=\"23:53\">\n      and say, well, these are all, you know, class ID\n    </rh-cue>\n    <rh-cue start=\"23:53\" end=\"24:00\">\n      65, 132, 92 and 158 all belong to a group of dogs.\n    </rh-cue>\n    <rh-cue start=\"24:00\" end=\"24:04\">\n      So if I wanted to just write an application to tell me if this is a dog\n    </rh-cue>\n    <rh-cue start=\"24:04\" end=\"24:07\">\n      or not, I would probably use that to determine if it's a dog.\n    </rh-cue>\n    <rh-cue start=\"24:08\" end=\"24:10\">\n      But how you measure that is accuracy.\n    </rh-cue>\n    <rh-cue start=\"24:10\" end=\"24:11\">\n      Well, that's where it gets a little bit complicated,\n    </rh-cue>\n    <rh-cue start=\"24:11\" end=\"24:15\">\n      because if you're being really strict about the definition and you're\n    </rh-cue>\n    <rh-cue start=\"24:15\" end=\"24:18\">\n      trying to validate against the data set of labeled images\n    </rh-cue>\n    <rh-cue start=\"24:18\" end=\"24:22\">\n      and I have specific dog breeds or some specific detail\n    </rh-cue>\n    <rh-cue start=\"24:22\" end=\"24:25\">\n      and it doesn't match, well, then the accuracy is going to go down.\n    </rh-cue>\n    <rh-cue start=\"24:25\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"24:25\" end=\"24:29\">\n      That's especially important when we talk about things like compression\n    </rh-cue>\n    <rh-cue start=\"24:29\" end=\"24:30\">\n      and quantization,\n    </rh-cue>\n    <rh-cue start=\"24:30\" end=\"24:34\">\n      which, you know, historically has been difficult for to get adoption\n    </rh-cue>\n    <rh-cue start=\"24:34\" end=\"24:40\">\n      in some domains like health care, where even the hint of accuracy going down\n    </rh-cue>\n    <rh-cue start=\"24:40\" end=\"24:44\">\n      implies that we're not going to be able to help in some small case,\n    </rh-cue>\n    <rh-cue start=\"24:44\" end=\"24:47\">\n      maybe if it's even half a percent of the time\n    </rh-cue>\n    <rh-cue start=\"24:47\" end=\"24:51\">\n      we want to take that that tube is placed incorrectly or that, you know,\n    </rh-cue>\n    <rh-cue start=\"24:51\" end=\"24:54\">\n      that patient's, you know, lung has collapsed or something like that.\n    </rh-cue>\n    <rh-cue start=\"24:54\" end=\"24:58\">\n      And that's something that really prevents adoption of some of these methods\n    </rh-cue>\n    <rh-cue start=\"24:58\" end=\"25:01\">\n      that can really boost performance like quantization.\n    </rh-cue>\n    <rh-cue start=\"25:01\" end=\"25:05\">\n      But if you take that example of sort of different from the dog example\n    </rh-cue>\n    <rh-cue start=\"25:05\" end=\"25:07\">\n      and you think about like segmentation of kidneys.\n    </rh-cue>\n    <rh-cue start=\"25:07\" end=\"25:11\">\n      So if I'm doing kidney segmentation, which is, you know, taking a CT scan\n    </rh-cue>\n    <rh-cue start=\"25:12\" end=\"25:14\">\n      and then trying to pick the pixels out of that\n    </rh-cue>\n    <rh-cue start=\"25:14\" end=\"25:17\">\n      scan that belong to a kidney,\n    </rh-cue>\n    <rh-cue start=\"25:17\" end=\"25:20\">\n      how I measure accuracy may be\n    </rh-cue>\n    <rh-cue start=\"25:20\" end=\"25:24\">\n      how many of those pixels I'm able to detect and how many did I miss?\n    </rh-cue>\n    <rh-cue start=\"25:25\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"25:25\" end=\"25:29\">\n      Missing some of the pixels is maybe not a problem, right,\n    </rh-cue>\n    <rh-cue start=\"25:29\" end=\"25:33\">\n      depending on how you built the application because you still detect the kidney\n    </rh-cue>\n    <rh-cue start=\"25:34\" end=\"25:38\">\n      and maybe you just need to apply padding around the region of interest\n    </rh-cue>\n    <rh-cue start=\"25:38\" end=\"25:41\">\n      so that you don't miss any of the the actual kidney.\n    </rh-cue>\n    <rh-cue start=\"25:42\" end=\"25:45\">\n      When you compress the model and when you quantized the model. But\n    </rh-cue>\n    <rh-cue start=\"25:45\" end=\"25:50\">\n      that requires, you know, data scientist and email engineer somebody to really\n    </rh-cue>\n    <rh-cue start=\"25:51\" end=\"25:52\">\n      they have to be\n    </rh-cue>\n    <rh-cue start=\"25:52\" end=\"25:55\">\n      able to go and apply that after the fact, after the inference\n    </rh-cue>\n    <rh-cue start=\"25:55\" end=\"25:59\">\n      happens to make sure that you're not losing critical information,\n    </rh-cue>\n    <rh-cue start=\"25:59\" end=\"26:02\">\n      because the next step from detecting the kidney may be detecting a tumor.\n    </rh-cue>\n    <rh-cue start=\"26:03\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"26:03\" end=\"26:06\">\n      And so maybe you can use the more optimized model\n    </rh-cue>\n    <rh-cue start=\"26:06\" end=\"26:11\">\n      to detect the kidney, but then you can use a slower model to detect the tumor.\n    </rh-cue>\n    <rh-cue start=\"26:11\" end=\"26:15\">\n      But that also requires somebody to architect and make that decision\n    </rh-cue>\n    <rh-cue start=\"26:15\" end=\"26:16\">\n      or that tradeoff and say,\n    </rh-cue>\n    <rh-cue start=\"26:16\" end=\"26:20\">\n      well, I need to add padding, or I should only use the quantized model\n    </rh-cue>\n    <rh-cue start=\"26:20\" end=\"26:24\">\n      to detect the region of interest for the kidney and then use the model\n    </rh-cue>\n    <rh-cue start=\"26:24\" end=\"26:27\">\n      that takes longer to do the inference\n    </rh-cue>\n    <rh-cue start=\"26:27\" end=\"26:30\">\n      just to find the tumor, which is going to be on a smaller size.\n    </rh-cue>\n    <rh-cue start=\"26:30\" end=\"26:33\">\n      Right. The dimensions are going to be much smaller\n    </rh-cue>\n    <rh-cue start=\"26:33\" end=\"26:35\">\n      once we crop to the region of interest.\n    </rh-cue>\n    <rh-cue start=\"26:35\" end=\"26:40\">\n      But all of those details, that's maybe not easy to explain in a few sentences.\n    </rh-cue>\n    <rh-cue start=\"26:40\" end=\"26:43\">\n      And even the way I explained it is probably really confusing.\n    </rh-cue>\n    <rh-cue start=\"26:45\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"26:45\" end=\"26:46\">\n      I do love that use case.\n    </rh-cue>\n    <rh-cue start=\"26:46\" end=\"26:47\">\n      Like you mentioned, the cropping\n    </rh-cue>\n    <rh-cue start=\"26:47\" end=\"26:50\">\n      even in one such an area that we worked on for another project,\n    </rh-cue>\n    <rh-cue start=\"26:50\" end=\"26:53\">\n      we specifically decided to pix like the image that we had taken\n    </rh-cue>\n    <rh-cue start=\"26:53\" end=\"26:57\">\n      because we knew that we could get the outcome we wanted by even\n    </rh-cue>\n    <rh-cue start=\"26:57\" end=\"27:01\">\n      just using a smaller or less having less resolution in our image.\n    </rh-cue>\n    <rh-cue start=\"27:01\" end=\"27:04\">\n      And therefore, as we transferred it from the mobile device storage device\n    </rh-cue>\n    <rh-cue start=\"27:04\" end=\"27:08\">\n      up into the cloud, we wanted that smaller image just for transfer purposes\n    </rh-cue>\n    <rh-cue start=\"27:08\" end=\"27:11\">\n      and it still we could get the accuracy we needed by a lot of testing.\n    </rh-cue>\n    <rh-cue start=\"27:11\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"27:11\" end=\"27:14\">\n      And one thing that's interesting about that from my perspective is\n    </rh-cue>\n    <rh-cue start=\"27:15\" end=\"27:18\">\n      if you're doing image processing, sometimes it takes a while\n    </rh-cue>\n    <rh-cue start=\"27:18\" end=\"27:20\">\n      for this transaction to occur.\n    </rh-cue>\n    <rh-cue start=\"27:20\" end=\"27:20\">\n      Like I,\n    </rh-cue>\n    <rh-cue start=\"27:20\" end=\"27:24\">\n      I come from a traditional application background, you know, where I'm reading\n    </rh-cue>\n    <rh-cue start=\"27:24\" end=\"27:25\">\n      and writing things from a database\n    </rh-cue>\n    <rh-cue start=\"27:25\" end=\"27:28\">\n      or a message broker or moving data from one place to another.\n    </rh-cue>\n    <rh-cue start=\"27:28\" end=\"27:29\">\n      Those things happen subsequent.\n    </rh-cue>\n    <rh-cue start=\"27:29\" end=\"27:33\">\n      Normally, even with great latency between your data centers, you know,\n    </rh-cue>\n    <rh-cue start=\"27:33\" end=\"27:34\">\n      it's still subsequent.\n    </rh-cue>\n    <rh-cue start=\"27:34\" end=\"27:38\">\n      In most cases, while on a transaction like this, one can actually take 2 seconds\n    </rh-cue>\n    <rh-cue start=\"27:38\" end=\"27:42\">\n      or 4 seconds as it's doing its analysis and actually coming back with, you know,\n    </rh-cue>\n    <rh-cue start=\"27:42\" end=\"27:46\">\n      I think it's a dog, I think it's a kidney, I think it's whatever, and provided me\n    </rh-cue>\n    <rh-cue start=\"27:46\" end=\"27:48\">\n      that accuracy statement.\n    </rh-cue>\n    <rh-cue start=\"27:48\" end=\"27:51\">\n      So that concept of optimization is very important\n    </rh-cue>\n    <rh-cue start=\"27:51\" end=\"27:53\">\n      in the overall application architecture.\n    </rh-cue>\n    <rh-cue start=\"27:53\" end=\"27:56\">\n      Would you agree with that or how do you think about that concept?\n    </rh-cue>\n    <rh-cue start=\"27:56\" end=\"27:56\">\n      Yeah, definitely.\n    </rh-cue>\n    <rh-cue start=\"27:56\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"27:56\" end=\"27:58\">\n      It depends too on the use case.\n    </rh-cue>\n    <rh-cue start=\"27:58\" end=\"28:02\">\n      So if you think about how important it is to reduce the latency\n    </rh-cue>\n    <rh-cue start=\"28:02\" end=\"28:06\">\n      and increase the number of frames per second that you can process when you're\n    </rh-cue>\n    <rh-cue start=\"28:06\" end=\"28:10\">\n      talking about a loss prevention model that's running at a grocery store.\n    </rh-cue>\n    <rh-cue start=\"28:10\" end=\"28:13\">\n      So you want to keep the lines moving.\n    </rh-cue>\n    <rh-cue start=\"28:13\" end=\"28:16\">\n      You don't want every person who's at the self-checkout\n    </rh-cue>\n    <rh-cue start=\"28:16\" end=\"28:19\">\n      to have to wait 5 seconds for every item they scan.\n    </rh-cue>\n    <rh-cue start=\"28:19\" end=\"28:22\">\n      You need it to happen as quickly as possible.\n    </rh-cue>\n    <rh-cue start=\"28:22\" end=\"28:25\">\n      And if sometimes you, you know, the accuracy\n    </rh-cue>\n    <rh-cue start=\"28:25\" end=\"28:28\">\n      decreases slightly or the I'd say the accuracy of the whole pipeline.\n    </rh-cue>\n    <rh-cue start=\"28:28\" end=\"28:32\">\n      So not just looking at the individual model or the individual inference, but\n    </rh-cue>\n    <rh-cue start=\"28:32\" end=\"28:36\">\n      let's say that the the whole pipeline is not as successful at detecting\n    </rh-cue>\n    <rh-cue start=\"28:37\" end=\"28:40\">\n      when somebody steals one item from the self-checkout,\n    </rh-cue>\n    <rh-cue start=\"28:41\" end=\"28:43\">\n      it's not going to be a life threatening situation.\n    </rh-cue>\n    <rh-cue start=\"28:43\" end=\"28:47\">\n      Whereas, you know, being in the hooked up to the X-ray machine\n    </rh-cue>\n    <rh-cue start=\"28:47\" end=\"28:51\">\n      with the two placement model, they might be willing to have the doctor,\n    </rh-cue>\n    <rh-cue start=\"28:51\" end=\"28:54\">\n      the nurse wait 5 seconds to get the result.\n    </rh-cue>\n    <rh-cue start=\"28:55\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"28:55\" end=\"28:58\">\n      They don't need it to happen in 500 milliseconds.\n    </rh-cue>\n    <rh-cue start=\"28:58\" end=\"29:02\">\n      So they're willing their threshold for waiting is a little bit higher.\n    </rh-cue>\n    <rh-cue start=\"29:02\" end=\"29:05\">\n      So that, I think, also drives some of the decision, like\n    </rh-cue>\n    <rh-cue start=\"29:06\" end=\"29:09\">\n      you want to keep people moving through the checkout line\n    </rh-cue>\n    <rh-cue start=\"29:09\" end=\"29:13\">\n      and you can afford to to potentially if you lose a little bit of accuracy here\n    </rh-cue>\n    <rh-cue start=\"29:13\" end=\"29:14\">\n      and there, it's not going to\n    </rh-cue>\n    <rh-cue start=\"29:14\" end=\"29:18\">\n      cost the company that much money or it's not going to be life threatening.\n    </rh-cue>\n    <rh-cue start=\"29:18\" end=\"29:21\">It's going to be worth the tradeoff of keeping the line moving</rh-cue>\n    <rh-cue start=\"29:21\" end=\"29:24\">and not having people leave the store and not check out at all.</rh-cue>\n    <rh-cue start=\"29:24\" end=\"29:27\">And to say, I'm not going to shop today because the line's too long.</rh-cue>\n    <rh-cue start=\"29:30\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"29:30\" end=\"29:32\">There are so many trade offs and enterprise</rh-cue>\n    <rh-cue start=\"29:32\" end=\"29:35\">AML use cases, things like latency, accuracy and availability.</rh-cue>\n    <rh-cue start=\"29:35\" end=\"29:40\">And certainly complexities abound, especially in an obviously ever evolving</rh-cue>\n    <rh-cue start=\"29:40\" end=\"29:43\">technological landscape where we are still very early in the adoption of AML.</rh-cue>\n    <rh-cue start=\"29:44\" end=\"29:47\">And to navigate that complexity, the direct feedback from real world</rh-cue>\n    <rh-cue start=\"29:47\" end=\"29:51\">end users is essential to Ryan and his team at Intel.</rh-cue>\n    <rh-cue start=\"29:52\" end=\"29:54\">What would you say are some of the big hurdles or big</rh-cue>\n    <rh-cue start=\"29:54\" end=\"29:57\">outcomes, big opportunities in that space?</rh-cue>\n    <rh-cue start=\"29:57\" end=\"30:01\">And do you agree that we're kind of still at the very beginning in our infancy,</rh-cue>\n    <rh-cue start=\"30:01\" end=\"30:01\">if you will,</rh-cue>\n    <rh-cue start=\"30:01\" end=\"30:05\">of adopting these technologies and and discovering what they can do for us?</rh-cue>\n    <rh-cue start=\"30:05\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"30:05\" end=\"30:07\">Yeah, I think we're definitely in the infancy</rh-cue>\n    <rh-cue start=\"30:07\" end=\"30:10\">and I think that what we've seen is our customers are evolving</rh-cue>\n    <rh-cue start=\"30:10\" end=\"30:14\">and the people who are deploying on Intel hardware, they're trying to run</rh-cue>\n    <rh-cue start=\"30:14\" end=\"30:16\">more complicated models.</rh-cue>\n    <rh-cue start=\"30:16\" end=\"30:19\">They're the models that are doing object detection or, you know,</rh-cue>\n    <rh-cue start=\"30:19\" end=\"30:22\">detecting defects and, you know, doing segmentation.</rh-cue>\n    <rh-cue start=\"30:23\" end=\"30:27\">You know, in the past you could say, oh, here's a generic model that will do face</rh-cue>\n    <rh-cue start=\"30:27\" end=\"30:31\">detection or person detection or vehicle detection and license plate detection.</rh-cue>\n    <rh-cue start=\"30:32\" end=\"30:33\">And those are sort of like</rh-cue>\n    <rh-cue start=\"30:33\" end=\"30:36\">general purpose models that you can just grab off the shelf and use them.</rh-cue>\n    <rh-cue start=\"30:37\" end=\"30:40\">But now we're moving into like the anomaly scenarios</rh-cue>\n    <rh-cue start=\"30:40\" end=\"30:44\">where I've got my own data and I'm trying to do something very specific</rh-cue>\n    <rh-cue start=\"30:45\" end=\"30:47\">and I'm the only one that has access to this data.</rh-cue>\n    <rh-cue start=\"30:47\" end=\"30:51\">And you don't have a public data set that you can go download</rh-cue>\n    <rh-cue start=\"30:51\" end=\"30:54\">that's under Creative Commons license for, you know, car batteries.</rh-cue>\n    <rh-cue start=\"30:54\" end=\"30:57\">It's, you know, it's just not something that's available.</rh-cue>\n    <rh-cue start=\"30:57\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"30:57\" end=\"31:02\">And so those use cases, the challenge with with training those models</rh-cue>\n    <rh-cue start=\"31:02\" end=\"31:06\">and and getting them optimized is the beginning of the pipeline.</rh-cue>\n    <rh-cue start=\"31:06\" end=\"31:10\">It's the data you have to get the data you have to annotated</rh-cue>\n    <rh-cue start=\"31:10\" end=\"31:12\">and the tools have to exist for you to do that.</rh-cue>\n    <rh-cue start=\"31:12\" end=\"31:15\">And that's part of the problem that we're trying to help solve.</rh-cue>\n    <rh-cue start=\"31:16\" end=\"31:17\">And then the models are getting more complex.</rh-cue>\n    <rh-cue start=\"31:17\" end=\"31:21\">So if you think, you know, just from working with customers recently,</rh-cue>\n    <rh-cue start=\"31:21\" end=\"31:22\">you know, they're no longer</rh-cue>\n    <rh-cue start=\"31:22\" end=\"31:26\">just trying to do image classification and, you know, like is it a dog or a cat?</rh-cue>\n    <rh-cue start=\"31:26\" end=\"31:29\">They've moved on to like 3D point clouds</rh-cue>\n    <rh-cue start=\"31:29\" end=\"31:34\">and, you know, 3D segmentation models and things that are like the speech</rh-cue>\n    <rh-cue start=\"31:34\" end=\"31:39\">synthesis example, doing things these GPT models that are generating,</rh-cue>\n    <rh-cue start=\"31:40\" end=\"31:44\">you know, you, you put a text input and it generates an image for you.</rh-cue>\n    <rh-cue start=\"31:44\" end=\"31:47\">It's just becoming much more advanced, much more sophisticated</rh-cue>\n    <rh-cue start=\"31:48\" end=\"31:50\">and on larger images.</rh-cue>\n    <rh-cue start=\"31:50\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"31:50\" end=\"31:54\">And so things like running super resolution enhancing images, upscaling</rh-cue>\n    <rh-cue start=\"31:54\" end=\"31:59\">images, instead of just trying to take that, you know, 200 by 200 pixel</rh-cue>\n    <rh-cue start=\"32:00\" end=\"32:02\">image and classifying if it's a cat.</rh-cue>\n    <rh-cue start=\"32:02\" end=\"32:05\">Now we're talking about gigantic</rh-cue>\n    <rh-cue start=\"32:05\" end=\"32:09\">huge images that we're processing and that all requires</rh-cue>\n    <rh-cue start=\"32:09\" end=\"32:12\">more resources or more optimized models.</rh-cue>\n    <rh-cue start=\"32:13\" end=\"32:16\">And, you know, every computer vision conference or A.I.</rh-cue>\n    <rh-cue start=\"32:16\" end=\"32:19\">conference, there's there's a new latest and greatest architecture.</rh-cue>\n    <rh-cue start=\"32:19\" end=\"32:22\">There's new research paper, and things are getting adopted much faster.</rh-cue>\n    <rh-cue start=\"32:23\" end=\"32:27\">The lead time for a nurse paper or CV PR</rh-cue>\n    <rh-cue start=\"32:27\" end=\"32:30\">for a company to actually adopt and put those into production.</rh-cue>\n    <rh-cue start=\"32:30\" end=\"32:32\">It's like the time shortens every year.</rh-cue>\n    <rh-cue start=\"32:33\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"32:33\" end=\"32:35\">Well, Ryan, I got to tell you, I could talk to you</rh-cue>\n    <rh-cue start=\"32:35\" end=\"32:39\">literally all day about these topics, the various use cases, the various ways</rh-cue>\n    <rh-cue start=\"32:39\" end=\"32:41\">models are being optimized,</rh-cue>\n    <rh-cue start=\"32:41\" end=\"32:44\">how to put models into a pipeline for average enterprise applications.</rh-cue>\n    <rh-cue start=\"32:44\" end=\"32:47\">I've enjoyed learning about pop and vino and anomalies,</rh-cue>\n    <rh-cue start=\"32:47\" end=\"32:50\">but I'm fascinated by this because I will have a chance to go try this myself.</rh-cue>\n    <rh-cue start=\"32:51\" end=\"32:52\">Taking advantage of Red Hat OpenShift</rh-cue>\n    <rh-cue start=\"32:52\" end=\"32:54\">and taking advantage of our data science platform.</rh-cue>\n    <rh-cue start=\"32:54\" end=\"32:58\">On top of that, I will definitely go be poking at this myself.</rh-cue>\n    <rh-cue start=\"32:58\" end=\"33:00\">So thank you so much for your time today.</rh-cue>\n    <rh-cue start=\"33:00\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"33:00\" end=\"33:00\">Thanks, Burr.</rh-cue>\n    <rh-cue start=\"33:00\" end=\"33:01\">This was a lot of fun.</rh-cue>\n    <rh-cue start=\"33:01\" end=\"33:04\">Thanks for having me.</rh-cue>\n    <rh-cue start=\"33:04\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"33:04\" end=\"33:06\">And you can check out</rh-cue>\n    <rh-cue start=\"33:06\" end=\"33:09\">the full transcript of our conversation and more resources,</rh-cue>\n    <rh-cue start=\"33:09\" end=\"33:12\">like a link to a white paper on open vino and normal lib at Red Hat dot</rh-cue>\n    <rh-cue start=\"33:12\" end=\"33:15\">com slash code Comments Podcast.</rh-cue>\n    <rh-cue start=\"33:15\" end=\"33:19\">This episode was produced by Brant Seminole and Caroline Prickett.</rh-cue>\n    <rh-cue start=\"33:20\" end=\"33:21\">Our sound designer is Christian.</rh-cue>\n    <rh-cue start=\"33:21\" end=\"33:26\">From our audio team includes Lee Day, Stephanie Wunderlich, Mike Esser,</rh-cue>\n    <rh-cue start=\"33:27\" end=\"33:32\">Laura Barnes, Claire Allison, Nick Burns, Aaron Williamson, Karen King,</rh-cue>\n    <rh-cue start=\"33:32\" end=\"33:36\">Booboo House, Rachel Artell, Mike Compton, Ocean</rh-cue>\n    <rh-cue start=\"33:36\" end=\"33:40\">Mathews, Laura Walters, Alex Trabelsi and Victoria Lutton.</rh-cue>\n    <rh-cue start=\"33:41\" end=\"33:43\">I'm your host, Burt Sutter.</rh-cue>\n    <rh-cue start=\"33:43\" end=\"33:45\">Thank you for joining me today on Code Comments.</rh-cue>\n    <rh-cue start=\"33:45\" end=\"33:48\">I hope you enjoyed today's session and today's conversation, and I</rh-cue>\n    <rh-cue start=\"33:48\" end=\"33:52\">look forward to many more in.</rh-cue>\n  </rh-transcript>\n</rh-audio-player>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\" src=\"customization.js\"></script>\n<!--playground-fold--><link rel=\"stylesheet\" href=\"../rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Detailed Transcript"
    },
    "demo/detailed-transcript/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "demo/detailed-transcript/customization.js": {
      "content": "import '@rhds/elements/rh-audio-player/rh-audio-player.js';\nconst form = document.querySelector('form');\nconst player = document.querySelector('rh-audio-player');\nconst { poster } = player;\n\n/**\n * update audio player demo based on form selections\n */\nfunction updateDemo() {\n  const colorPalette = ['cyan', 'light'].includes(form.palette.value) ?\n    'light' : 'dark';\n  const colorClass =\n    ['cyan', 'purple', 'purple img'].includes(form.palette.value) ? form.palette.value : '';\n  player.poster = !form.poster.checked || form.palette.value === 'purple img' ? undefined : poster;\n  player.layout = form.layout.value !== '' ? form.layout.value : undefined;\n  if (colorPalette === player.colorPalette) {\n    const oldOn = player.colorPalette;\n    player.colorPalette = oldOn === 'dark' ? 'light' : 'dark';\n  }\n  player.setAttribute('class', colorClass);\n  player.colorPalette = colorPalette;\n  player.hasAccentColor = ['cyan', 'purple', 'purple img'].includes(form.palette.value);\n  player.requestUpdate();\n}\n\nif (form) {\n  form.addEventListener('input', updateDemo);\n  updateDemo();\n}\n",
      "hidden": true
    },
    "demo/heading-levels/index.html": {
      "contentType": "text/html",
      "selected": false,
      "content": "<p>Audio player should automatically calculate it's heading levels</p>\n\n<h3>Root Level h3</h3>\n\n<p>Transcript should be <code>h5</code>, cues should be <code>h6</code></p>\n\n<rh-audio-player layout=\"full\" poster=\"https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png\">\n  <p slot=\"series\">Code Comments</p>\n  <h4 slot=\"title\">Bringing Deep Learning to Enterprise Applications</h4>\n  <rh-audio-player-about slot=\"about\">\n    <h5 slot=\"heading\">About the episode</h5>\n    <p>\n      There are a lot of publicly available data sets out there. But when it\n      comes to specific enterprise use cases, you're not necessarily going to\n      able to find one to train your models. To realize the power of AI/ML in\n      enterprise environments, end users need an inference engine to run on\n      their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open\n      toolkits from Intel that do precisely that. He looks specifically at\n      anomaly detection in use cases as varied as medical imaging and\n      manufacturing.\n    </p>\n    <p>\n      Want to learn more about Anomalib? Check out the research paper that\n      introduces the deep learning library.\n    </p>\n    <rh-avatar slot=\"profile\" src=\"https://www.redhat.com/cms/managed-files/ryan-loney.png\">\n      Ryan Loney\n      <span slot=\"subtitle\">Product manager, OpenVINO Developer Tools, <em>IntelÂ®</em></span>\n    </rh-avatar>\n  </rh-audio-player-about>\n  <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n    <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3\">\n  </audio>\n  <rh-audio-player-subscribe slot=\"subscribe\">\n    <h5 slot=\"heading\">Subscribe</h5>\n    <p>Subscribe here:</p>\n    <a slot=\"link\" href=\"https://podcasts.apple.com/us/podcast/code-comments/id1649848507\" target=\"_blank\" title=\"Listen on Apple Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Apple Podcasts\" data-analytics-category=\"Hero|Listen on Apple Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg\" alt=\"Listen on Apple Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD\" target=\"_blank\" title=\"Listen on Spotify\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Spotify\" data-analytics-category=\"Hero|Listen on Spotify\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_spotify.svg\" alt=\"Listen on Spotify\">\n    </a>\n    <a slot=\"link\" href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw\" target=\"_blank\" title=\"Listen on Google Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Google Podcasts\" data-analytics-category=\"Hero|Listen on Google Podcasts\">\n      <img src=\"https://www.redhat.com/cms/managed-files/badge_google-podcast.svg\" alt=\"Listen on Google Podcasts\">\n    </a>\n    <a slot=\"link\" href=\"https://feeds.pacific-content.com/codecomments\" target=\"_blank\" title=\"Subscribe via RSS Feed\" data-analytics-linktype=\"cta\" data-analytics-text=\"Subscribe via RSS Feed\" data-analytics-category=\"Hero|Subscribe via RSS Feed\">\n      <img class=\"img-fluid\" src=\"https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg\" alt=\"Subscribe via RSS Feed\">\n    </a>\n  </rh-audio-player-subscribe>\n  <rh-transcript slot=\"transcript\">\n    <rh-cue start=\"00:02\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"00:02\" end=\"00:04\">Hi, I'm Burr Sutter.</rh-cue>\n    <rh-cue start=\"00:04\" end=\"00:05\">I'm a Red Hatter</rh-cue>\n    <rh-cue start=\"00:05\" end=\"00:08\">who spends a lot of time talking to technologists about technologies.</rh-cue>\n    <rh-cue start=\"00:09\" end=\"00:10\">We say this a lot of Red Hat.</rh-cue>\n    <rh-cue start=\"00:10\" end=\"00:14\">No single technology provider holds the key to success, including us.</rh-cue>\n    <rh-cue start=\"00:15\" end=\"00:17\">And I would say the same thing about myself.</rh-cue>\n    <rh-cue start=\"00:17\" end=\"00:18\">I love to share ideas.</rh-cue>\n    <rh-cue start=\"00:18\" end=\"00:19\">So I thought it'd be awesome</rh-cue>\n    <rh-cue start=\"00:19\" end=\"00:22\">to talk to some brilliant technologists at Red Hat Partners.</rh-cue>\n    <rh-cue start=\"00:23\" end=\"00:26\">This is Code Comments, an original podcast</rh-cue>\n    <rh-cue start=\"00:26\" end=\"00:29\">from Red Hat.</rh-cue>\n    <rh-cue start=\"00:29\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"00:29\" end=\"00:32\">I'm sure, like many of you here, you have been thinking about</rh-cue>\n    <rh-cue start=\"00:32\" end=\"00:36\">AI, ML, artificial intelligence and machine learning.</rh-cue>\n    <rh-cue start=\"00:36\" end=\"00:38\">I've been thinking about that for quite some time</rh-cue>\n    <rh-cue start=\"00:38\" end=\"00:39\">and actually had the opportunity</rh-cue>\n    <rh-cue start=\"00:39\" end=\"00:43\">to work on a few successful projects here at Red Hat using those technologies,</rh-cue>\n    <rh-cue start=\"00:43\" end=\"00:46\">actually enabling a dataset, gathering a dataset,</rh-cue>\n    <rh-cue start=\"00:46\" end=\"00:49\">working with data scientists and data engineering team,</rh-cue>\n    <rh-cue start=\"00:49\" end=\"00:51\">and then training a model and putting that model into production</rh-cue>\n    <rh-cue start=\"00:51\" end=\"00:53\">runtime environment.</rh-cue>\n    <rh-cue start=\"00:53\" end=\"00:55\">It was an exciting set of projects and you can kind of see</rh-cue>\n    <rh-cue start=\"00:55\" end=\"00:58\">those on numerous YouTube videos I have published out there before.</rh-cue>\n    <rh-cue start=\"00:59\" end=\"01:01\">But I want you to think about the problem space a little bit</rh-cue>\n    <rh-cue start=\"01:01\" end=\"01:04\">because there are some interesting challenges about AI/ML.</rh-cue>\n    <rh-cue start=\"01:04\" end=\"01:06\">One is simply just getting access to the data,</rh-cue>\n    <rh-cue start=\"01:06\" end=\"01:09\">and while there are numerous publicly available datasets</rh-cue>\n    <rh-cue start=\"01:09\" end=\"01:12\">when it comes to your specific enterprise use case, you might not be to find</rh-cue>\n    <rh-cue start=\"01:12\" end=\"01:14\">publicly available data.</rh-cue>\n    <rh-cue start=\"01:14\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"01:14\" end=\"01:17\">In many cases, you cannot, even for our applications that we created,</rh-cue>\n    <rh-cue start=\"01:17\" end=\"01:20\">we had to create our dataset, capture our dataset,</rh-cue>\n    <rh-cue start=\"01:21\" end=\"01:24\">explore the dataset, and of course train a model accordingly.</rh-cue>\n    <rh-cue start=\"01:24\" end=\"01:27\">And we also found there's another challenge to be overcome</rh-cue>\n    <rh-cue start=\"01:27\" end=\"01:30\">in this AML world, and that is access to certain types of hardware.</rh-cue>\n    <rh-cue start=\"01:31\" end=\"01:33\">If you think about the enterprise environment</rh-cue>\n    <rh-cue start=\"01:33\" end=\"01:36\">and the creation of an enterprise application specifically for AML</rh-cue>\n    <rh-cue start=\"01:37\" end=\"01:40\">and users need an inference engine to run on their hardware,</rh-cue>\n    <rh-cue start=\"01:40\" end=\"01:43\">hardware that's available to them to be effective for their application.</rh-cue>\n    <rh-cue start=\"01:43\" end=\"01:45\">Let's say an application like computer vision,</rh-cue>\n    <rh-cue start=\"01:45\" end=\"01:49\">one that can detect anomalies in medical imaging or maybe on a factory floor,</rh-cue>\n    <rh-cue start=\"01:49\" end=\"01:52\">You know, those things are whizzing by on the factory line.</rh-cue>\n    <rh-cue start=\"01:52\" end=\"01:55\">They're looking at them and trying to determine if there is an error or not.</rh-cue>\n    <rh-cue start=\"01:56\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"01:56\" end=\"01:58\">Well, how do you actually make it run on your hardware,</rh-cue>\n    <rh-cue start=\"01:58\" end=\"02:01\">your accessible technology that you have today?</rh-cue>\n    <rh-cue start=\"02:01\" end=\"02:05\">Well, there's a solution for this as an open toolkit called Open vino.</rh-cue>\n    <rh-cue start=\"02:05\" end=\"02:07\">And you might be thinking, hey, wait a minute,</rh-cue>\n    <rh-cue start=\"02:07\" end=\"02:10\">don't you need a GPU for a I inferencing a GPU</rh-cue>\n    <rh-cue start=\"02:10\" end=\"02:12\">for artificial intelligence machine learning?</rh-cue>\n    <rh-cue start=\"02:12\" end=\"02:15\">Well, not according to Ryan Loney, product manager of Open Vino Developer</rh-cue>\n    <rh-cue start=\"02:15\" end=\"02:16\">Tools at Intel.</rh-cue>\n    <rh-cue start=\"02:20\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"02:20\" end=\"02:20\">I guess we'll</rh-cue>\n    <rh-cue start=\"02:20\" end=\"02:23\">start with trying to maybe dispel the myths, right?</rh-cue>\n    <rh-cue start=\"02:23\" end=\"02:27\">I think that CPUs are widely used for inference today.</rh-cue>\n    <rh-cue start=\"02:27\" end=\"02:32\">So and if we look at the data center segment, you know, about 70% of the A.I.</rh-cue>\n    <rh-cue start=\"02:32\" end=\"02:36\">inference is happening on Intel Xeon on our data center CPUs.</rh-cue>\n    <rh-cue start=\"02:36\" end=\"02:40\">And so you don't needed a GPU, especially for running inference.</rh-cue>\n    <rh-cue start=\"02:40\" end=\"02:43\">And that's part of the value of open vino, is that we're you know,</rh-cue>\n    <rh-cue start=\"02:43\" end=\"02:47\">we're taking models that may have been trained on a GPU</rh-cue>\n    <rh-cue start=\"02:47\" end=\"02:50\">using deep learning frameworks like PyTorch or TensorFlow</rh-cue>\n    <rh-cue start=\"02:51\" end=\"02:54\">and then optimizing them to run on Intel hardware.</rh-cue>\n    <rh-cue start=\"02:57\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"02:56\" end=\"03:00\">Ryan joined me to discuss AI/ML and the enterprise</rh-cue>\n    <rh-cue start=\"03:00\" end=\"03:03\">across various industries and exploring numerous use cases.</rh-cue>\n    <rh-cue start=\"03:05\" end=\"03:08\">Let's talk a little bit about the origin story behind Open Vino.</rh-cue>\n    <rh-cue start=\"03:08\" end=\"03:10\">Tell us more about it and how it came to be</rh-cue>\n    <rh-cue start=\"03:10\" end=\"03:13\">and why it came out of Intel.</rh-cue>\n    <rh-cue start=\"03:12\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"03:12\" end=\"03:16\">Definitely. So we had the first release of Open Vino</rh-cue>\n    <rh-cue start=\"03:16\" end=\"03:20\">was back in 2018, so still relatively new.</rh-cue>\n    <rh-cue start=\"03:20\" end=\"03:25\">And at that time we were focused on computer vision and pretty tightly coupled</rh-cue>\n    <rh-cue start=\"03:25\" end=\"03:31\">with open CV, which is another open source library with origins at Intel.</rh-cue>\n    <rh-cue start=\"03:31\" end=\"03:31\">You know, it</rh-cue>\n    <rh-cue start=\"03:31\" end=\"03:36\">had its first release back in 1999, so it's been around a little bit longer.</rh-cue>\n    <rh-cue start=\"03:36\" end=\"03:40\">And many of the software engineers and architects at Intel</rh-cue>\n    <rh-cue start=\"03:40\" end=\"03:45\">that were involved with and contributing to open CV are working on open Vino.</rh-cue>\n    <rh-cue start=\"03:45\" end=\"03:49\">So you can think of open vino as complementary software to open CV.</rh-cue>\n    <rh-cue start=\"03:50\" end=\"03:53\">And we're providing like an engine for executing inference</rh-cue>\n    <rh-cue start=\"03:53\" end=\"03:57\">as part of a computer vision pipeline, or at least that's how we started.</rh-cue>\n    <rh-cue start=\"03:58\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"03:58\" end=\"04:01\">But since 2018, we've we've started to move beyond</rh-cue>\n    <rh-cue start=\"04:01\" end=\"04:02\">just computer vision inference.</rh-cue>\n    <rh-cue start=\"04:02\" end=\"04:05\">So when I say computer vision inference, I mean like image</rh-cue>\n    <rh-cue start=\"04:05\" end=\"04:08\">classification, object detection, segmentation.</rh-cue>\n    <rh-cue start=\"04:09\" end=\"04:12\">And now we're moving into natural language processing, things</rh-cue>\n    <rh-cue start=\"04:12\" end=\"04:16\">like speech synthesis, speech recognition, knowledge, graphs,</rh-cue>\n    <rh-cue start=\"04:17\" end=\"04:21\">time series forecasting, and other use cases that don't involve</rh-cue>\n    <rh-cue start=\"04:21\" end=\"04:24\">computer vision and don't involve inference on pixels.</rh-cue>\n    <rh-cue start=\"04:25\" end=\"04:28\">Our latest release, the 20 22.1 that came out earlier this year,</rh-cue>\n    <rh-cue start=\"04:29\" end=\"04:32\">there was a most significant update that we've had to open vino</rh-cue>\n    <rh-cue start=\"04:32\" end=\"04:36\">since we started in 2018, and the major focus of that release</rh-cue>\n    <rh-cue start=\"04:36\" end=\"04:40\">was optimizing for use cases that go beyond computer vision.</rh-cue>\n    <rh-cue start=\"04:41\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"04:41\" end=\"04:44\">And I like that concept that you just mentioned right there, computer vision.</rh-cue>\n    <rh-cue start=\"04:44\" end=\"04:47\">And you said that you extended those use cases and went beyond that.</rh-cue>\n    <rh-cue start=\"04:47\" end=\"04:50\">So could you give us more concrete examples of computer vision?</rh-cue>\n    <rh-cue start=\"04:50\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"04:50\" end=\"04:50\">Yeah, sure.</rh-cue>\n    <rh-cue start=\"04:50\" end=\"04:55\">So when you think about manufacturing quality control in factories, everything</rh-cue>\n    <rh-cue start=\"04:55\" end=\"05:01\">from ARC welding, defect detection to inspecting BMW cars on assembly lines,</rh-cue>\n    <rh-cue start=\"05:02\" end=\"05:05\">they're using cameras or sensors to collect data.</rh-cue>\n    <rh-cue start=\"05:05\" end=\"05:11\">And usually it's cameras collecting images like RGV images that you and I can see.</rh-cue>\n    <rh-cue start=\"05:11\" end=\"05:14\">And looks like something taken from a camera or video camera,</rh-cue>\n    <rh-cue start=\"05:15\" end=\"05:19\">but also things like infrared or computerized tomography</rh-cue>\n    <rh-cue start=\"05:19\" end=\"05:24\">scans used in health care, X-ray, different types of images where we can</rh-cue>\n    <rh-cue start=\"05:25\" end=\"05:28\">draw bounding boxes around regions of interest</rh-cue>\n    <rh-cue start=\"05:29\" end=\"05:32\">and say, you know, this is a defect or this is not a defect.</rh-cue>\n    <rh-cue start=\"05:32\" end=\"05:37\">And also, is this worker wearing a safety hat or did they forget to put it on?</rh-cue>\n    <rh-cue start=\"05:37\" end=\"05:41\">And so you can take this and integrate it into a pipeline</rh-cue>\n    <rh-cue start=\"05:41\" end=\"05:44\">where you're triggering an alert if somebody forgets</rh-cue>\n    <rh-cue start=\"05:44\" end=\"05:49\">to wear their safety mask or if there's a defect in a product</rh-cue>\n    <rh-cue start=\"05:49\" end=\"05:53\">on an assembly line, you can just use cameras and open</rh-cue>\n    <rh-cue start=\"05:53\" end=\"05:58\">vino and open CV running these on Intel hardware and help to analyze.</rh-cue>\n    <rh-cue start=\"05:58\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"05:58\" end=\"06:01\">And that's what a lot of the partners that we work with are doing.</rh-cue>\n    <rh-cue start=\"06:01\" end=\"06:03\">So these independent software vendors</rh-cue>\n    <rh-cue start=\"06:03\" end=\"06:06\">and there's other use cases for things like retail.</rh-cue>\n    <rh-cue start=\"06:06\" end=\"06:10\">You think about going to a store and using an automated checkout system.</rh-cue>\n    <rh-cue start=\"06:11\" end=\"06:13\">You know, sometimes people use those automated checkouts</rh-cue>\n    <rh-cue start=\"06:13\" end=\"06:17\">and they they slide a few extra items into their bag that they don't scan.</rh-cue>\n    <rh-cue start=\"06:17\" end=\"06:21\">And it's a huge loss for the retail outlets</rh-cue>\n    <rh-cue start=\"06:21\" end=\"06:25\">that are providing this way to check out real time shelf monitoring.</rh-cue>\n    <rh-cue start=\"06:25\" end=\"06:29\">We have this bear on one of our is fees that helps keep store shelves</rh-cue>\n    <rh-cue start=\"06:29\" end=\"06:33\">stocked by just analyzing the cameras in the stores, detecting</rh-cue>\n    <rh-cue start=\"06:33\" end=\"06:37\">when objects are missing from the shelves so that they can be restocked.</rh-cue>\n    <rh-cue start=\"06:37\" end=\"06:41\">We have Vistry, another ISP that works with quick service restaurants.</rh-cue>\n    <rh-cue start=\"06:41\" end=\"06:44\">So when you think about automating the process of</rh-cue>\n    <rh-cue start=\"06:44\" end=\"06:48\">when do I drop the fries into the fryer so that they're warm</rh-cue>\n    <rh-cue start=\"06:48\" end=\"06:50\">when the car gets to the drive thru window,</rh-cue>\n    <rh-cue start=\"06:50\" end=\"06:54\">you know, there's quite a bit of industrial health care retail examples</rh-cue>\n    <rh-cue start=\"06:54\" end=\"06:57\">that we can walk through and we should dig into some more of those.</rh-cue>\n    <rh-cue start=\"06:57\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"06:57\" end=\"06:59\">But I got to tell you, I have I have a personal experience</rh-cue>\n    <rh-cue start=\"06:59\" end=\"06:59\">in this category</rh-cue>\n    <rh-cue start=\"06:59\" end=\"07:01\">that I want to share with, and you can tell me how</rh-cue>\n    <rh-cue start=\"07:01\" end=\"07:03\">how silly you might think at this point in time.</rh-cue>\n    <rh-cue start=\"07:03\" end=\"07:04\">It is.</rh-cue>\n    <rh-cue start=\"07:04\" end=\"07:08\">We actually built an AI keynote demonstration for the Red Hat big stage</rh-cue>\n    <rh-cue start=\"07:08\" end=\"07:12\">back in 2015, and I really want to illustrate the concept of asset tracking.</rh-cue>\n    <rh-cue start=\"07:12\" end=\"07:15\">So we actually gave everybody in the conference a little Bluetooth token,</rh-cue>\n    <rh-cue start=\"07:16\" end=\"07:19\">but a little battery, a little watch battery and a little Bluetooth emitter.</rh-cue>\n    <rh-cue start=\"07:19\" end=\"07:22\">And we basically tracked those things around the conference.</rh-cue>\n    <rh-cue start=\"07:22\" end=\"07:24\">We basically put a Raspberry Pi in each of the meeting rooms</rh-cue>\n    <rh-cue start=\"07:24\" end=\"07:26\">and up in the lunch room, and you could see how the tokens</rh-cue>\n    <rh-cue start=\"07:26\" end=\"07:30\">moved from room to room to room as a relatively simple application.</rh-cue>\n    <rh-cue start=\"07:30\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"07:30\" end=\"07:32\">But it occurred to me after we figured out,</rh-cue>\n    <rh-cue start=\"07:32\" end=\"07:34\">okay, how to do that with Bluetooth and triangulating</rh-cue>\n    <rh-cue start=\"07:34\" end=\"07:39\">Bluetooth signals by looking at relative signal strength from one radio to another</rh-cue>\n    <rh-cue start=\"07:39\" end=\"07:42\">and putting that through an Apache Spark application at the time,</rh-cue>\n    <rh-cue start=\"07:42\" end=\"07:45\">we then realized, you know what, this is easier done with cameras</rh-cue>\n    <rh-cue start=\"07:45\" end=\"07:49\">and just simply looking at a camera and having some form of animal</rh-cue>\n    <rh-cue start=\"07:49\" end=\"07:51\">or machine learning model that would say, Oh,</rh-cue>\n    <rh-cue start=\"07:51\" end=\"07:53\">there are people here now are there are no people here now.</rh-cue>\n    <rh-cue start=\"07:53\" end=\"07:55\">What do you think about that?</rh-cue>\n    <rh-cue start=\"07:55\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"07:55\" end=\"07:59\">Yeah, I mean, what you just described is sort of exactly that the product</rh-cue>\n    <rh-cue start=\"07:59\" end=\"08:02\">that either one of our partners is offering,</rh-cue>\n    <rh-cue start=\"08:02\" end=\"08:04\">you know, that they're doing it with computer vision and cameras.</rh-cue>\n    <rh-cue start=\"08:04\" end=\"08:08\">So when partner tries to help retail stores</rh-cue>\n    <rh-cue start=\"08:08\" end=\"08:12\">analyze the foot traffic and understand with Heatmaps,</rh-cue>\n    <rh-cue start=\"08:12\" end=\"08:16\">where people are spending the most time in stores, how many people are coming</rh-cue>\n    <rh-cue start=\"08:16\" end=\"08:19\">in, what size groups are coming into the store,</rh-cue>\n    <rh-cue start=\"08:19\" end=\"08:23\">you know, and trying to help understand if there was a successful transaction</rh-cue>\n    <rh-cue start=\"08:23\" end=\"08:27\">from the people who entered the store and left the store so that you can,</rh-cue>\n    <rh-cue start=\"08:27\" end=\"08:30\">you know, to help with the, you know, retail analytics</rh-cue>\n    <rh-cue start=\"08:30\" end=\"08:33\">and marketing sales and positioning of products.</rh-cue>\n    <rh-cue start=\"08:34\" end=\"08:37\">And so they're doing that in a way that also protects privacy.</rh-cue>\n    <rh-cue start=\"08:37\" end=\"08:38\">And that's something that's really important.</rh-cue>\n    <rh-cue start=\"08:38\" end=\"08:41\">So when you talked about those Bluetooth beacons, probably,</rh-cue>\n    <rh-cue start=\"08:41\" end=\"08:44\">\n      you know, if everyone who walked into a grocery store was asked\n    </rh-cue>\n    <rh-cue start=\"08:44\" end=\"08:49\">\n      to put a tracking device in their cart or on their person and say, you know,\n    </rh-cue>\n    <rh-cue start=\"08:49\" end=\"08:50\">\n      you're going\n    </rh-cue>\n    <rh-cue start=\"08:50\" end=\"08:53\">\n      to be tracked around the store, they probably wouldn't want to do that.\n    </rh-cue>\n    <rh-cue start=\"08:53\" end=\"08:56\">\n      The way that you can do this with cameras is you can,\n    </rh-cue>\n    <rh-cue start=\"08:53\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"08:56\" end=\"09:01\">\n      you know, detect people as they enter and, you know, remove their face.\n    </rh-cue>\n    <rh-cue start=\"09:01\" end=\"09:01\">\n      Right.\n    </rh-cue>\n    <rh-cue start=\"09:01\" end=\"09:05\">\n      So you can ignore any biometric information\n    </rh-cue>\n    <rh-cue start=\"09:05\" end=\"09:08\">\n      and and just track the person based on pixels\n    </rh-cue>\n    <rh-cue start=\"09:09\" end=\"09:12\">\n      that are present in the detected region of interest.\n    </rh-cue>\n    <rh-cue start=\"09:12\" end=\"09:15\">\n      So they're able to analyze, say, a family walks in the door\n    </rh-cue>\n    <rh-cue start=\"09:16\" end=\"09:20\">\n      and they can group those people together with object detection\n    </rh-cue>\n    <rh-cue start=\"09:20\" end=\"09:23\">\n      and then they can track their movement throughout the store\n    </rh-cue>\n    <rh-cue start=\"09:23\" end=\"09:26\">\n      without keeping track of their face or any biometric\n    </rh-cue>\n    <rh-cue start=\"09:26\" end=\"09:30\">\n      or any personal identifiable information to avoid things like bias\n    </rh-cue>\n    <rh-cue start=\"09:30\" end=\"09:35\">\n      and to make sure that they're protecting the privacy of the shoppers in the store\n    </rh-cue>\n    <rh-cue start=\"09:35\" end=\"09:39\">\n      while still getting that really useful marketing analytics data rate\n    </rh-cue>\n    <rh-cue start=\"09:39\" end=\"09:42\">\n      so that they can make better decisions about where to place their products.\n    </rh-cue>\n    <rh-cue start=\"09:42\" end=\"09:45\">\n      So that's one really good example of how\n    </rh-cue>\n    <rh-cue start=\"09:45\" end=\"09:48\">\n      computer vision AI with open vino is being used today.\n    </rh-cue>\n    <rh-cue start=\"09:48\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"09:48\" end=\"09:51\">\n      And that is a great example because you're definitely spot on.\n    </rh-cue>\n    <rh-cue start=\"09:51\" end=\"09:53\">\n      It is invasive when you hand someone to Bluetooth devices,\n    </rh-cue>\n    <rh-cue start=\"09:53\" end=\"09:56\">\n      say, please keep this with you as you go throughout our our store\n    </rh-cue>\n    <rh-cue start=\"09:56\" end=\"09:59\">\n      or our mall or throughout our hospital, wherever you might be.\n    </rh-cue>\n    <rh-cue start=\"09:59\" end=\"10:01\">\n      Now, you mentioned another example earlier\n    </rh-cue>\n    <rh-cue start=\"10:01\" end=\"10:03\">\n      in the conversation which was related to like worker safety.\n    </rh-cue>\n    <rh-cue start=\"10:03\" end=\"10:05\">\n      Are they wearing a helmet?\n    </rh-cue>\n    <rh-cue start=\"10:05\" end=\"10:08\">\n      I want to talk more about that concept in a real industrial setting,\n    </rh-cue>\n    <rh-cue start=\"10:08\" end=\"10:11\">\n      a manufacturing setting where there might be a factory floor\n    </rh-cue>\n    <rh-cue start=\"10:11\" end=\"10:13\">\n      and there are certain requirements, or better yet, there's like a\n    </rh-cue>\n    <rh-cue start=\"10:13\" end=\"10:16\">\n      a quality assurance requirement, let's say, when it comes to looking\n    </rh-cue>\n    <rh-cue start=\"10:16\" end=\"10:20\">\n      at a factory line, I run to that use case often what some of our customers.\n    </rh-cue>\n    <rh-cue start=\"10:20\" end=\"10:22\">\n      Can you talk more about those kinds of use cases? Yeah.\n    </rh-cue>\n    <rh-cue start=\"10:22\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"10:22\" end=\"10:27\">\n      So one of our partners, Robuchon and we you know, published a case study\n    </rh-cue>\n    <rh-cue start=\"10:27\" end=\"10:31\">\n      I think last year where they're working with BMW at one of their factories\n    </rh-cue>\n    <rh-cue start=\"10:32\" end=\"10:35\">\n      and they do quality control inspection, but they're also doing\n    </rh-cue>\n    <rh-cue start=\"10:35\" end=\"10:38\">\n      things related to worker safety and analyzing.\n    </rh-cue>\n    <rh-cue start=\"10:38\" end=\"10:40\">\n      You know, I used the safety had example.\n    </rh-cue>\n    <rh-cue start=\"10:40\" end=\"10:45\">\n      There's a number of of our ISP's and partners who have similar use cases.\n    </rh-cue>\n    <rh-cue start=\"10:45\" end=\"10:48\">\n      And it comes down to there's a few reasons\n    </rh-cue>\n    <rh-cue start=\"10:48\" end=\"10:51\">\n      that are motivating this and some are related to like insurance, right?\n    </rh-cue>\n    <rh-cue start=\"10:51\" end=\"10:53\">\n      It's important to make sure that\n    </rh-cue>\n    <rh-cue start=\"10:53\" end=\"10:56\">\n      if you want to have your factory insured and that your workers\n    </rh-cue>\n    <rh-cue start=\"10:56\" end=\"10:58\">\n      are protecting themselves and wearing the gear.\n    </rh-cue>\n    <rh-cue start=\"10:58\" end=\"11:00\">\n      Regulatory compliance. Right.\n    </rh-cue>\n    <rh-cue start=\"11:00\" end=\"11:05\">\n      You're you're being asked to properly protect from exposure to chemicals or,\n    </rh-cue>\n    <rh-cue start=\"11:05\" end=\"11:09\">\n      you know, potentially having something fall and and hit someone on the head.\n    </rh-cue>\n    <rh-cue start=\"11:09\" end=\"11:13\">\n      So wearing a safety vest, wearing goggles, wearing a helmet,\n    </rh-cue>\n    <rh-cue start=\"11:14\" end=\"11:17\">\n      these are things that you need to do inside the factory.\n    </rh-cue>\n    <rh-cue start=\"11:17\" end=\"11:21\">\n      And you can really easily automate and detect and sometimes without bias.\n    </rh-cue>\n    <rh-cue start=\"11:21\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"11:21\" end=\"11:26\">\n      I think that's one of the interesting things about the robots on BMW example\n    </rh-cue>\n    <rh-cue start=\"11:26\" end=\"11:31\">\n      is that they were also blurring sort of blocking out and so drawing a box\n    </rh-cue>\n    <rh-cue start=\"11:31\" end=\"11:35\">\n      to cover the face of the workers in the factory\n    </rh-cue>\n    <rh-cue start=\"11:35\" end=\"11:38\">\n      so that somebody who was analyzing the video footage\n    </rh-cue>\n    <rh-cue start=\"11:38\" end=\"11:43\">\n      and getting the alerts saying that, hey, you know, Bay 21 has a worker\n    </rh-cue>\n    <rh-cue start=\"11:43\" end=\"11:47\">\n      without a hat on, that it's not sending their face\n    </rh-cue>\n    <rh-cue start=\"11:47\" end=\"11:50\">\n      and in the alert and potentially, you know, invading\n    </rh-cue>\n    <rh-cue start=\"11:50\" end=\"11:54\">\n      or going against privacy laws or just the ethics of the company.\n    </rh-cue>\n    <rh-cue start=\"11:54\" end=\"11:54\">\n      Right.\n    </rh-cue>\n    <rh-cue start=\"11:54\" end=\"11:58\">\n      They don't want to introduce bias or have people targeted because\n    </rh-cue>\n    <rh-cue start=\"11:58\" end=\"12:02\">\n      it's much better to to have it be, you know, blur the face\n    </rh-cue>\n    <rh-cue start=\"12:02\" end=\"12:06\">\n      and alert and have somebody take care of it on the floor.\n    </rh-cue>\n    <rh-cue start=\"12:06\" end=\"12:09\">\n      And then if you ever need to audit that information later,\n    </rh-cue>\n    <rh-cue start=\"12:09\" end=\"12:12\">\n      they have a way to do it where people who need to be able to see\n    </rh-cue>\n    <rh-cue start=\"12:12\" end=\"12:17\">\n      who the employee was and look up their personal information, they can do that.\n    </rh-cue>\n    <rh-cue start=\"12:17\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"12:17\" end=\"12:20\">\n      But then just for the purposes of maintaining safety,\n    </rh-cue>\n    <rh-cue start=\"12:20\" end=\"12:21\">\n      they don't need to have access\n    </rh-cue>\n    <rh-cue start=\"12:21\" end=\"12:24\">\n      to that personal information or biometric information,\n    </rh-cue>\n    <rh-cue start=\"12:25\" end=\"12:28\">\n      because that's one thing that when you hear about computer vision\n    </rh-cue>\n    <rh-cue start=\"12:28\" end=\"12:31\">\n      or object person tracking, object detection,\n    </rh-cue>\n    <rh-cue start=\"12:32\" end=\"12:36\">\n      there's a lot of concern, and rightfully so, about privacy\n    </rh-cue>\n    <rh-cue start=\"12:36\" end=\"12:40\">\n      being invaded and about tracking information, face ID,\n    </rh-cue>\n    <rh-cue start=\"12:41\" end=\"12:45\">\n      identifying people who may have committed crimes through video footage.\n    </rh-cue>\n    <rh-cue start=\"12:45\" end=\"12:48\">\n      And that's just not something that a lot of companies want to\n    </rh-cue>\n    <rh-cue start=\"12:49\" end=\"12:51\">\n      you know, they want to protect privacy\n    </rh-cue>\n    <rh-cue start=\"12:51\" end=\"12:52\">\n      and they don't they don't want to be in a situation\n    </rh-cue>\n    <rh-cue start=\"12:52\" end=\"12:55\">\n      where they might be violating someone's rights.\n    </rh-cue>\n    <rh-cue start=\"12:56\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"12:56\" end=\"12:58\">\n      Well, privacy is certainly opening up Pandora's box.\n    </rh-cue>\n    <rh-cue start=\"12:58\" end=\"13:00\">\n      There's a lot to be explored in that area,\n    </rh-cue>\n    <rh-cue start=\"13:00\" end=\"13:02\">\n      especially in a digital world that we now live in.\n    </rh-cue>\n    <rh-cue start=\"13:02\" end=\"13:05\">\n      But for now, let's move on and explore different area.\n    </rh-cue>\n    <rh-cue start=\"13:05\" end=\"13:08\">\n      I'm interested in how machines and computers offer advantages,\n    </rh-cue>\n    <rh-cue start=\"13:08\" end=\"13:12\">\n      specifically in certain use cases like a quality control scenario.\n    </rh-cue>\n    <rh-cue start=\"13:12\" end=\"13:15\">\n      I asked Ryan to explain how AML and specifically machines\n    </rh-cue>\n    <rh-cue start=\"13:15\" end=\"13:18\">\n      computers can augment that capability.\n    </rh-cue>\n    <rh-cue start=\"13:19\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"13:19\" end=\"13:22\">\n      I can give a specific example where we have a partner\n    </rh-cue>\n    <rh-cue start=\"13:22\" end=\"13:25\">\n      that's there doing defect detection with\n    </rh-cue>\n    <rh-cue start=\"13:25\" end=\"13:28\">\n      and looking for anomalies in batteries.\n    </rh-cue>\n    <rh-cue start=\"13:28\" end=\"13:31\">\n      So, you know, sure, you've heard there's a lot of interest right now\n    </rh-cue>\n    <rh-cue start=\"13:31\" end=\"13:34\">\n      in electric vehicles, a lot of batteries being produced.\n    </rh-cue>\n    <rh-cue start=\"13:34\" end=\"13:36\">\n      And so if you go into one of these factories,\n    </rh-cue>\n    <rh-cue start=\"13:36\" end=\"13:40\">\n      they have images that they collect of every battery that's going through this\n    </rh-cue>\n    <rh-cue start=\"13:40\" end=\"13:44\">\n      assembly line and through these images, people\n    </rh-cue>\n    <rh-cue start=\"13:44\" end=\"13:47\">\n      can look and see and visually inspect with their eyes and say,\n    </rh-cue>\n    <rh-cue start=\"13:48\" end=\"13:50\">\n      this battery has a defect, send it back.\n    </rh-cue>\n    <rh-cue start=\"13:50\" end=\"13:53\">\n      And that's one step in the quality control process.\n    </rh-cue>\n    <rh-cue start=\"13:53\" end=\"13:58\">\n      And there's other steps, I'm sure, like running diagnostic tests and, you know,\n    </rh-cue>\n    <rh-cue start=\"13:58\" end=\"14:02\">\n      measuring voltage and doing other types of non-visual inspection.\n    </rh-cue>\n    <rh-cue start=\"14:02\" end=\"14:06\">\n      But for the visual inspection piece where you can really easily identify\n    </rh-cue>\n    <rh-cue start=\"14:06\" end=\"14:10\">\n      some problems, it's much more efficient to introduce computer vision.\n    </rh-cue>\n    <rh-cue start=\"14:11\" end=\"14:14\">\n      And so that's where we have this new library that we've introduced\n    </rh-cue>\n    <rh-cue start=\"14:14\" end=\"14:17\">\n      called Anomali, that's open vino.\n    </rh-cue>\n    <rh-cue start=\"14:17\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"14:17\" end=\"14:20\">\n      While we're focused on inference, you know, we're also thinking\n    </rh-cue>\n    <rh-cue start=\"14:20\" end=\"14:25\">\n      about the pipeline or the funnel that gets these models to open vino.\n    </rh-cue>\n    <rh-cue start=\"14:25\" end=\"14:28\">\n      And so we've we've invested in this anomaly segmentation,\n    </rh-cue>\n    <rh-cue start=\"14:28\" end=\"14:32\">\n      anomaly detection library and that we've recently open source\n    </rh-cue>\n    <rh-cue start=\"14:32\" end=\"14:35\">\n      and there's a great research paper about it about Anomali.\n    </rh-cue>\n    <rh-cue start=\"14:36\" end=\"14:39\">\n      But the idea is you can take just a few images\n    </rh-cue>\n    <rh-cue start=\"14:39\" end=\"14:43\">\n      and train a model and start detecting these defects.\n    </rh-cue>\n    <rh-cue start=\"14:43\" end=\"14:46\">\n      And so for this battery example, that's a more advanced example.\n    </rh-cue>\n    <rh-cue start=\"14:46\" end=\"14:52\">\n      But to make it simpler, you know, take some bolts and, you know, take ten bolts.\n    </rh-cue>\n    <rh-cue start=\"14:52\" end=\"14:55\">\n      You have one that has a scratch on it or one that is chipped\n    </rh-cue>\n    <rh-cue start=\"14:56\" end=\"14:58\">\n      or has some damage to it.\n    </rh-cue>\n    <rh-cue start=\"14:58\" end=\"15:00\">\n      And you can easily get started in training\n    </rh-cue>\n    <rh-cue start=\"15:00\" end=\"15:03\">\n      to recognize the bolts that do not have an anomaly.\n    </rh-cue>\n    <rh-cue start=\"15:04\" end=\"15:06\">\n      And the ones that do, which is a small data set\n    </rh-cue>\n    <rh-cue start=\"15:06\" end=\"15:10\">\n      and I think that's really one of the most important things today.\n    </rh-cue>\n    <rh-cue start=\"15:11\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"15:11\" end=\"15:14\">\n      Challenges is one is access to data, but the other is\n    </rh-cue>\n    <rh-cue start=\"15:14\" end=\"15:17\">\n      is needing a massive amount of data to do something meaningful.\n    </rh-cue>\n    <rh-cue start=\"15:18\" end=\"15:22\">\n      And so we're starting to try to change that dynamic with Anomali.\n    </rh-cue>\n    <rh-cue start=\"15:22\" end=\"15:27\">\n      So you may not need 100,000 images, you may need 100 images,\n    </rh-cue>\n    <rh-cue start=\"15:27\" end=\"15:33\">\n      and you can start detecting anomalies in everything from batteries to bolts to,\n    </rh-cue>\n    <rh-cue start=\"15:33\" end=\"15:37\">\n      you know, maybe even the wood varnish use case that you mentioned.\n    </rh-cue>\n    <rh-cue start=\"15:37\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"15:37\" end=\"15:40\">\n      That is very key point because often in that data scientist\n    </rh-cue>\n    <rh-cue start=\"15:40\" end=\"15:43\">\n      process, that data engineer and data scientist process, right.\n    </rh-cue>\n    <rh-cue start=\"15:43\" end=\"15:44\">\n      The one key thing is can you gather\n    </rh-cue>\n    <rh-cue start=\"15:44\" end=\"15:47\">\n      the data that you need for the input for the model training?\n    </rh-cue>\n    <rh-cue start=\"15:47\" end=\"15:49\">\n      And we've often sat at least people I've worked\n    </rh-cue>\n    <rh-cue start=\"15:49\" end=\"15:52\">\n      with over the last couple of years, you know, you need a lot of data.\n    </rh-cue>\n    <rh-cue start=\"15:52\" end=\"15:55\">\n      You need tens of thousands of correct images\n    </rh-cue>\n    <rh-cue start=\"15:55\" end=\"15:58\">\n      so we can sort out the difference between dogs versus cats, let's say,\n    </rh-cue>\n    <rh-cue start=\"15:58\" end=\"16:01\">\n      or you need dozens and dozens of situations\n    </rh-cue>\n    <rh-cue start=\"16:01\" end=\"16:03\">\n      where if it's a natural language processing scenario,\n    </rh-cue>\n    <rh-cue start=\"16:03\" end=\"16:06\">\n      you know, a good customer interaction, a good customer conversation,\n    </rh-cue>\n    <rh-cue start=\"16:06\" end=\"16:07\">\n      and in this case,\n    </rh-cue>\n    <rh-cue start=\"16:07\" end=\"16:11\">\n      it sounds like what you're saying is show us just the bad things, right?\n    </rh-cue>\n    <rh-cue start=\"16:11\" end=\"16:14\">\n      Fewer images, fewer incorrect things,\n    </rh-cue>\n    <rh-cue start=\"16:14\" end=\"16:17\">\n      and then let us look for those kind of anomalies.\n    </rh-cue>\n    <rh-cue start=\"16:18\" end=\"16:20\">\n      Can tell us more about that because that is very interesting.\n    </rh-cue>\n    <rh-cue start=\"16:20\" end=\"16:23\">\n      The concept that I can use a much smaller dataset as my input\n    </rh-cue>\n    <rh-cue start=\"16:23\" end=\"16:26\">\n      as opposed to gathering terabytes of data in some cases\n    </rh-cue>\n    <rh-cue start=\"16:26\" end=\"16:29\">\n      to just simply get my model training underway.\n    </rh-cue>\n    <rh-cue start=\"16:30\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"16:30\" end=\"16:34\">\n      You know, like you described, the idea is if you have some good images\n    </rh-cue>\n    <rh-cue start=\"16:34\" end=\"16:37\">\n      and then you have some of the the known defects\n    </rh-cue>\n    <rh-cue start=\"16:38\" end=\"16:41\">\n      and you can just label here's a set of good images\n    </rh-cue>\n    <rh-cue start=\"16:41\" end=\"16:44\">\n      and here's a few of the defects and you can right away\n    </rh-cue>\n    <rh-cue start=\"16:44\" end=\"16:48\">\n      start detecting those specific defects that you've identified.\n    </rh-cue>\n    <rh-cue start=\"16:48\" end=\"16:49\">\n      And then also, you know, be able to\n    </rh-cue>\n    <rh-cue start=\"16:50\" end=\"16:53\">\n      determine when it doesn't match\n    </rh-cue>\n    <rh-cue start=\"16:53\" end=\"16:57\">\n      the expected appearance of a non defective item.\n    </rh-cue>\n    <rh-cue start=\"16:57\" end=\"17:00\">\n      So if I have the undamaged screw and then I introduce\n    </rh-cue>\n    <rh-cue start=\"17:00\" end=\"17:03\">\n      one with some new anomaly that's never been seen before,\n    </rh-cue>\n    <rh-cue start=\"17:04\" end=\"17:07\">\n      I can say, you know, this one is not a valid screw.\n    </rh-cue>\n    <rh-cue start=\"17:07\" end=\"17:11\">\n      And so that's sort of the the approach that we're taking.\n    </rh-cue>\n    <rh-cue start=\"17:11\" end=\"17:15\">\n      And it's really important because so often you need to have\n    </rh-cue>\n    <rh-cue start=\"17:15\" end=\"17:19\">\n      subject matter experts often like if you think the take the battery example,\n    </rh-cue>\n    <rh-cue start=\"17:20\" end=\"17:23\">\n      there's these workers who are on the floor\n    </rh-cue>\n    <rh-cue start=\"17:23\" end=\"17:27\">\n      in a factory and they're the ones who know best when they look at these images,\n    </rh-cue>\n    <rh-cue start=\"17:28\" end=\"17:31\">\n      which one's going to have an issue, which one's defective?\n    </rh-cue>\n    <rh-cue start=\"17:31\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"17:31\" end=\"17:34\">\n      And then they also need to take that subject matter, expertise\n    </rh-cue>\n    <rh-cue start=\"17:35\" end=\"17:38\">\n      and then use it to annotate data sets.\n    </rh-cue>\n    <rh-cue start=\"17:38\" end=\"17:39\">\n      And when you have these, you know,\n    </rh-cue>\n    <rh-cue start=\"17:39\" end=\"17:43\">\n      tens of thousands of images you need to annotate, it's asking those people\n    </rh-cue>\n    <rh-cue start=\"17:43\" end=\"17:47\">\n      to stop working on the factory floor so they can come annotate some images.\n    </rh-cue>\n    <rh-cue start=\"17:47\" end=\"17:49\">\n      That's a tough business call to make, right?\n    </rh-cue>\n    <rh-cue start=\"17:49\" end=\"17:53\">\n      But if you only need them to annotate a handful of images, it's a much easier\n    </rh-cue>\n    <rh-cue start=\"17:53\" end=\"17:56\">\n      ask to get the ball rolling and demonstrate value.\n    </rh-cue>\n    <rh-cue start=\"17:56\" end=\"17:59\">\n      And maybe over time you will want to annotate more\n    </rh-cue>\n    <rh-cue start=\"17:59\" end=\"18:03\">\n      and more images because you'll get even better accuracy in the model.\n    </rh-cue>\n    <rh-cue start=\"18:03\" end=\"18:07\">\n      Even better, even if it's just small incremental improvements.\n    </rh-cue>\n    <rh-cue start=\"18:08\" end=\"18:11\">\n      You know, that's something that if it generates value for the business,\n    </rh-cue>\n    <rh-cue start=\"18:11\" end=\"18:14\">\n      it's something the business will invest in over time.\n    </rh-cue>\n    <rh-cue start=\"18:14\" end=\"18:17\">\n      But you have to convince the decision makers that it's worth\n    </rh-cue>\n    <rh-cue start=\"18:17\" end=\"18:22\">\n      the time of these subject matter experts to stop what they're doing\n    </rh-cue>\n    <rh-cue start=\"18:22\" end=\"18:26\">\n      and go and label some images of the things that they're working on in the factory.\n    </rh-cue>\n    <rh-cue start=\"18:26\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"18:26\" end=\"18:30\">\n      And that labeling process can be very labor intensive of the annotations,\n    </rh-cue>\n    <rh-cue start=\"18:30\" end=\"18:33\">\n      basically saying what is correct, what's wrong, what is this, what is that?\n    </rh-cue>\n    <rh-cue start=\"18:33\" end=\"18:36\">\n      And therefore, if we can minimize that time frame to get the value quicker,\n    </rh-cue>\n    <rh-cue start=\"18:36\" end=\"18:40\">\n      then there's something that's useful for the business, useful for the organization\n    </rh-cue>\n    <rh-cue start=\"18:40\" end=\"18:41\">\n      long before we necessarily good.\n    </rh-cue>\n    <rh-cue start=\"18:41\" end=\"18:43\">\n      There are huge model training based,\n    </rh-cue>\n    <rh-cue start=\"18:49\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"18:49\" end=\"18:52\">\n      so we talk about labeling and how that is labor intensive activity.\n    </rh-cue>\n    <rh-cue start=\"18:52\" end=\"18:54\">\n      But I love the idea of helping the human\n    </rh-cue>\n    <rh-cue start=\"18:54\" end=\"18:57\">\n      and helping the human models specifically not get bored.\n    </rh-cue>\n    <rh-cue start=\"18:57\" end=\"19:01\">\n      Basically, if the human is eyeballing a bunch of widgets flying by over time,\n    </rh-cue>\n    <rh-cue start=\"19:01\" end=\"19:03\">\n      they make mistakes, they get bored\n    </rh-cue>\n    <rh-cue start=\"19:03\" end=\"19:06\">\n      and they don't pay as close attention as they should.\n    </rh-cue>\n    <rh-cue start=\"19:07\" end=\"19:10\">\n      That's why the concept of Amazon specifically computer vision, augmenting\n    </rh-cue>\n    <rh-cue start=\"19:10\" end=\"19:14\">\n      that capability and really helping the human identify anomalies faster,\n    </rh-cue>\n    <rh-cue start=\"19:14\" end=\"19:17\">\n      more quickly, maybe with greater accuracy could be a big win.\n    </rh-cue>\n    <rh-cue start=\"19:18\" end=\"19:21\">\n      We focused on manufacturing, but let's actually go into health care\n    </rh-cue>\n    <rh-cue start=\"19:21\" end=\"19:24\">\n      and learn how these tools can be used in that sector and that industry.\n    </rh-cue>\n    <rh-cue start=\"19:24\" end=\"19:28\">\n      Ryan talked to me about how Open Windows runtime can be incorporated into medical\n    </rh-cue>\n    <rh-cue start=\"19:28\" end=\"19:32\">\n      imaging equipment with intel processors and better than c.T.\n    </rh-cue>\n    <rh-cue start=\"19:32\" end=\"19:34\">\n      MRI and ultrasound machines.\n    </rh-cue>\n    <rh-cue start=\"19:34\" end=\"19:37\">\n      Well, these inferences, this AML workload can be operating\n    </rh-cue>\n    <rh-cue start=\"19:37\" end=\"19:41\">\n      and executing right there in the same physical room as the patient.\n    </rh-cue>\n    <rh-cue start=\"19:44\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"19:44\" end=\"19:46\">\n      We did a presentation when she last year.\n    </rh-cue>\n    <rh-cue start=\"19:46\" end=\"19:47\">\n      I think they said\n    </rh-cue>\n    <rh-cue start=\"19:47\" end=\"19:50\">\n      there's at least 80 countries that have their X-ray machines deployed\n    </rh-cue>\n    <rh-cue start=\"19:51\" end=\"19:56\">\n      and they're doing things like helping doctors place breathing tubes in patients.\n    </rh-cue>\n    <rh-cue start=\"19:56\" end=\"20:01\">\n      So during COVID, during the pandemic, that was a really important tool\n    </rh-cue>\n    <rh-cue start=\"20:01\" end=\"20:05\">\n      to help with nurses and doctors who were intubating patients\n    </rh-cue>\n    <rh-cue start=\"20:05\" end=\"20:09\">\n      sometimes like in a parking lot or a hallway of the hospital.\n    </rh-cue>\n    <rh-cue start=\"20:09\" end=\"20:14\">\n      And, you know, when they had a statistic that you said, I think one out of four\n    </rh-cue>\n    <rh-cue start=\"20:14\" end=\"20:17\">\n      breathing tubes gets placed incorrectly\n    </rh-cue>\n    <rh-cue start=\"20:17\" end=\"20:19\">\n      when you're doing it outside the operating room,\n    </rh-cue>\n    <rh-cue start=\"20:19\" end=\"20:22\">\n      because when you're in an operating room, it's much more controlled\n    </rh-cue>\n    <rh-cue start=\"20:22\" end=\"20:24\">\n      and there's someone who's an expert at placing the tubes.\n    </rh-cue>\n    <rh-cue start=\"20:24\" end=\"20:28\">\n      It's something you have more of a controlled environment\n    </rh-cue>\n    <rh-cue start=\"20:28\" end=\"20:31\">\n      than when you're out in a parking lot, in a tent.\n    </rh-cue>\n    <rh-cue start=\"20:31\" end=\"20:34\">\n      You know, when the hospital's completely full and you're triaging patients\n    </rh-cue>\n    <rh-cue start=\"20:34\" end=\"20:37\">\n      with COVID, that's when they're more likely to make mistakes.\n    </rh-cue>\n    <rh-cue start=\"20:37\" end=\"20:40\">\n      And so they had this endotracheal tube placement\n    </rh-cue>\n    <rh-cue start=\"20:42\" end=\"20:43\">\n      model that they trained,\n    </rh-cue>\n    <rh-cue start=\"20:43\" end=\"20:47\">\n      and it helped to use an x ray and give an alert and say, hey,\n    </rh-cue>\n    <rh-cue start=\"20:47\" end=\"20:50\">\n      this tube is placed wrong, pull it out and do it again.\n    </rh-cue>\n    <rh-cue start=\"20:50\" end=\"20:53\">\n      And so things like that help doctors so that they can avoid mistakes.\n    </rh-cue>\n    <rh-cue start=\"20:54\" end=\"20:57\">\n      And, you know, having a breathing tube placed incorrectly\n    </rh-cue>\n    <rh-cue start=\"20:57\" end=\"21:01\">\n      can cause collapsed lung and a number of other unwanted side effects.\n    </rh-cue>\n    <rh-cue start=\"21:01\" end=\"21:03\">\n      So it's really important to do it correctly.\n    </rh-cue>\n    <rh-cue start=\"21:03\" end=\"21:06\">\n      Another example is Samsung Medicine.\n    </rh-cue>\n    <rh-cue start=\"21:06\" end=\"21:10\">\n      They actually are doing estimating fetal angle of progression.\n    </rh-cue>\n    <rh-cue start=\"21:10\" end=\"21:13\">\n      So this is analyzing ultrasound\n    </rh-cue>\n    <rh-cue start=\"21:14\" end=\"21:18\">\n      of pregnant women with that, being able to to help take measurements\n    </rh-cue>\n    <rh-cue start=\"21:18\" end=\"21:22\">\n      that are usually hard to calculate that can be done in an automated way.\n    </rh-cue>\n    <rh-cue start=\"21:22\" end=\"21:26\">\n      They're already taking the ultrasound scan and now they're executing this model.\n    </rh-cue>\n    <rh-cue start=\"21:26\" end=\"21:31\">\n      They can take some of these measurements to help the doctor avoid potentially more\n    </rh-cue>\n    <rh-cue start=\"21:31\" end=\"21:34\">\n      intrusive alternative methods so the patient wins.\n    </rh-cue>\n    <rh-cue start=\"21:35\" end=\"21:36\">\n      It makes their life better.\n    </rh-cue>\n    <rh-cue start=\"21:36\" end=\"21:39\">\n      And the doctors is getting help from this A.I.\n    </rh-cue>\n    <rh-cue start=\"21:39\" end=\"21:40\">\n      model.\n    </rh-cue>\n    <rh-cue start=\"21:40\" end=\"21:42\">\n      And those are, you know, just a few examples.\n    </rh-cue>\n    <rh-cue start=\"21:42\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"21:42\" end=\"21:45\">\n      Those are some amazing examples when it comes to all these things.\n    </rh-cue>\n    <rh-cue start=\"21:45\" end=\"21:49\">\n      We're talking like CT scans, right, and x rays, other examples of computer vision.\n    </rh-cue>\n    <rh-cue start=\"21:49\" end=\"21:52\">\n      One thing that's kind of interesting in the space, I think\n    </rh-cue>\n    <rh-cue start=\"21:52\" end=\"21:56\">\n      whenever I get a chance to work on, let's say an object traction model\n    </rh-cue>\n    <rh-cue start=\"21:56\" end=\"21:58\">\n      and one of our workshops, by the way, is actually putting that out\n    </rh-cue>\n    <rh-cue start=\"21:58\" end=\"22:01\">\n      in front of people to say, Hey, look, you can use your phone.\n    </rh-cue>\n    <rh-cue start=\"22:01\" end=\"22:04\">\n      And it basically sends the image over to our OpenShift, right,\n    </rh-cue>\n    <rh-cue start=\"22:04\" end=\"22:07\">\n      with our data science platform and then analyzes what you see.\n    </rh-cue>\n    <rh-cue start=\"22:08\" end=\"22:09\">\n      And even in my case, where I take a picture of my dog\n    </rh-cue>\n    <rh-cue start=\"22:09\" end=\"22:13\">\n      as an example, it can't really decide is it a dog or a cat?\n    </rh-cue>\n    <rh-cue start=\"22:13\" end=\"22:15\">\n      I have a very funny looking dog,\n    </rh-cue>\n    <rh-cue start=\"22:15\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"22:15\" end=\"22:18\">\n      and so there's always a percentage outcome, you know?\n    </rh-cue>\n    <rh-cue start=\"22:18\" end=\"22:21\">\n      In other words, I think it's a dog 52%.\n    </rh-cue>\n    <rh-cue start=\"22:21\" end=\"22:22\">\n      So I want to talk about that more.\n    </rh-cue>\n    <rh-cue start=\"22:22\" end=\"22:25\">\n      What how important is it to get to 100% accuracy?\n    </rh-cue>\n    <rh-cue start=\"22:25\" end=\"22:29\">\n      How important is it to really, depending on the use case, to allow\n    </rh-cue>\n    <rh-cue start=\"22:29\" end=\"22:34\">\n      for the gray area, if you will, where it's an 80% accuracy or 70% accuracy?\n    </rh-cue>\n    <rh-cue start=\"22:34\" end=\"22:36\">\n      And where are the trade offs there associated with the application?\n    </rh-cue>\n    <rh-cue start=\"22:36\" end=\"22:38\">\n      Can you can you discuss that more?\n    </rh-cue>\n    <rh-cue start=\"22:38\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"22:38\" end=\"22:40\">\n      Accuracy is definitely, you know, a touchy subject\n    </rh-cue>\n    <rh-cue start=\"22:40\" end=\"22:43\">\n      because how you measure it makes a huge difference.\n    </rh-cue>\n    <rh-cue start=\"22:43\" end=\"22:46\">\n      And then I think with like what you were describing with the dog example, there's\n    </rh-cue>\n    <rh-cue start=\"22:46\" end=\"22:51\">\n      sort of a top five potential classes that might may be identified.\n    </rh-cue>\n    <rh-cue start=\"22:51\" end=\"22:55\">\n      So let's say you're doing object detection and you detect a region of interest\n    </rh-cue>\n    <rh-cue start=\"22:55\" end=\"22:57\">\n      and it says 65% confidence.\n    </rh-cue>\n    <rh-cue start=\"22:57\" end=\"22:58\">\n      This is a dog.\n    </rh-cue>\n    <rh-cue start=\"22:58\" end=\"23:03\">\n      Well, the next potential label that could be maybe 50% confidence\n    </rh-cue>\n    <rh-cue start=\"23:03\" end=\"23:08\">\n      or 20% confidence might be something similar to a dog or in the case of models\n    </rh-cue>\n    <rh-cue start=\"23:08\" end=\"23:11\">\n      that have been trained on like the image net dataset\n    </rh-cue>\n    <rh-cue start=\"23:11\" end=\"23:15\">\n      or on cocoa data set, they have like actual breeds of dogs.\n    </rh-cue>\n    <rh-cue start=\"23:15\" end=\"23:20\">\n      So if I want to look at the top five labels for a dog,\n    </rh-cue>\n    <rh-cue start=\"23:20\" end=\"23:24\">\n      for my dog, for example, she's a mixed mostly Labrador retriever.\n    </rh-cue>\n    <rh-cue start=\"23:24\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"23:24\" end=\"23:29\">\n      But I may look at the top five labels and it may say 65% confidence that she's\n    </rh-cue>\n    <rh-cue start=\"23:29\" end=\"23:34\">\n      a flat coated retriever and then confidence that she's a husky,\n    </rh-cue>\n    <rh-cue start=\"23:34\" end=\"23:39\">\n      as you know, 20% and then 5% confidence that she's a Greyhound or something.\n    </rh-cue>\n    <rh-cue start=\"23:40\" end=\"23:42\">\n      Those labels, all of them are dogs.\n    </rh-cue>\n    <rh-cue start=\"23:42\" end=\"23:45\">\n      So if I'm just trying to figure out is, is this a dog,\n    </rh-cue>\n    <rh-cue start=\"23:45\" end=\"23:50\">\n      I could probably find all of the, you know, classes within the data set\n    </rh-cue>\n    <rh-cue start=\"23:50\" end=\"23:53\">\n      and say, well, these are all, you know, class ID\n    </rh-cue>\n    <rh-cue start=\"23:53\" end=\"24:00\">\n      65, 132, 92 and 158 all belong to a group of dogs.\n    </rh-cue>\n    <rh-cue start=\"24:00\" end=\"24:04\">\n      So if I wanted to just write an application to tell me if this is a dog\n    </rh-cue>\n    <rh-cue start=\"24:04\" end=\"24:07\">\n      or not, I would probably use that to determine if it's a dog.\n    </rh-cue>\n    <rh-cue start=\"24:08\" end=\"24:10\">\n      But how you measure that is accuracy.\n    </rh-cue>\n    <rh-cue start=\"24:10\" end=\"24:11\">\n      Well, that's where it gets a little bit complicated,\n    </rh-cue>\n    <rh-cue start=\"24:11\" end=\"24:15\">\n      because if you're being really strict about the definition and you're\n    </rh-cue>\n    <rh-cue start=\"24:15\" end=\"24:18\">\n      trying to validate against the data set of labeled images\n    </rh-cue>\n    <rh-cue start=\"24:18\" end=\"24:22\">\n      and I have specific dog breeds or some specific detail\n    </rh-cue>\n    <rh-cue start=\"24:22\" end=\"24:25\">\n      and it doesn't match, well, then the accuracy is going to go down.\n    </rh-cue>\n    <rh-cue start=\"24:25\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"24:25\" end=\"24:29\">\n      That's especially important when we talk about things like compression\n    </rh-cue>\n    <rh-cue start=\"24:29\" end=\"24:30\">\n      and quantization,\n    </rh-cue>\n    <rh-cue start=\"24:30\" end=\"24:34\">\n      which, you know, historically has been difficult for to get adoption\n    </rh-cue>\n    <rh-cue start=\"24:34\" end=\"24:40\">\n      in some domains like health care, where even the hint of accuracy going down\n    </rh-cue>\n    <rh-cue start=\"24:40\" end=\"24:44\">\n      implies that we're not going to be able to help in some small case,\n    </rh-cue>\n    <rh-cue start=\"24:44\" end=\"24:47\">\n      maybe if it's even half a percent of the time\n    </rh-cue>\n    <rh-cue start=\"24:47\" end=\"24:51\">\n      we want to take that that tube is placed incorrectly or that, you know,\n    </rh-cue>\n    <rh-cue start=\"24:51\" end=\"24:54\">\n      that patient's, you know, lung has collapsed or something like that.\n    </rh-cue>\n    <rh-cue start=\"24:54\" end=\"24:58\">\n      And that's something that really prevents adoption of some of these methods\n    </rh-cue>\n    <rh-cue start=\"24:58\" end=\"25:01\">\n      that can really boost performance like quantization.\n    </rh-cue>\n    <rh-cue start=\"25:01\" end=\"25:05\">\n      But if you take that example of sort of different from the dog example\n    </rh-cue>\n    <rh-cue start=\"25:05\" end=\"25:07\">\n      and you think about like segmentation of kidneys.\n    </rh-cue>\n    <rh-cue start=\"25:07\" end=\"25:11\">\n      So if I'm doing kidney segmentation, which is, you know, taking a CT scan\n    </rh-cue>\n    <rh-cue start=\"25:12\" end=\"25:14\">\n      and then trying to pick the pixels out of that\n    </rh-cue>\n    <rh-cue start=\"25:14\" end=\"25:17\">\n      scan that belong to a kidney,\n    </rh-cue>\n    <rh-cue start=\"25:17\" end=\"25:20\">\n      how I measure accuracy may be\n    </rh-cue>\n    <rh-cue start=\"25:20\" end=\"25:24\">\n      how many of those pixels I'm able to detect and how many did I miss?\n    </rh-cue>\n    <rh-cue start=\"25:25\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"25:25\" end=\"25:29\">\n      Missing some of the pixels is maybe not a problem, right,\n    </rh-cue>\n    <rh-cue start=\"25:29\" end=\"25:33\">\n      depending on how you built the application because you still detect the kidney\n    </rh-cue>\n    <rh-cue start=\"25:34\" end=\"25:38\">\n      and maybe you just need to apply padding around the region of interest\n    </rh-cue>\n    <rh-cue start=\"25:38\" end=\"25:41\">\n      so that you don't miss any of the the actual kidney.\n    </rh-cue>\n    <rh-cue start=\"25:42\" end=\"25:45\">\n      When you compress the model and when you quantized the model. But\n    </rh-cue>\n    <rh-cue start=\"25:45\" end=\"25:50\">\n      that requires, you know, data scientist and email engineer somebody to really\n    </rh-cue>\n    <rh-cue start=\"25:51\" end=\"25:52\">\n      they have to be\n    </rh-cue>\n    <rh-cue start=\"25:52\" end=\"25:55\">\n      able to go and apply that after the fact, after the inference\n    </rh-cue>\n    <rh-cue start=\"25:55\" end=\"25:59\">\n      happens to make sure that you're not losing critical information,\n    </rh-cue>\n    <rh-cue start=\"25:59\" end=\"26:02\">\n      because the next step from detecting the kidney may be detecting a tumor.\n    </rh-cue>\n    <rh-cue start=\"26:03\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"26:03\" end=\"26:06\">\n      And so maybe you can use the more optimized model\n    </rh-cue>\n    <rh-cue start=\"26:06\" end=\"26:11\">\n      to detect the kidney, but then you can use a slower model to detect the tumor.\n    </rh-cue>\n    <rh-cue start=\"26:11\" end=\"26:15\">\n      But that also requires somebody to architect and make that decision\n    </rh-cue>\n    <rh-cue start=\"26:15\" end=\"26:16\">\n      or that tradeoff and say,\n    </rh-cue>\n    <rh-cue start=\"26:16\" end=\"26:20\">\n      well, I need to add padding, or I should only use the quantized model\n    </rh-cue>\n    <rh-cue start=\"26:20\" end=\"26:24\">\n      to detect the region of interest for the kidney and then use the model\n    </rh-cue>\n    <rh-cue start=\"26:24\" end=\"26:27\">\n      that takes longer to do the inference\n    </rh-cue>\n    <rh-cue start=\"26:27\" end=\"26:30\">\n      just to find the tumor, which is going to be on a smaller size.\n    </rh-cue>\n    <rh-cue start=\"26:30\" end=\"26:33\">\n      Right. The dimensions are going to be much smaller\n    </rh-cue>\n    <rh-cue start=\"26:33\" end=\"26:35\">\n      once we crop to the region of interest.\n    </rh-cue>\n    <rh-cue start=\"26:35\" end=\"26:40\">\n      But all of those details, that's maybe not easy to explain in a few sentences.\n    </rh-cue>\n    <rh-cue start=\"26:40\" end=\"26:43\">\n      And even the way I explained it is probably really confusing.\n    </rh-cue>\n    <rh-cue start=\"26:45\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"26:45\" end=\"26:46\">\n      I do love that use case.\n    </rh-cue>\n    <rh-cue start=\"26:46\" end=\"26:47\">\n      Like you mentioned, the cropping\n    </rh-cue>\n    <rh-cue start=\"26:47\" end=\"26:50\">\n      even in one such an area that we worked on for another project,\n    </rh-cue>\n    <rh-cue start=\"26:50\" end=\"26:53\">\n      we specifically decided to pix like the image that we had taken\n    </rh-cue>\n    <rh-cue start=\"26:53\" end=\"26:57\">\n      because we knew that we could get the outcome we wanted by even\n    </rh-cue>\n    <rh-cue start=\"26:57\" end=\"27:01\">\n      just using a smaller or less having less resolution in our image.\n    </rh-cue>\n    <rh-cue start=\"27:01\" end=\"27:04\">\n      And therefore, as we transferred it from the mobile device storage device\n    </rh-cue>\n    <rh-cue start=\"27:04\" end=\"27:08\">\n      up into the cloud, we wanted that smaller image just for transfer purposes\n    </rh-cue>\n    <rh-cue start=\"27:08\" end=\"27:11\">\n      and it still we could get the accuracy we needed by a lot of testing.\n    </rh-cue>\n    <rh-cue start=\"27:11\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"27:11\" end=\"27:14\">\n      And one thing that's interesting about that from my perspective is\n    </rh-cue>\n    <rh-cue start=\"27:15\" end=\"27:18\">\n      if you're doing image processing, sometimes it takes a while\n    </rh-cue>\n    <rh-cue start=\"27:18\" end=\"27:20\">\n      for this transaction to occur.\n    </rh-cue>\n    <rh-cue start=\"27:20\" end=\"27:20\">\n      Like I,\n    </rh-cue>\n    <rh-cue start=\"27:20\" end=\"27:24\">\n      I come from a traditional application background, you know, where I'm reading\n    </rh-cue>\n    <rh-cue start=\"27:24\" end=\"27:25\">\n      and writing things from a database\n    </rh-cue>\n    <rh-cue start=\"27:25\" end=\"27:28\">\n      or a message broker or moving data from one place to another.\n    </rh-cue>\n    <rh-cue start=\"27:28\" end=\"27:29\">\n      Those things happen subsequent.\n    </rh-cue>\n    <rh-cue start=\"27:29\" end=\"27:33\">\n      Normally, even with great latency between your data centers, you know,\n    </rh-cue>\n    <rh-cue start=\"27:33\" end=\"27:34\">\n      it's still subsequent.\n    </rh-cue>\n    <rh-cue start=\"27:34\" end=\"27:38\">\n      In most cases, while on a transaction like this, one can actually take 2 seconds\n    </rh-cue>\n    <rh-cue start=\"27:38\" end=\"27:42\">\n      or 4 seconds as it's doing its analysis and actually coming back with, you know,\n    </rh-cue>\n    <rh-cue start=\"27:42\" end=\"27:46\">\n      I think it's a dog, I think it's a kidney, I think it's whatever, and provided me\n    </rh-cue>\n    <rh-cue start=\"27:46\" end=\"27:48\">\n      that accuracy statement.\n    </rh-cue>\n    <rh-cue start=\"27:48\" end=\"27:51\">\n      So that concept of optimization is very important\n    </rh-cue>\n    <rh-cue start=\"27:51\" end=\"27:53\">\n      in the overall application architecture.\n    </rh-cue>\n    <rh-cue start=\"27:53\" end=\"27:56\">\n      Would you agree with that or how do you think about that concept?\n    </rh-cue>\n    <rh-cue start=\"27:56\" end=\"27:56\">\n      Yeah, definitely.\n    </rh-cue>\n    <rh-cue start=\"27:56\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"27:56\" end=\"27:58\">\n      It depends too on the use case.\n    </rh-cue>\n    <rh-cue start=\"27:58\" end=\"28:02\">\n      So if you think about how important it is to reduce the latency\n    </rh-cue>\n    <rh-cue start=\"28:02\" end=\"28:06\">\n      and increase the number of frames per second that you can process when you're\n    </rh-cue>\n    <rh-cue start=\"28:06\" end=\"28:10\">\n      talking about a loss prevention model that's running at a grocery store.\n    </rh-cue>\n    <rh-cue start=\"28:10\" end=\"28:13\">\n      So you want to keep the lines moving.\n    </rh-cue>\n    <rh-cue start=\"28:13\" end=\"28:16\">\n      You don't want every person who's at the self-checkout\n    </rh-cue>\n    <rh-cue start=\"28:16\" end=\"28:19\">\n      to have to wait 5 seconds for every item they scan.\n    </rh-cue>\n    <rh-cue start=\"28:19\" end=\"28:22\">\n      You need it to happen as quickly as possible.\n    </rh-cue>\n    <rh-cue start=\"28:22\" end=\"28:25\">\n      And if sometimes you, you know, the accuracy\n    </rh-cue>\n    <rh-cue start=\"28:25\" end=\"28:28\">\n      decreases slightly or the I'd say the accuracy of the whole pipeline.\n    </rh-cue>\n    <rh-cue start=\"28:28\" end=\"28:32\">\n      So not just looking at the individual model or the individual inference, but\n    </rh-cue>\n    <rh-cue start=\"28:32\" end=\"28:36\">\n      let's say that the the whole pipeline is not as successful at detecting\n    </rh-cue>\n    <rh-cue start=\"28:37\" end=\"28:40\">\n      when somebody steals one item from the self-checkout,\n    </rh-cue>\n    <rh-cue start=\"28:41\" end=\"28:43\">\n      it's not going to be a life threatening situation.\n    </rh-cue>\n    <rh-cue start=\"28:43\" end=\"28:47\">\n      Whereas, you know, being in the hooked up to the X-ray machine\n    </rh-cue>\n    <rh-cue start=\"28:47\" end=\"28:51\">\n      with the two placement model, they might be willing to have the doctor,\n    </rh-cue>\n    <rh-cue start=\"28:51\" end=\"28:54\">\n      the nurse wait 5 seconds to get the result.\n    </rh-cue>\n    <rh-cue start=\"28:55\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"28:55\" end=\"28:58\">\n      They don't need it to happen in 500 milliseconds.\n    </rh-cue>\n    <rh-cue start=\"28:58\" end=\"29:02\">\n      So they're willing their threshold for waiting is a little bit higher.\n    </rh-cue>\n    <rh-cue start=\"29:02\" end=\"29:05\">\n      So that, I think, also drives some of the decision, like\n    </rh-cue>\n    <rh-cue start=\"29:06\" end=\"29:09\">\n      you want to keep people moving through the checkout line\n    </rh-cue>\n    <rh-cue start=\"29:09\" end=\"29:13\">\n      and you can afford to to potentially if you lose a little bit of accuracy here\n    </rh-cue>\n    <rh-cue start=\"29:13\" end=\"29:14\">\n      and there, it's not going to\n    </rh-cue>\n    <rh-cue start=\"29:14\" end=\"29:18\">\n      cost the company that much money or it's not going to be life threatening.\n    </rh-cue>\n    <rh-cue start=\"29:18\" end=\"29:21\">It's going to be worth the tradeoff of keeping the line moving</rh-cue>\n    <rh-cue start=\"29:21\" end=\"29:24\">and not having people leave the store and not check out at all.</rh-cue>\n    <rh-cue start=\"29:24\" end=\"29:27\">And to say, I'm not going to shop today because the line's too long.</rh-cue>\n    <rh-cue start=\"29:30\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"29:30\" end=\"29:32\">There are so many trade offs and enterprise</rh-cue>\n    <rh-cue start=\"29:32\" end=\"29:35\">AML use cases, things like latency, accuracy and availability.</rh-cue>\n    <rh-cue start=\"29:35\" end=\"29:40\">And certainly complexities abound, especially in an obviously ever evolving</rh-cue>\n    <rh-cue start=\"29:40\" end=\"29:43\">technological landscape where we are still very early in the adoption of AML.</rh-cue>\n    <rh-cue start=\"29:44\" end=\"29:47\">And to navigate that complexity, the direct feedback from real world</rh-cue>\n    <rh-cue start=\"29:47\" end=\"29:51\">end users is essential to Ryan and his team at Intel.</rh-cue>\n    <rh-cue start=\"29:52\" end=\"29:54\">What would you say are some of the big hurdles or big</rh-cue>\n    <rh-cue start=\"29:54\" end=\"29:57\">outcomes, big opportunities in that space?</rh-cue>\n    <rh-cue start=\"29:57\" end=\"30:01\">And do you agree that we're kind of still at the very beginning in our infancy,</rh-cue>\n    <rh-cue start=\"30:01\" end=\"30:01\">if you will,</rh-cue>\n    <rh-cue start=\"30:01\" end=\"30:05\">of adopting these technologies and and discovering what they can do for us?</rh-cue>\n    <rh-cue start=\"30:05\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"30:05\" end=\"30:07\">Yeah, I think we're definitely in the infancy</rh-cue>\n    <rh-cue start=\"30:07\" end=\"30:10\">and I think that what we've seen is our customers are evolving</rh-cue>\n    <rh-cue start=\"30:10\" end=\"30:14\">and the people who are deploying on Intel hardware, they're trying to run</rh-cue>\n    <rh-cue start=\"30:14\" end=\"30:16\">more complicated models.</rh-cue>\n    <rh-cue start=\"30:16\" end=\"30:19\">They're the models that are doing object detection or, you know,</rh-cue>\n    <rh-cue start=\"30:19\" end=\"30:22\">detecting defects and, you know, doing segmentation.</rh-cue>\n    <rh-cue start=\"30:23\" end=\"30:27\">You know, in the past you could say, oh, here's a generic model that will do face</rh-cue>\n    <rh-cue start=\"30:27\" end=\"30:31\">detection or person detection or vehicle detection and license plate detection.</rh-cue>\n    <rh-cue start=\"30:32\" end=\"30:33\">And those are sort of like</rh-cue>\n    <rh-cue start=\"30:33\" end=\"30:36\">general purpose models that you can just grab off the shelf and use them.</rh-cue>\n    <rh-cue start=\"30:37\" end=\"30:40\">But now we're moving into like the anomaly scenarios</rh-cue>\n    <rh-cue start=\"30:40\" end=\"30:44\">where I've got my own data and I'm trying to do something very specific</rh-cue>\n    <rh-cue start=\"30:45\" end=\"30:47\">and I'm the only one that has access to this data.</rh-cue>\n    <rh-cue start=\"30:47\" end=\"30:51\">And you don't have a public data set that you can go download</rh-cue>\n    <rh-cue start=\"30:51\" end=\"30:54\">that's under Creative Commons license for, you know, car batteries.</rh-cue>\n    <rh-cue start=\"30:54\" end=\"30:57\">It's, you know, it's just not something that's available.</rh-cue>\n    <rh-cue start=\"30:57\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"30:57\" end=\"31:02\">And so those use cases, the challenge with with training those models</rh-cue>\n    <rh-cue start=\"31:02\" end=\"31:06\">and and getting them optimized is the beginning of the pipeline.</rh-cue>\n    <rh-cue start=\"31:06\" end=\"31:10\">It's the data you have to get the data you have to annotated</rh-cue>\n    <rh-cue start=\"31:10\" end=\"31:12\">and the tools have to exist for you to do that.</rh-cue>\n    <rh-cue start=\"31:12\" end=\"31:15\">And that's part of the problem that we're trying to help solve.</rh-cue>\n    <rh-cue start=\"31:16\" end=\"31:17\">And then the models are getting more complex.</rh-cue>\n    <rh-cue start=\"31:17\" end=\"31:21\">So if you think, you know, just from working with customers recently,</rh-cue>\n    <rh-cue start=\"31:21\" end=\"31:22\">you know, they're no longer</rh-cue>\n    <rh-cue start=\"31:22\" end=\"31:26\">just trying to do image classification and, you know, like is it a dog or a cat?</rh-cue>\n    <rh-cue start=\"31:26\" end=\"31:29\">They've moved on to like 3D point clouds</rh-cue>\n    <rh-cue start=\"31:29\" end=\"31:34\">and, you know, 3D segmentation models and things that are like the speech</rh-cue>\n    <rh-cue start=\"31:34\" end=\"31:39\">synthesis example, doing things these GPT models that are generating,</rh-cue>\n    <rh-cue start=\"31:40\" end=\"31:44\">you know, you, you put a text input and it generates an image for you.</rh-cue>\n    <rh-cue start=\"31:44\" end=\"31:47\">It's just becoming much more advanced, much more sophisticated</rh-cue>\n    <rh-cue start=\"31:48\" end=\"31:50\">and on larger images.</rh-cue>\n    <rh-cue start=\"31:50\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"31:50\" end=\"31:54\">And so things like running super resolution enhancing images, upscaling</rh-cue>\n    <rh-cue start=\"31:54\" end=\"31:59\">images, instead of just trying to take that, you know, 200 by 200 pixel</rh-cue>\n    <rh-cue start=\"32:00\" end=\"32:02\">image and classifying if it's a cat.</rh-cue>\n    <rh-cue start=\"32:02\" end=\"32:05\">Now we're talking about gigantic</rh-cue>\n    <rh-cue start=\"32:05\" end=\"32:09\">huge images that we're processing and that all requires</rh-cue>\n    <rh-cue start=\"32:09\" end=\"32:12\">more resources or more optimized models.</rh-cue>\n    <rh-cue start=\"32:13\" end=\"32:16\">And, you know, every computer vision conference or A.I.</rh-cue>\n    <rh-cue start=\"32:16\" end=\"32:19\">conference, there's there's a new latest and greatest architecture.</rh-cue>\n    <rh-cue start=\"32:19\" end=\"32:22\">There's new research paper, and things are getting adopted much faster.</rh-cue>\n    <rh-cue start=\"32:23\" end=\"32:27\">The lead time for a nurse paper or CV PR</rh-cue>\n    <rh-cue start=\"32:27\" end=\"32:30\">for a company to actually adopt and put those into production.</rh-cue>\n    <rh-cue start=\"32:30\" end=\"32:32\">It's like the time shortens every year.</rh-cue>\n    <rh-cue start=\"32:33\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"32:33\" end=\"32:35\">Well, Ryan, I got to tell you, I could talk to you</rh-cue>\n    <rh-cue start=\"32:35\" end=\"32:39\">literally all day about these topics, the various use cases, the various ways</rh-cue>\n    <rh-cue start=\"32:39\" end=\"32:41\">models are being optimized,</rh-cue>\n    <rh-cue start=\"32:41\" end=\"32:44\">how to put models into a pipeline for average enterprise applications.</rh-cue>\n    <rh-cue start=\"32:44\" end=\"32:47\">I've enjoyed learning about pop and vino and anomalies,</rh-cue>\n    <rh-cue start=\"32:47\" end=\"32:50\">but I'm fascinated by this because I will have a chance to go try this myself.</rh-cue>\n    <rh-cue start=\"32:51\" end=\"32:52\">Taking advantage of Red Hat OpenShift</rh-cue>\n    <rh-cue start=\"32:52\" end=\"32:54\">and taking advantage of our data science platform.</rh-cue>\n    <rh-cue start=\"32:54\" end=\"32:58\">On top of that, I will definitely go be poking at this myself.</rh-cue>\n    <rh-cue start=\"32:58\" end=\"33:00\">So thank you so much for your time today.</rh-cue>\n    <rh-cue start=\"33:00\" voice=\"Ryan Loney\"></rh-cue>\n    <rh-cue start=\"33:00\" end=\"33:00\">Thanks, Burr.</rh-cue>\n    <rh-cue start=\"33:00\" end=\"33:01\">This was a lot of fun.</rh-cue>\n    <rh-cue start=\"33:01\" end=\"33:04\">Thanks for having me.</rh-cue>\n    <rh-cue start=\"33:04\" voice=\"Burr Sutter\"></rh-cue>\n    <rh-cue start=\"33:04\" end=\"33:06\">And you can check out</rh-cue>\n    <rh-cue start=\"33:06\" end=\"33:09\">the full transcript of our conversation and more resources,</rh-cue>\n    <rh-cue start=\"33:09\" end=\"33:12\">like a link to a white paper on open vino and normal lib at Red Hat dot</rh-cue>\n    <rh-cue start=\"33:12\" end=\"33:15\">com slash code Comments Podcast.</rh-cue>\n    <rh-cue start=\"33:15\" end=\"33:19\">This episode was produced by Brant Seminole and Caroline Prickett.</rh-cue>\n    <rh-cue start=\"33:20\" end=\"33:21\">Our sound designer is Christian.</rh-cue>\n    <rh-cue start=\"33:21\" end=\"33:26\">From our audio team includes Lee Day, Stephanie Wunderlich, Mike Esser,</rh-cue>\n    <rh-cue start=\"33:27\" end=\"33:32\">Laura Barnes, Claire Allison, Nick Burns, Aaron Williamson, Karen King,</rh-cue>\n    <rh-cue start=\"33:32\" end=\"33:36\">Booboo House, Rachel Artell, Mike Compton, Ocean</rh-cue>\n    <rh-cue start=\"33:36\" end=\"33:40\">Mathews, Laura Walters, Alex Trabelsi and Victoria Lutton.</rh-cue>\n    <rh-cue start=\"33:41\" end=\"33:43\">I'm your host, Burt Sutter.</rh-cue>\n    <rh-cue start=\"33:43\" end=\"33:45\">Thank you for joining me today on Code Comments.</rh-cue>\n    <rh-cue start=\"33:45\" end=\"33:48\">I hope you enjoyed today's session and today's conversation, and I</rh-cue>\n    <rh-cue start=\"33:48\" end=\"33:52\">look forward to many more in.</rh-cue>\n  </rh-transcript>\n</rh-audio-player>\n\n<h2>Even with more headings</h2>\n<p>This last heading is allowed to go back up to h2, but the player should still take h3 as its root heading level.</p>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\" src=\"customization.js\"></script>\n<!--playground-fold--><link rel=\"stylesheet\" href=\"../rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Heading Levels"
    },
    "demo/heading-levels/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "demo/heading-levels/customization.js": {
      "content": "import '@rhds/elements/rh-audio-player/rh-audio-player.js';\nconst form = document.querySelector('form');\nconst player = document.querySelector('rh-audio-player');\nconst { poster } = player;\n\n/**\n * update audio player demo based on form selections\n */\nfunction updateDemo() {\n  const colorPalette = ['cyan', 'light'].includes(form.palette.value) ?\n    'light' : 'dark';\n  const colorClass =\n    ['cyan', 'purple', 'purple img'].includes(form.palette.value) ? form.palette.value : '';\n  player.poster = !form.poster.checked || form.palette.value === 'purple img' ? undefined : poster;\n  player.layout = form.layout.value !== '' ? form.layout.value : undefined;\n  if (colorPalette === player.colorPalette) {\n    const oldOn = player.colorPalette;\n    player.colorPalette = oldOn === 'dark' ? 'light' : 'dark';\n  }\n  player.setAttribute('class', colorClass);\n  player.colorPalette = colorPalette;\n  player.hasAccentColor = ['cyan', 'purple', 'purple img'].includes(form.palette.value);\n  player.requestUpdate();\n}\n\nif (form) {\n  form.addEventListener('input', updateDemo);\n  updateDemo();\n}\n",
      "hidden": true
    },
    "demo/language-localization/index.html": {
      "contentType": "text/html",
      "selected": false,
      "content": "<h2>rh-audio-player: Language Localization</h2>\n\n<!-- Options for demo -->\n<form>\n  <p>Options:</p>\n  <ul>\n    <li><label>Poster: <input name=\"poster\" type=\"checkbox\" checked=\"\"></label></li>\n    <li><label>Color Palette:\n        <select name=\"palette\">\n          <option value=\"light\" selected=\"\">Light</option>\n          <option value=\"dark\">Dark</option>\n          <option value=\"purple\">Purple</option>\n          <option value=\"purple-img\">Purple with Image</option>\n          <option value=\"cyan\">Cyan </option>\n        </select>\n      </label></li>\n    <li>\n      <label>Layout:\n        <select name=\"layout\">\n          <option value=\"full\" selected=\"\">Full</option>\n          <option value=\"compact-wide\">Compact Wide</option>\n          <option value=\"compact\">Compact</option>\n          <option value=\"\">Mini</option>\n        </select>\n      </label>\n    </li>\n  </ul>\n</form>\n\n<rh-audio-player lang=\"es\" layout=\"full\" poster=\"https://www.redhat.com/cms/managed-files/img-clh-s4e1-hero-455x539_0.png\">\n  <p slot=\"series\">Temporada 4, Episodio 1</p>\n  <h3 slot=\"title\">Minicomputadoras: el alma de las mÃ¡quinas de antes</h3>\n  <audio crossorigin=\"anonymous\" slot=\"media\" control=\"\" srclang=\"es\" src=\"https://cdn.simplecast.com/audio/ec894038-2e91-449e-9bff-e7ebc323c3e6/episodes/7507a7a1-7340-43f9-bde2-bb0645646ff6/audio/ab8ed5c7-9fdc-4a39-8384-141d66f02c46/default_tc.mp3\"></audio>\n  <rh-audio-player-about slot=\"about\" label=\"Notas del podcast\">\n    <p>SÃ­, es cierto, las minicomputadoras no caben en tu bolsillo, pero en su momento representaron un avance importante porque redujeron el espacio que necesitaban sus antecesoras, las mainframes,\n      que ocupaban habitaciones enteras. AdemÃ¡s, abrieron la posibilidad de que las computadoras personales cupieran en una bolsa y de que, posteriormente, se convirtieran en el telÃ©fono que traes en\n      tu bolsillo. </p>\n    <p>Las computadoras de 16 bits cambiaron el mundo de la tecnologÃ­a de la informaciÃ³n en los aÃ±os 70. Gracias a ellas, las empresas tuvieron la posibilidad de darle a cada ingeniero su propia\n      mÃ¡quina. Pero los avances aÃºn no eran suficientes; todavÃ­a faltaba que llegaran las versiones de 32 bits. </p>\n    <p>Carl Alsing y Jim Guyer nos hablan del trabajo que realizaron en Data General para crear una nueva y revolucionaria mÃ¡quina de 32 bits. Y aunque ahora esos esfuerzos son toda una leyenda, en su\n      momento se realizaron en secreto. âEagleâ era el nombre clave de la computadora que diseÃ±aron, cuyo primer propÃ³sito era competir con otra mÃ¡quina que estaba desarrollando otro equipo de la\n      misma empresa. Los ingenieros nos hablan de las polÃ­ticas corporativas y nos explican todas las tramas necesarias para que el proyecto pudiera seguir su curso, e incluso nos dicen cÃ³mo lograron\n      que las restricciones jugaran a su favor. Neal Firth nos cuenta cÃ³mo viviÃ³ un proyecto muy emocionante pero exigente, en que nuestros hÃ©roes trabajaron juntos por pura voluntad, sin ninguna\n      expectativa de fama ni fortuna. Y los tres nos mencionan que la historia quedÃ³ inmortalizada en el libro clÃ¡sico de ingenierÃ­a de Tracy Kidder, <em>El alma de una nueva mÃ¡quina,</em> que se basa\n      en hechos reales.</p>\n  </rh-audio-player-about>\n  <rh-transcript slot=\"transcript\" label=\"TranscripciÃ³n\">\n    <rh-cue start=\"00:03\" voice=\"Presentadora\">\n      CorrÃ­a el aÃ±o de 1978 y en el sector de las minicomputadoras habÃ­a una guerra a punto de estallar. Apenas un aÃ±o antes, Digital Equipment Corporation, o DEC, habÃ­a lanzado su computadora VAX\n      11 780 de 32 bits. TenÃ­a una capacidad mucho mayor que las mÃ¡quinas de 16 bits del mercado. Las ventas de la VAX pronto arrasaron con las de la competencia, que ofrecÃ­a computadoras mÃ¡s\n      lentas. A Data General, la archienemiga de DEC, le urgÃ­a diseÃ±ar una nueva mÃ¡quina capaz de competir con la VAX. Necesitaba su propia computadora de 32 bits y la necesitaba ya, pero la\n      competencia entre Data General y DEC no era el Ãºnico conflicto del momento. TambiÃ©n habÃ­a una disputa territorial en el interior de Data General, y el resultado de ambas guerras serÃ­a la\n      creaciÃ³n de una computadora increÃ­ble, en circunstancias igual de increÃ­bles. Una laptop de 13 pulgadas pesa como kilo y medio. Hoy en dÃ­a damos por hecho la portabilidad y la practicidad de\n      nuestras computadoras, pero en la dÃ©cada de 1970 la mayorÃ­a eran mainframes del tamaÃ±o de una habitaciÃ³n; eran aparatos que costaban millones de dÃ³lares y pesaban varias toneladas. Luego,\n      cuando se desplomaron los costos del hardware, comenzÃ³ la carrera para desarrollar computadoras mÃ¡s pequeÃ±as, mÃ¡s rÃ¡pidas y mÃ¡s baratas. La minicomputadora abriÃ³ la posibilidad de que los\n      ingenieros y los investigadores tuvieran su propia terminal, y nos trajo a donde estamos en la actualidad.\n    </rh-cue>\n    <rh-cue start=\"01:37\" voice=\"Presentadora\">\n      En la temporada pasada de Command Line Heroes en espaÃ±ol, analizamos un Ã¡rea clave para el desarrollo del software: el mundo de los lenguajes de programaciÃ³n. Hablamos de su historia, de los\n      problemas que resolvieron y de su evoluciÃ³n a travÃ©s del tiempo. Abordamos lenguajes como JavaScript, Python, C, Perl, COBOL y Go. En esta temporada, que es la cuarta, por si alguien lleva la\n      cuenta, vamos a profundizar en el hardware en que se ejecuta nuestro software. Te vamos a contar siete historias maravillosas sobre las personas y los equipos que se atrevieron a cambiar las\n      reglas del hardware. Piensa en la laptop que estÃ¡ en tu escritorio, o en el telÃ©fono que traes en el bolsilloâ¦ Los hÃ©roes de la lÃ­nea de comandos siempre dejan el alma en tu hardware; con su\n      pasiÃ³n por el diseÃ±o informÃ¡tico y su ingenio para que cada pieza se vuelva realidad, han revolucionado la forma en que programamos hoy en dÃ­a.\n    </rh-cue>\n    <rh-cue start=\"02:36\" voice=\"Presentadora\">\n      Esto es Command Line Heroes en espaÃ±ol, un podcast original de Red Hat.\n    </rh-cue>\n    <rh-cue start=\"02:45\" voice=\"Presentadora\">\n      El primer episodio de esta temporada cuenta la carrera contrarreloj de un equipo de ingenieros que tenÃ­an que diseÃ±ar, depurar y entregar una computadora de vanguardia. Su trabajo se\n      convirtiÃ³ en el tema principal del bestseller El alma de una nueva mÃ¡quina, de Tracy Kidder, que posteriormente se harÃ­a acreedor al premio Pulitzer y que habla de muchos de los invitados de\n      este episodio.\n    </rh-cue>\n    <rh-cue start=\"03:07\" voice=\"Presentadora\">\n      Pero volvamos a Data General. El presidente de la compaÃ±Ã­a, Ed de Castro, habÃ­a trazado un plan para competir con DEC. DividiÃ³ al departamento de ingenierÃ­a y trasladÃ³ a una parte del equipo\n      de la sede de Westboro, Massachusetts, en Estados Unidos, a una nueva oficina que estaba en Carolina del Norte. Â¿Su misiÃ³n? DiseÃ±ar una computadora avanzada de 32 bits que hiciera trizas a la\n      VAX. El proyecto se llamaba Fountainhead, y de Castro le dio apoyo y recursos casi ilimitados. Fountainhead iba a ser la salvaciÃ³n de la empresa. Los pocos ingenieros que se quedaron en\n      Massachusetts se sintieron terriblemente menospreciados. SabÃ­an que eran capaces de crear una computadora que destrozara a la VAX, y que probablemente serÃ­a mejor que la de Fountainhead, pero\n      de Castro no les daba la oportunidad. AsÃ­ que Tom West, que era el lÃ­der del grupo, decidiÃ³ ocuparse personalmente del asunto. El ingeniero en ComputaciÃ³n Tom West, de formaciÃ³n autodidacta,\n      dirigÃ­a el departamento Eclipse de Data General. Eclipse era la gama de minicomputadoras de 16 bits mÃ¡s exitosa de Data General. Tom sabÃ­a fabricar y distribuir computadoras, y tambiÃ©n sabÃ­a\n      lo que querÃ­a el mercado. DespuÃ©s de poner en marcha el proyecto Fountainhead, de Castro les pidiÃ³ a los demÃ¡s ingenieros que siguieran mejorando la gama de productos del aÃ±o anterior. Ni a\n      Tom ni a los demÃ¡s les convencÃ­a la idea.\n    </rh-cue>\n    <rh-cue start=\"04:31\" voice=\"Carl Alsing\">\n      No nos hacÃ­a ninguna gracia. Algunos decidieron cambiar de empleo y otros estÃ¡bamos deprimidos y preocupados por nuestras carreras; no nos sentÃ­amos nada entusiasmados. Y nos imaginÃ¡bamos que\n      el otro grupo no lo iba a lograr.\n    </rh-cue>\n    <rh-cue start=\"04:46\" voice=\"Presentadora\">\n      Carl Alsing era el gerente del grupo de microprogramaciÃ³n de Data General. Era el segundo al mando, despuÃ©s de Tom. AsÃ­ que los dos decidieron empezar su propio proyecto.\n    </rh-cue>\n    <rh-cue start=\"04:56\" voice=\"Carl Alsing\">\n      Iba a ser un diseÃ±o completamente nuevo, con las tÃ©cnicas mÃ¡s avanzadas, para diseÃ±ar una computadora de 32 bits que superara a la VAX de DEC. Preparamos nuestra propuesta, se la presentamos\n      al presidente, Ed de Castro, que nos dice: âNo, para nada. El grupo de Carolina del Norte estÃ¡ en eso. No se preocupenâ. Nos desanimamos, pero se nos ocurriÃ³ otra propuesta a la que le pusimos\n      VÃ­ctor. Buscamos formas de mejorar el producto del aÃ±o pasado. Le pusimos un pequeÃ±o interruptor, un bit de modo en el sistema; si lo encendÃ­as permitÃ­a que la computadora funcionara como una\n      minicomputadora moderna de 32 bits, pero lenta. Se lo llevamos a Ed de Castro y se lo presentamos. Y total que nos dijo: âEso es un bit de modo. No quiero ni ver diseÃ±os con bits de modo. La\n      que se encarga de los nuevos diseÃ±os es Carolina del Norteâ. Entonces otra vez nos desanimamos, y creo que en fue en ese momento que Tom West decidiÃ³ hacer algo a escondidas.\n    </rh-cue>\n    <rh-cue start=\"06:06\" voice=\"Presentadora\">\n      A Tom se le ocurrieron dos cosas. Una de ellas era para de Castro. Iban a mejorar la antigua lÃ­nea de productos Eclipse: la harÃ­an un poco mÃ¡s rÃ¡pida, le agregarÃ­an unos cuantos botones, le\n      cambiarÃ­an el color. Tom lo presentÃ³ como una especie de plan b, en caso de que algo saliera mal en Carolina del Norte. De Castro lo aprobÃ³. Pero a su equipo Tom le contÃ³ otra historia, una\n      mÃ¡s interesante.\n    </rh-cue>\n    <rh-cue start=\"06:32\" voice=\"Carl Alsing\">\n      Tom West nos propuso diseÃ±ar una computadora moderna, muy buena, que fuera totalmente compatible con las anteriores y que pudiera manejar lo Ãºltimo en alta tecnologÃ­a. Iba a tener memoria\n      virtual, 32 bits, cÃ³digos de correcciÃ³n de errores y esas cosas. Multitareas, multiprocesamiento, mucha memoria. âOigan, vamos a diseÃ±ar una computadora nueva que se va a comer vivo al\n      mercadoâ.\n    </rh-cue>\n    <rh-cue start=\"07:04\" voice=\"Presentadora\">\n      El cÃ³digo de esta maravilla informÃ¡tica: la âEagleâ. Actualmente parece que no hay lÃ­mites con lo que podemos hacer con la memoria integrada de nuestra computadora, pero en ese entonces,\n      pasar de 16 a 32 bits era un paso enorme. De un dÃ­a para otro, el espacio de direcciones habÃ­a pasado de 65 mil bytes de informaciÃ³n a mÃ¡s de 4 mil millones. Y con ese aumento, el software\n      podÃ­a procesar mayores cantidades de datos. Esto generÃ³ dos grandes desafÃ­os para las empresas de informÃ¡tica: obviamente habÃ­a que pasar de 16 a 32 bits, pero ademÃ¡s habÃ­a que dejar conformes\n      a los antiguos clientes, que todavÃ­a utilizaban el software anterior. AsÃ­ que habÃ­a que desarrollar una computadora que pudiera funcionar con el viejo software; una computadora de 32 bits que\n      fuera compatible con lo anterior. La VAX tenÃ­a una gran potencia, pero no tenÃ­a ninguna soluciÃ³n elegante para el segundo problema. Tom estaba decidido a que su Eagle fuera la respuesta.\n    </rh-cue>\n    <rh-cue start=\"08:14\" voice=\"Presentadora\">\n      La Eagle estaba escondida en el sÃ³tano del edificio Westborough, 14 AB. Tom le pidiÃ³ a Carl que dirigiera la microcodificaciÃ³n. Carl nombrÃ³ a Chuck Holland como gerente de los programadores,\n      que se pusieron el nombre de Micro Kids. Mientras, Ed Rasala supervisarÃ­a el hardware. Y Ed designÃ³ a Ken Holberger para que dirigiera al equipo, al que llamaron, muy apropiadamente, los Hardy\n      Boys. Tom encontrÃ³ un aliado: el vicepresidente de IngenierÃ­a, Carl Carman. Carman tambiÃ©n tenÃ­a cuentas pendientes con De Castro, que se habÃ­a negado a ponerlo a cargo del grupo de Carolina\n      del Norte.\n    </rh-cue>\n    <rh-cue start=\"08:51\" voice=\"Carl Alsing\">\n      Carl Carman sabÃ­a en quÃ© andÃ¡bamos, pero no le dijo nada a su jefe. Ãl era el que nos financiaba; el problema es que necesitÃ¡bamos ingenieros muy, pero muy buenos, pero tenÃ­amos que mantener\n      bajos los salarios. AsÃ­ que decidimos contratar estudiantes universitarios. Una de las ventajas es que no conocen tus lÃ­mites. Creen que puedes hacer cualquier cosa.\n    </rh-cue>\n    <rh-cue start=\"09:15\" voice=\"Presentadora\">\n      Jim Guyer habÃ­a egresado de la universidad dos aÃ±os antes y trabajaba en Data General cuando lo pusieron a cargo de los Hardy Boys.\n    </rh-cue>\n    <rh-cue start=\"09:21\" voice=\"Jim Guyer\">\n      La computadora que estaban desarrollando en Carolina del Norte tenÃ­a una tecnologÃ­a informÃ¡tica mucho mÃ¡s avanzada, casi como una mainframe. Y bueno, digamos que en esa Ã©poca no era cualquier\n      cosa ponerse a competir con IBM y las demÃ¡s empresas de mainframes. CreÃ­amos que tenÃ­amos ventaja porque nuestro proyecto no era tan ambicioso y estÃ¡bamos muy, muy concentrados en una\n      implementaciÃ³n clara, sencilla y elegante, de bajo costo, pocos componentes... cosas asÃ­.\n    </rh-cue>\n    <rh-cue start=\"09:51\" voice=\"Presentadora\">\n      Bajo costo, diseÃ±o sencillo... Eso los hizo entender que tendrÃ­an que usar el firmware para controlar todo. Mientras mÃ¡s funciones lograran implementar en el firmware en vez del hardware, mÃ¡s\n      barato y flexible serÃ­a el resultado\n    </rh-cue>\n    <rh-cue start=\"10:03\" voice=\"Presentadora\">\n      AdemÃ¡s, podrÃ­an hacer los cambios a medida que se necesitaran. Actualmente nos parece lÃ³gico porque asÃ­ funcionan las computadoras, pero en 1978 era algo completamente nuevo.\n    </rh-cue>\n    <rh-cue start=\"10:15\" voice=\"Carl Alsing\">\n      El diseÃ±o que estÃ¡bamos haciendo era algo bÃ¡sico. Lo que querÃ­amos era encontrar formas sencillas y directas de hacer las cosas, sin complicaciones, porque sabÃ­amos que no podÃ­amos terminar\n      diseÃ±ando una computadora grande y cara. NecesitÃ¡bamos usar pocas tarjetas, pocos circuitos, y de hecho eso nos ayudaba para que fuera rÃ¡pida. No es lo mismo diseÃ±ar un producto seguro y sin\n      riesgos, que diseÃ±ar un producto exitoso. Y no nos importaban los riesgos. Nos importaba el Ã©xito. QuerÃ­amos que nuestra computadora fuera rÃ¡pida y barata, y querÃ­amos diseÃ±arla rÃ¡pido. AsÃ­\n      que le pusimos unas tres o cuatro tarjetas, lo mÃ­nimo que podÃ­amos de hardware, y lo compensamos con el firmware.\n    </rh-cue>\n    <rh-cue start=\"11:06\" voice=\"Presentadora\">\n      Pero el equipo de la Eagle se enfrentaba a varios obstÃ¡culos difÃ­ciles de superar. La VAX era la computadora de 32 bits con mejor rendimiento del mundo. La Eagle necesitaba estar a la altura.\n      Pero ademÃ¡s, tenÃ­a que ser compatible con la arquitectura anterior de 16 bits de Data General. Para lograr todo eso, pero con menos tiempo y dinero que los demÃ¡s equipos, habÃ­a que apostarle\n      mucho a la Eagle. Pero el equipo de Tom West estaba dispuesto a jugÃ¡rselo todo.\n    </rh-cue>\n    <rh-cue start=\"11:32\" voice=\"Jim Guyer\">\n      HabÃ­a dos sistemas que funcionaban las 24 horas del dÃ­a, los 7 dÃ­as de la semana, y tenÃ­amos dos turnos de ingenieros que trabajaban en eso. Todos necesitÃ¡bamos entender cÃ³mo funcionaba todo.\n      AsÃ­ que tuvimos que aprender quÃ© hacÃ­a cada una de las piezas que construÃ­an los demÃ¡s. Me costÃ³ mucho trabajo, pero al mismo tiempo aprendÃ­ muchÃ­simo. Todos participÃ¡bamos en el trabajo de\n      los demÃ¡s y pensÃ¡bamos: âÂ¿CuÃ¡l es el siguiente paso para resolver este problema? Â¿En quÃ© hay que fijarse?â Todos revisÃ¡bamos los diagramas de circuitos y demÃ¡s documentos para tratar de\n      entender: âA ver, fÃ­jate en esta seÃ±al, ve el estado de la computadora, revisa la secuencia de pasos del microcÃ³digo. Â¿SÃ­ estÃ¡ haciendo lo que tiene que hacer? Uy, espÃ©rense, va para el otro\n      lado. Ay, Â¿pero por quÃ© hizo eso?â\n    </rh-cue>\n    <rh-cue start=\"12:13\" voice=\"Carl Alsing\">\n      Lo tomÃ¡bamos muy en serio, era parte de la Ã©tica de trabajo. El ambiente era intenso. A veces habÃ­a discusiones sobre la manera de hacer las cosas. Por ejemplo, tal vez habÃ­a una forma un\n      poco mÃ¡s cara y otra que era mÃ¡s barata pero no tan rÃ¡pida o eficaz. Y habÃ­a discusiones acaloradas y reuniones en las que tenÃ­amos que esforzarnos por llegar a un acuerdo. Pero al final\n      logrÃ¡bamos tomar una decisiÃ³n. Y empezÃ¡bamos a trabajar juntos.\n    </rh-cue>\n    <rh-cue start=\"12:44\" voice=\"Carl Alsing\">\n      TrabajÃ¡bamos dÃ­a y noche, nos repartÃ­amos las horas que se necesitaban para diseÃ±ar el prototipo. Solo tenÃ­amos dos prototipos, y era muy importante que los dos equipos trabajaran en ellos.\n      Algunos trabajaban en la noche, otros trabajaban en el dÃ­a, y ya empezÃ¡bamos a cansarnos. Pero estÃ¡bamos muy motivados, asÃ­ que sentÃ­amos mucha satisfacciÃ³n. AsÃ­ que nadie se quejaba mucho de\n      las condiciones laborales.\n    </rh-cue>\n    <rh-cue start=\"13:11\" voice=\"Presentadora\">\n      Las condiciones laborales. Algunos relatos de esa Ã©poca dicen que, para que el equipo funcionara, Tom West puso en prÃ¡ctica una cosa que se llama âla gestiÃ³n de los hongosâ: si les das de\n      comer cualquier porquerÃ­a y los mantienes en la oscuridad vas a verlos crecer. Estaban encerrados en un espacio de trabajo abarrotado y caluroso, asÃ­ que las horas se hacÃ­an largas y los\n      plazos eran poco realistas. Dicen que Tom era enigmÃ¡tico, frÃ­o, indiferente. Uno de los ingenieros incluso lo llamaba el âPrÃ­ncipe de las Tinieblasâ. Â¿Pero a Tom West le importaba tanto lograr\n      el Ã©xito que se aprovechÃ³ de su equipo? Â¿SacrificÃ³ el bienestar de los Micro Kids y los Hardy Boys para diseÃ±ar la computadora perfecta?\n    </rh-cue>\n    <rh-cue start=\"13:56\" voice=\"Jim Guyer\">\n      Era interesante trabajar con Tom. Porque tenÃ­a muchas expectativas, pero no te daba suficientes instrucciones. Esperaba que entendieras lo que tenÃ­as que hacer, y si no, pues quÃ© pena, te\n      sacaba del equipo.\n    </rh-cue>\n    <rh-cue start=\"14:10\" voice=\"Presentadora\">\n      Los que daban instrucciones eran Carl y Ed, los gerentes de lÃ­nea que trabajaban codo a codo con Jim y el resto del equipo. Pero estos jÃ³venes ingenieros tambiÃ©n buscaban el Ã©xito, y les\n      gustaba tener la oportunidad de resolver las cosas ellos mismos.\n    </rh-cue>\n    <rh-cue start=\"14:26\" voice=\"Jim Guyer\">\n      Yo me ganÃ© el primer lugar de los Micro Kids por aguantar toda la noche sin dormir. QuiÃ©n sabe, a lo mejor Ã©ramos jÃ³venes, empezÃ¡bamos nuestra vida profesional, Ã©ramos bravucones, muy\n      seguros, y no entendÃ­amos nada de nada. ConfiÃ¡bamos en nosotros mismos. Nos sentÃ­amos muy inteligentes, creÃ­amos que podÃ­amos resolver todo, y yo supongo que el ego de los demÃ¡s tambiÃ©n nos...\n      nos alimentaba, en cierto sentido. Yo me la pasaba muy bien. Yo creo que la mayorÃ­a de nosotros nos divertÃ­amos mucho.\n    </rh-cue>\n    <rh-cue start=\"14:56\" voice=\"Presentadora\">\n      Carl no estÃ¡ de acuerdo con lo de la gestiÃ³n de los hongos. En su opiniÃ³n no estaban en la oscuridad, sino al contrario: todos sabÃ­an exactamente lo que estaba pasando y lo que se esperaba.\n      Los directores eran los que no sabÃ­an. Al mismo tiempo, Tom West estaba bajo una enorme presiÃ³n de varios frentes, y se la transmitÃ­a al grupo.\n    </rh-cue>\n    <rh-cue start=\"15:18\" voice=\"Carl Alsing\">\n      Tom mantenÃ­a en secreto la verdadera finalidad del proyecto. AsÃ­ que no hablaba mucho con los ingenieros, se mantenÃ­a a distancia y obviamente les decÃ­a que no hablaran del proyecto fuera del\n      grupo, ni siquiera en su casa. Les decÃ­a que ni mencionaran la palabra Eagle. AsÃ­ que tambiÃ©n dejÃ¡bamos muy claro que esto era muy urgente, que tenÃ­amos que lograrlo en un aÃ±o, que la\n      competencia ya estaba en el mercado, y si querÃ­amos salir al mercado en medio del pico de ventas, tenÃ­amos que lograrlo ya. Estaban muy estresados, y se esperaba que trabajaran en la noche y\n      los fines de semana; se esperaba que olvidaran los picnics con la familia; no habÃ­a tiempo para nada que no fuera del trabajo.\n    </rh-cue>\n    <rh-cue start=\"16:06\" voice=\"Presentadora\">\n      Como me daba curiosidad saber cÃ³mo era trabajar en las trincheras del Edificio 14 AB, me sentÃ© a conversar con Neal Firth, que era uno de los Micro Kids. Acababa de salir de la universidad\n      cuando se incorporÃ³ al equipo.\n    </rh-cue>\n    <rh-cue start=\"16:20\" voice=\"Presentadora\">\n      Â¿CÃ³mo era trabajar para Tom West? Â¿Te comunicabas mucho con Ã©l?\n    </rh-cue>\n    <rh-cue start=\"16:24\" voice=\"Neal Firth\">\n      No tanto. Era como un fantasma. A veces lo veÃ­amos por ahÃ­. Intentaba no interferir para que hiciÃ©ramos lo que necesitÃ¡bamos y alcanzÃ¡ramos los objetivos. El proyecto era algo completamente\n      nuevo en comparaciÃ³n con lo que hacÃ­a Data General, y Tom no querÃ­a imponernos nada forzoso respecto a la generaciÃ³n anterior de procesadores.\n    </rh-cue>\n    <rh-cue start=\"16:49\" voice=\"Presentadora\">\n      Suena intenso, suena a que habÃ­a que trabajar sin descanso y a que siempre habÃ­a algo que resolver. Â¿CÃ³mo te sentÃ­as de que no tuvieran el tiempo necesario para lograrlo?\n    </rh-cue>\n    <rh-cue start=\"16:57\" voice=\"Neal Firth\">\n      Sinceramente no nos preocupaba. En realidad la falta de tiempo no era problema. Nosotros nos tomÃ¡bamos el tiempo que hiciera falta para lograr el resultado. Por eso necesitÃ¡bamos que nuestras\n      esposas nos apoyaran y fueran comprensivas, porque no siempre aceptaban todo. Era mÃ¡s o menos lo que sucedÃ­a con las personas de Silicon Valley de esa Ã©poca, o con Jobs y Wozniak: âvamos a\n      ponernos a hacer esto hasta terminarloâ. No vivÃ­amos en el mismo departamento ni nos sentÃ¡bamos en el piso a escribir cÃ³digo, pero tenÃ­amos mucho en comÃºn con ellos.\n    </rh-cue>\n    <rh-cue start=\"17:35\" voice=\"Presentadora\">\n      Â¿Y quÃ© te impulsaba a seguir adelante? Â¿Por quÃ© estabas tan motivado?\n    </rh-cue>\n    <rh-cue start=\"17:39\" voice=\"Neal Firth\">\n      La verdad solo era la posibilidad de resolver algÃºn problema. Siempre me habÃ­an gustado los acertijos, los problemas que necesitaban soluciÃ³n. De hecho, asÃ­ Ã©ramos casi todos. Todos\n      compartÃ­amos eso, y todos lo disfrutÃ¡bamos. Nos motivaba resolver los problemas, solucionar esas cosas, descubrir una manera nueva de hacer algo.\n    </rh-cue>\n    <rh-cue start=\"18:01\" voice=\"Presentadora\">\n      Â¿Y cuÃ¡l fue el momento del proyecto que no vas a poder olvidar?\n    </rh-cue>\n    <rh-cue start=\"18:05\" voice=\"Neal Firth\">\n      Fue... Ya llevÃ¡bamos mucho tiempo con el proyecto, y estÃ¡bamos ejecutando el simulador de microcÃ³digo. Y resulta que lo que se estaba ejecutando era la propuesta del simulador de producciÃ³n,\n      que ya llevaba como 10 o 12 horas funcionando. Y de repente aparece la letra E en la consolaâ¦ Nos esperamos un rato y de pronto aparece otra letra, y luego otra. Y entonces nos dimos cuenta de\n      que lo que estÃ¡bamos ejecutando como cÃ³digo de prueba era el diagnÃ³stico que estÃ¡bamos diseÃ±ando para que se ejecutara. AsÃ­ que el simulador ejecutaba el microcÃ³digo, y ya habÃ­a empezado a\n      imprimir letras como si realmente estuviera funcionando. Era mil veces mÃ¡s lento que en la vida real, o sea, era mÃ¡s lento que cuando se lanzÃ³ realmente, pero ese fue uno de los momentos que\n      nunca voy a olvidar.\n    </rh-cue>\n    <rh-cue start=\"19:02\" voice=\"Presentadora\">\n      Y ahora que lo piensas, Â¿te parece que te explotaron?\n    </rh-cue>\n    <rh-cue start=\"19:07\" voice=\"Neal Firth\">\n      No. O sea, yo sabÃ­a. Yo sabÃ­a lo que estaba pasando. Entoncesâ¦ no. No me siento explotado. En realidad, mis expectativas... yo nunca hubiera esperado participar en un proyecto tan importante\n      justo al salir de la universidad, ni tener la oportunidad de desempeÃ±ar un papel tan interesante en un proyecto asÃ­.\n    </rh-cue>\n    <rh-cue start=\"19:31\" voice=\"Presentadora\">\n      Me gustarÃ­a saber tu opiniÃ³n sobre el sacrificio que requiere el inventar algo, porque cuando hacemos algo importante, en general hay que renunciar a algo para lograrlo, Â¿no? Para lograr\n      algo, hay que renunciar a algo, Â¿no?Â¿Ese fue el caso? Y si sÃ­, Â¿a quÃ© tuviste que renunciar?\n    </rh-cue>\n    <rh-cue start=\"19:48\" voice=\"Neal Firth\">\n      Yo no creo que haya tenido la conciencia de que iba a renunciar a algo. MÃ¡s bien creo que lo que pasÃ³ fue que empecÃ© a ser un poco mÃ¡s consciente de lo que estaba haciendo, y de que eso\n      afectaba a los que me rodeaban.\n    </rh-cue>\n    <rh-cue start=\"20:03\" voice=\"Neal Firth\">\n      Pero para mÃ­ no... no era un sacrificio, y las personas que me rodeaban lo vivÃ­an como algo normal; asÃ­ son las cosas y punto. A mÃ­ me han contado cosas horribles de lo que se vive hoy:\n      amanece, te despiertas, te inyectas cafÃ©, muerdes un pedazo de pizza o cualquier cosa... y empiezas a escribir cÃ³digo hasta que te quedas dormido encima del teclado. Y al dÃ­a siguiente, igual.\n    </rh-cue>\n    <rh-cue start=\"20:35\" voice=\"Neal Firth\">\n      Nosotros no hacÃ­amos tantos sacrificios. Digo, yo seguÃ­a casado, tenÃ­a amigos, los veÃ­a... SÃ­, no era un trabajo de nueve a cinco, pero me permitiÃ³ obtener muchos logros personales y\n      tÃ©cnicos, y pude compartirlos con mi esposa, mi hermana, mi mamÃ¡, mi papÃ¡ y mi suegro. O sea, mi familia lo apreciaba.\n    </rh-cue>\n    <rh-cue start=\"20:59\" voice=\"Presentadora\">\n      SÃ­. Â¿Y cuÃ¡l es el secreto para lograr algo maravilloso?\n    </rh-cue>\n    <rh-cue start=\"21:06\" voice=\"Neal Firth\">\n      Â¿Para lograr algo maravilloso? QuÃ© interesante. Creo que la cosa es que quien participe lo haga porque quiere, no porque busca logros, fama o dinero. Porque esas son cosas muy fugaces y...\n      casi nunca te dejan satisfecho. Pero si la idea es alcanzar un objetivo, y colaboras con muchas personas y lo logras, ahÃ­ vas a saber lo que es la satisfacciÃ³n.\n    </rh-cue>\n    <rh-cue start=\"21:42\" voice=\"Presentadora\">\n      Neal Firth era uno de los Micro Kids del proyecto Eagle. Hoy en dÃ­a es el presidente de VIZIM Worldwide, que es una empresa de software.\n    </rh-cue>\n    <rh-cue start=\"21:57\" voice=\"Presentadora\">\n      Como bien dice el libro de Tracy Kidder, la indiferencia y el distanciamiento de Tom West eran a propÃ³sito. Era un intento de mantener la cabeza despejada, por encima de toda la chÃ¡chara\n      diaria, para conservar intacto el objetivo de la Eagle. Pero lo que mÃ¡s querÃ­a era proteger al equipo, aislarlo de la polÃ­tica y de los estira y afloja corporativos de su entorno. TambiÃ©n\n      protegiÃ³ a los Micro Kids y a los Hardy Boys de las ideas preconcebidas de lo que se podÃ­a lograr.\n    </rh-cue>\n    <rh-cue start=\"22:28\" voice=\"Presentadora\">\n      En 1980 se terminÃ³ el proyecto Eagle. Un aÃ±o despuÃ©s de lo que Tom habÃ­a prometido, pero se logrÃ³, a diferencia de Fountainhead. Y tal como pensaba el equipo sÃ©nior, el objetivo de\n      Fountainhead no se logrÃ³, y el proyecto se quedÃ³ olvidado en algÃºn cajÃ³n. Bill Foster, que en ese entonces era director de desarrollo de software, nos cuenta las dificultades de Fountainhead.\n\n    </rh-cue>\n    <rh-cue start=\"22:50\" voice=\"Bill Foster\">\n      Creo que el mayor error fue que no se les puso ningÃºn lÃ­mite. HabÃ­a que hacer la mejor computadora del mundo. âÂ¿Pero para cuÃ¡ndo?â âPues... la verdad no tenemos fecha.â. âÂ¿Y cuÃ¡nto debe\n      costar?â âEhâ¦ tampoco sabemosâ. Y yo le atribuyo el fracaso a Edson. No les puso suficientes lÃ­mites a los programadores ni a los ingenieros.\n    </rh-cue>\n    <rh-cue start=\"23:15\" voice=\"Bill Foster\">\n      Â¿Y sabes quÃ© pasa cuando no les pones lÃ­mites? DiseÃ±an algo tan amplio y complejo que simplemente no se puede concretar.\n    </rh-cue>\n    <rh-cue start=\"23:26\" voice=\"Presentadora\">\n      Pero a ver, vamos a hacer memoria. Tom y su equipo decidieron diseÃ±ar la Eagle a escondidas, y es lo que hicieron durante dos aÃ±os. Y el presidente de la empresa nunca supo lo que estaba\n      pasando. La computadora ahora se llamaba oficialmente Eclipse MV/8000, y cuando ya estaba lista para salir al mercado, el jefe de marketing fue a ver a Ed de Castro para que aprobara la\n      campaÃ±a de publicidad. Vamos a escuchar a Carl Alsing.\n    </rh-cue>\n    <rh-cue start=\"23:53\" voice=\"Carl Alsing\">\n      El jefe de marketing nos dijo: âBueno, pues ya estamos listos para lanzar la Eagle, y vamos a necesitar varios miles de dÃ³lares. Vamos a hacer una conferencia de prensa en seis ciudades del mundo.\n      Y\n      despuÃ©s vamos a hacer una gira para visitar muchas ciudades, vamos a filmar una pelÃ­cula y a mostrarla, y vamos a ser la sensaciÃ³nâ.\n    </rh-cue>\n    <rh-cue start=\"24:14\" voice=\"Carl Alsing\">\n      Pero Ed de Castro contestÃ³: âNo entiendo. Â¿Para quÃ© quieren hacer eso?â Va a ser un peso mÃ¡s para la Eclipse. Es como hacerle cirugÃ­a estÃ©tica: promocionarla por encimita. Pero el gerente de\n      marketing le respondiÃ³: âNo, es una computadora completamente nueva. Es una computadora de 32 bits. Tiene memoria virtual. Es compatible. Va a arrasar con la VAX. Tiene todoâ.\n    </rh-cue>\n    <rh-cue start=\"24:37\" voice=\"Carl Alsing\">\n      Ed de Castro no entendÃ­a nada. Pensaba que nos habÃ­amos equivocado en Carolina del Norte, y que ese era el fin de la empresa, pero en realidad le habÃ­amos salvado el pellejo. Un dÃ­a nos invitÃ³ a\n      todos a almorzar. HabÃ­a sÃ¡ndwiches y gaseosas, y de repente nos dice: âBueno, pues felicidades por el trabajo que hicieron, estoy sorprendido. Yo no sabÃ­a que estaban con ese proyecto, pero vamos\n      a lanzarlo, y tengo entendido que va a haber una pelÃ­cula y varias giras, y ustedes van a participar en eso, asÃ­ que gracias y buen provecho con los sÃ¡ndwichesâ.\n    </rh-cue>\n    <rh-cue start=\"25:19\" voice=\"Presentadora\">\n      La Eagle, que ahora se llamaba MV/8000, apareciÃ³ en la portada de la revista Computer World. El lanzamiento con bombos y platillos en los medios de comunicaciÃ³n les dio cierta fama a aquellos\n      empleados, que hasta entonces se habÃ­an escondido en el sÃ³tano. HabÃ­an salvado a Data General.\n    </rh-cue>\n    <rh-cue start=\"25:38\" voice=\"Presentadora\">\n      Pero todo lo bueno dura poco. Tom West ya no podÃ­a seguir protegiendo al grupo de la polÃ­tica interna de la empresa. Y el equipo no estaba preparado para los resentimientos que surgieron. En la\n      compaÃ±Ã­a habÃ­a gente que envidiaba sus logros y no podÃ­a creer que se hubieran salido con la suya durante tanto tiempo con un proyecto secreto.\n    </rh-cue>\n    <rh-cue start=\"25:57\" voice=\"Presentadora\">\n      Pronto, el nuevo vicepresidente de IngenierÃ­a reemplazÃ³ a Carl Carman, que era el aliado del grupo. El reciÃ©n llegado desarmÃ³ el grupo de Eagle y enviÃ³ a Tom a la oficina de Data General de JapÃ³n\n      antes de que se vendiera la primera MV/8000.\n    </rh-cue>\n    <rh-cue start=\"26:13\" voice=\"Jim Guyer\">\n      Yo creÃ­a que habÃ­amos hecho la mejor superminicomputadora de 32 bits que el dinero podÃ­a comprar, lo cual era excelente para Data General, y que durante un tiempo destronarÃ­amos a Digital\n      Equipment Corporation, no que ya habÃ­amos acabado con ellos. La competencia era salvaje en esos tiempos, y no es fÃ¡cil tener Ã©xito en el sector de la alta tecnologÃ­a, pero yo pensaba que lo que\n      habÃ­amos hecho valÃ­a la pena.\n    </rh-cue>\n    <rh-cue start=\"26:42\" voice=\"Presentadora\">\n      Sin duda, el lanzamiento de la Eagle salvÃ³ a Data General, pero habÃ­an perdido participaciÃ³n en el mercado frente a DEC durante tres aÃ±os, asÃ­ que la empresa nunca se recuperÃ³ realmente y la\n      industria habÃ­a seguido avanzando. Las minicomputadoras ya no eran lo mÃ¡s importante. La carrera de las microcomputadoras ya habÃ­a comenzado, y le abriÃ³ camino a la revoluciÃ³n de las computadoras\n      personales.\n    </rh-cue>\n    <rh-cue start=\"27:04\" voice=\"Carl Alsing\">\n      Data General siguiÃ³ adelante, sacÃ³ nuevas versiones, las mejorÃ³ en los siguientes modelos y las vendiÃ³ durante un tiempo, asÃ­ que disfrutÃ³ de cierto Ã©xito. Pero, bueno, las cosas cambian. El\n      mercado cambiÃ³ y... ellos se convirtieron en una empresa de software, y finalmente otra empresa los comprÃ³. Y ahora creo que lo Ãºnico que queda de ellos es algÃºn archivador en alguna empresa de\n      Hopkinton, Massachusetts.\n    </rh-cue>\n    <rh-cue start=\"27:36\" voice=\"Presentadora\">\n      Un aÃ±o despuÃ©s, muchos de los integrantes del grupo de la Eagle habÃ­an dejado Data General. Algunos estaban agotados. Otros ya querÃ­an diseÃ±ar alguna otra cosa. Otros se fueron al oeste, hacia\n      Silicon Valley, y estaban ansiosos por encontrar la siguiente chispa creativa. Cualquiera que fuera el caso, no tenÃ­a mucho sentido quedarse en una empresa que no reconocÃ­a todo lo que habÃ­an\n      hecho para salvarla. En ese mismo aÃ±o, en 1981, se publicÃ³ El alma de una nueva mÃ¡quina, de Tracy Kidder. Ahora el mundo sabrÃ­a cÃ³mo se habÃ­a diseÃ±ado la Eagle.\n    </rh-cue>\n    <rh-cue start=\"28:14\" voice=\"Carl Alsing\">\n      Si me preguntas quÃ© constituye el alma de una nueva mÃ¡quina, yo dirÃ­a que las personas y lo que les pasa a esas personas; los sacrificios que hacen, el esfuerzo y el entusiasmo que sienten, y las\n      satisfacciones que esperan obtener. Tal vez lo logren, tal vez no, pero tienen un objetivo y luchan por Ã©l.\n    </rh-cue>\n    <rh-cue start=\"28:35\" voice=\"Jim Guyer\">\n      En realidad, la computadora era un personaje secundario. El corazÃ³n del proyecto era la gente.\n    </rh-cue>\n    <rh-cue start=\"28:47\" voice=\"Presentadora\">\n      En el prÃ³ximo episodio de nuestra nueva temporada sobre el hardware, vamos a retroceder en el tiempo hasta la era de las computadoras mainframe, y te contaremos la historia de otro grupo de\n      empleados rebeldes. La computadora que construyeron hizo surgir un lenguaje de programaciÃ³n que cambiÃ³ el mundo.\n    </rh-cue>\n    <rh-cue start=\"29:04\" voice=\"Presentadora\">\n      Command Line Heroes en espaÃ±ol es un podcast original de Red Hat. Para esta temporada, recopilamos excelentes materiales de investigaciÃ³n para que puedas saber mÃ¡s sobre la historia del hardware\n      del que estamos hablando. Si quieres saber mÃ¡s sobre la Eagle y el equipo que la diseÃ±Ã³, visita redhat.com/commandlineheroes. Hasta la prÃ³xima, sigan programando.\n    </rh-cue>\n  </rh-transcript>\n</rh-audio-player>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\" src=\"customization.js\"></script>\n\n<!--playground-fold--><link rel=\"stylesheet\" href=\"../rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Language Localization"
    },
    "demo/language-localization/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "demo/language-localization/customization.js": {
      "content": "import '@rhds/elements/rh-audio-player/rh-audio-player.js';\nconst form = document.querySelector('form');\nconst player = document.querySelector('rh-audio-player');\nconst { poster } = player;\n\n/**\n * update audio player demo based on form selections\n */\nfunction updateDemo() {\n  const colorPalette = ['cyan', 'light'].includes(form.palette.value) ?\n    'light' : 'dark';\n  const colorClass =\n    ['cyan', 'purple', 'purple img'].includes(form.palette.value) ? form.palette.value : '';\n  player.poster = !form.poster.checked || form.palette.value === 'purple img' ? undefined : poster;\n  player.layout = form.layout.value !== '' ? form.layout.value : undefined;\n  if (colorPalette === player.colorPalette) {\n    const oldOn = player.colorPalette;\n    player.colorPalette = oldOn === 'dark' ? 'light' : 'dark';\n  }\n  player.setAttribute('class', colorClass);\n  player.colorPalette = colorPalette;\n  player.hasAccentColor = ['cyan', 'purple', 'purple img'].includes(form.palette.value);\n  player.requestUpdate();\n}\n\nif (form) {\n  form.addEventListener('input', updateDemo);\n  updateDemo();\n}\n",
      "hidden": true
    },
    "demo/prevent-concurrent-playback/index.html": {
      "contentType": "text/html",
      "selected": false,
      "content": "<h2>rh-audio-player: Prevent Concurrent Playback</h2>\n\n<p>Pressing play on any <code>rh-audio-player</code> element will pause\n  any other currently playing <code>rh-audio-player</code> elements.</p>\n\n<rh-audio-player>\n  <p slot=\"series\">Code Comments</p>\n  <h3 slot=\"title\">Bringing Deep Learning to Enterprise Applications</h3>\n  <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n    <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3\">\n  </audio>\n</rh-audio-player>\n\n<rh-audio-player>\n  <p slot=\"series\">Code Comments</p>\n  <h3 slot=\"title\">Rethinking Networks In Telecommunications</h3>\n  <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n    <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/32d79061-21f8-40a1-9ef8-d424e82a8326/audio/0a65ec45-a21a-4c31-94a6-61701a145b1d/default_tc.mp3\">\n  </audio>\n</rh-audio-player>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\">\n  import '@rhds/elements/rh-audio-player/rh-audio-player.js';\n</script>\n<!--playground-fold--><link rel=\"stylesheet\" href=\"../rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Prevent Concurrent Playback"
    },
    "demo/prevent-concurrent-playback/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "demo/right-to-left/index.html": {
      "contentType": "text/html",
      "selected": false,
      "content": "<h2>rh-audio-player: Right-to-left</h2>\n<!-- Options for demo -->\n<form>\n  <p>Options:</p>\n  <ul>\n    <li><label>Poster: <input name=\"poster\" type=\"checkbox\" checked=\"\"></label></li>\n    <li><label>Color Palette:\n        <select name=\"palette\">\n          <option value=\"light\" selected=\"\">Light</option>\n          <option value=\"dark\">Dark</option>\n          <option value=\"purple\">Purple</option>\n          <option value=\"purple-img\">Purple with Image</option>\n          <option value=\"cyan\">Cyan </option>\n        </select>\n      </label></li>\n    <li>\n      <label>Layout:\n        <select name=\"layout\">\n          <option value=\"full\" selected=\"\">Full</option>\n          <option value=\"compact-wide\">Compact Wide</option>\n          <option value=\"compact\">Compact</option>\n          <option value=\"\">Mini</option>\n        </select>\n      </label>\n    </li>\n  </ul>\n</form>\n\n<!-- Right to left layout will display based on form checkbox -->\n<div dir=\"rtl\">\n  <rh-audio-player layout=\"full\" poster=\"https://www.redhat.com/cms/managed-files/CLH-S7-ep1.png\">\n    <p slot=\"series\">Code Comments</p>\n    <h3 slot=\"title\">Bringing Deep Learning to Enterprise Applications</h3>\n    <rh-audio-player-about slot=\"about\">\n      <h4 slot=\"heading\">About the episode</h4>\n      <p>\n        There are a lot of publicly available data sets out there. But when it\n        comes to specific enterprise use cases, you're not necessarily going to\n        able to find one to train your models. To realize the power of AI/ML in\n        enterprise environments, end users need an inference engine to run on\n        their hardware. Ryan Loney takes us through OpenVINO and Anomalib, open\n        toolkits from Intel that do precisely that. He looks specifically at\n        anomaly detection in use cases as varied as medical imaging and\n        manufacturing.\n      </p>\n      <p>\n        Want to learn more about Anomalib? Check out the research paper that\n        introduces the deep learning library.\n      </p>\n      <rh-avatar slot=\"profile\" src=\"https://www.redhat.com/cms/managed-files/ryan-loney.png\">\n        Ryan Loney\n        <span slot=\"subtitle\">Product manager, OpenVINO Developer Tools, <em>IntelÂ®</em></span>\n      </rh-avatar>\n    </rh-audio-player-about>\n    <audio crossorigin=\"anonymous\" slot=\"media\" controls=\"\">\n      <source type=\"audio/mp3\" srclang=\"en\" src=\"https://cdn.simplecast.com/audio/28d037d3-7d17-42d4-a8e2-2e00fd8b602b/episodes/bd38190e-516f-49c0-b47e-6cf663d80986/audio/dc570fd1-7a5e-41e2-b9a4-96deb346c20f/default_tc.mp3\">\n    </audio>\n    <rh-audio-player-subscribe slot=\"subscribe\">\n      <h4 slot=\"heading\">Subscribe</h4>\n      <p>Subscribe here:</p>\n      <a slot=\"link\" href=\"https://podcasts.apple.com/us/podcast/code-comments/id1649848507\" target=\"_blank\" title=\"Listen on Apple Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Apple Podcasts\" data-analytics-category=\"Hero|Listen on Apple Podcasts\">\n        <img src=\"https://www.redhat.com/cms/managed-files/badge_apple-podcast-white.svg\" alt=\"Listen on Apple Podcasts\">\n      </a>\n      <a slot=\"link\" href=\"https://open.spotify.com/show/6eJc62sKckHs4uEQ8eoKzD\" target=\"_blank\" title=\"Listen on Spotify\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Spotify\" data-analytics-category=\"Hero|Listen on Spotify\">\n        <img src=\"https://www.redhat.com/cms/managed-files/badge_spotify.svg\" alt=\"Listen on Spotify\">\n      </a>\n      <a slot=\"link\" href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5wYWNpZmljLWNvbnRlbnQuY29tL2NvZGVjb21tZW50cw\" target=\"_blank\" title=\"Listen on Google Podcasts\" data-analytics-linktype=\"cta\" data-analytics-text=\"Listen on Google Podcasts\" data-analytics-category=\"Hero|Listen on Google Podcasts\">\n        <img src=\"https://www.redhat.com/cms/managed-files/badge_google-podcast.svg\" alt=\"Listen on Google Podcasts\">\n      </a>\n      <a slot=\"link\" href=\"https://feeds.pacific-content.com/codecomments\" target=\"_blank\" title=\"Subscribe via RSS Feed\" data-analytics-linktype=\"cta\" data-analytics-text=\"Subscribe via RSS Feed\" data-analytics-category=\"Hero|Subscribe via RSS Feed\">\n        <img class=\"img-fluid\" src=\"https://www.redhat.com/cms/managed-files/badge_RSS-feed.svg\" alt=\"Subscribe via RSS Feed\">\n      </a>\n    </rh-audio-player-subscribe>\n    <rh-transcript id=\"regular\" slot=\"transcript\">\n      <h4 slot=\"heading\">Transcript</h4>\n      <rh-cue start=\"00:02\" voice=\"Burr Sutter\">\n        Hi, I'm Burr Sutter. I'm a Red Hatter who spends a lot of time talking to technologists about technologies. We say this a lot at Red Hat. No single technology provider holds the key to\n        success, including us. And I would say the same thing about myself. I love to share ideas, so I thought it would be awesome to talk to some brilliant technologists at Red Hat Partners. This\n        is Code Comments, an original podcast from Red Hat.\n      </rh-cue>\n      <rh-cue start=\"00:29\" voice=\"Burr Sutter\">\n        I'm sure, like many of you here, you have been thinking about AI/ML, artificial intelligence and machine learning. I've been thinking about that for quite some time and I actually had the\n        opportunity to work on a few successful projects, here at Red Hat, using those technologies, actually enabling a data set, gathering a data set, working with a data scientist and data\n        engineering team, and then training a model and putting that model into production runtime environment. It was an exciting set of projects and you can see those on numerous YouTube videos\n        that have published out there before. But I want you to think about the problem space a little bit, because there are some interesting challenges about a AI/ML. One is simply just getting\n        access to the data, and while there are numerous publicly available data sets, when it comes to your specific enterprise use case, you might not be to find publicly available data.\n      </rh-cue>\n      <rh-cue start=\"01:14\" voice=\"Burr Sutter\">\n        In many cases you cannot, even for our applications that we created, we had to create our data set, capture our data set, explore the data set, and of course, train a model accordingly. And\n        we also found there's another challenge to be overcome in this a AI/ML world, and that is access to certain types of hardware. If you think about an enterprise environment and the creation\n        of an enterprise application specifically for a AI/ML, end users need an inference engine to run on their hardware. Hardware that's available to them, to be effective for their application.\n        Let's say an application like Computer Vision, one that can detect anomalies and medical imaging or maybe on a factory floor. As those things are whizzing by on the factory line there,\n        looking at them and trying to determine if there is an error or not.\n      </rh-cue>\n      <rh-cue start=\"01:56\" voice=\"Burr Sutter\">\n        Well, how do you actually make it run on your hardware, your accessible technology that you have today? Well, there's a solution for this as an open toolkit called OpenVINO. And you might\n        be thinking, \"Hey, wait a minute, don't you need a GPU for AI inferencing, a GPU for artificial intelligence, machine learning? Well, not according to Ryan Loney, product manager of OpenVINO\n        Developer Tools at Intel.\n      </rh-cue>\n      <rh-cue start=\"02:20\" voice=\"Ryan Loney\">\n        I guess I'll start with trying to maybe dispel a myth. I think that CPUs are widely used for inference today. So if we look at the data center segment, about 70% of the AI inference is\n        happening on Intel Xeon, on our data center CPUs. And so you don't need a GPU especially for running inference. And that's part of the value of OpenVINO, is that we're taking models that may\n        have been trained on a GPU using deep learning frameworks like PyTorch or TensorFlow, and then optimizing them to run on Intel hardware.\n      </rh-cue>\n      <rh-cue start=\"02:57\" voice=\"Burr Sutter\">\n        Ryan joined me to discuss AI/ML in the enterprise across various industries and exploring numerous use cases. Let's talk a little bit about the origin story behind OpenVINO. Tell us more\n        about it and how it came to be and why it came out of Intel.\n      </rh-cue>\n      <rh-cue start=\"03:12\" voice=\"Ryan Loney\">\n        Definitely. We had the first release of OpenVINO, was back in 2018, so still relatively new. And at that time, we were focused on Computer Vision and pretty tightly coupled with OpenCV,\n        which is another open source library with origins at Intel. It had its first release back in 1999, so it's been around a little bit longer. And many of the software engineers and architects\n        at Intel that were involved with and contributing to OpenCV are working on OpenVINO. So you can think of OpenVINO as complimentary software to OpenCV and we're providing an engine for\n        executing inferences as part of a Computer Vision pipeline, or at least that's how we started.\n      </rh-cue>\n      <rh-cue start=\"03:58\" voice=\"Ryan Loney\">\n        But since 2018, we've started to move beyond just Computer Vision inference. So when I say Computer Vision inference, I mean image classification, object detection, segmentation, and now\n        we're moving into natural language processing. Things like speech synthesis, speech recognition, knowledge graphs, time series forecasting and other use cases that don't involve Computer\n        Vision and don't involve inference on pixels. Our latest release, the 2022.1 that came out earlier this year, that was the most significant update that we've had to OpenVINO, since we\n        started in 2018. And the major focus of that release was optimizing for use cases that go beyond Computer Vision.\n      </rh-cue>\n      <rh-cue start=\"04:41\" voice=\"Burr Sutter\">\n        And I like that concept that you just mentioned right there, Computer Vision, and you said that you extended those use cases and went beyond that. Could you give us some more concrete\n        examples of Computer Vision?\n      </rh-cue>\n      <rh-cue start=\"04:50\" voice=\"Ryan Loney\">\n        Sure. When you think about manufacturing, quality control in factories, everything from arc welding, defect detection to inspecting BMW cars on assembly lines, they're using cameras or\n        sensors to collect data and usually it's cameras collecting images like RGB images that you and I can see and looks like something taken from a camera or video camera. But also, things like\n        infrared or computerized tomography scans used in healthcare, X-ray, different types of images where we can draw bounding boxes around regions of interest and say, \"This is a defect,\" or,\n        \"This is not a defect.\" And also, \"Is this worker wearing a safety hat or did they forget to put it on?\" And so, you can take this and integrate it into a pipeline where you're triggering an\n        alert if somebody forgets to wear their safety mask, or if there's a defect in a product on an assembly line, you can just use cameras and OpenVINO and OpenCV running these on Intel hardware\n        and help to analyze.\n      </rh-cue>\n      <rh-cue start=\"05:58\" voice=\"Ryan Loney\">\n        And that's what a lot of the partners that we work with are doing, so these independent software vendors. And there's other use cases for things like retail. You think about going to a\n        store and using an automated checkout system. Sometimes people use those automated checkouts and they slide a few extra items into their bag that they don't scan and it's a huge loss for the\n        retail outlets that are providing this way to check out realtime shelf monitoring. We have a Vispera, one of our ISVs that helps keep store shelves stocked by just analyzing the cameras in\n        the stores, detecting when objects are missing from the shelves so that they can be restocked. We have Vistry, another ISV that works with quick service restaurants. When you think about\n        automating the process of, when do I drop the fries into the fryer so that they're warm when the car gets to the drive through window, there's quite a bit of industrial healthcare retail\n        examples that we can walk through.\n      </rh-cue>\n      <rh-cue start=\"06:55\" voice=\"Burr Sutter\">\n        And we should dig into some more of those, but I got to tell you, I have a personal experience in this category that I want to share with and you can tell me how silly you might think at\n        this point in time it is. We actually built a keynote demonstration for the Red Hat big stage back in 2015. And I really want to illustrate the concept of asset tracking. So we actually gave\n        everybody in the conference a little Bluetooth token with a little battery, a little watch battery, and a little Bluetooth emitter. And we basically tracked those things around the\n        conference. We basically put a raspberry pi in each of the meeting rooms and up in the lunch room and you could see how the tokens moved from room to room to room.\n      </rh-cue>\n      <rh-cue start=\"07:28\" voice=\"Burr Sutter\">\n        It was a relatively simple application, but it occurred to me, after we figured out how to do that with Bluetooth and triangulating Bluetooth signals by looking at relative signal strength\n        from one radio to another and putting that through an Apache Spark application at the time, we then realized, \"You know what? This is easier done with cameras.\" And just simply looking at a\n        camera and having some form of a AI/ML model, a machine learning model, that would say, \"There are people here now,\" or, \"There are no people here now.\" What do you think about that?\n      </rh-cue>\n      <rh-cue start=\"07:56\" voice=\"Ryan Loney\">\n        What you just described is exactly the product that Pathr, one of our partners is offering, but they're doing it with Computer Vision and cameras. So when Pathr tries to help retail stores\n        analyze the foot traffic and understand, with heat maps, where are people spending the most time in stores, how many people are coming in, what size groups are coming into the store and\n        trying to help understand if there was a successful transaction from the people who entered the store and left the store, to help with the retail analytics and marketing sales and\n        positioning of products. And so, they're doing that in a way that also protects privacy. And that's something that's really important. So when you talked about those Bluetooth beacons,\n        probably if everyone who walked into a grocery store was asked to put a tracking device in their cart or on their person and say, \"You're going to be tracked around the store,\" they probably\n        wouldn't want to do that.\n      </rh-cue>\n      <rh-cue start=\"08:53\" voice=\"Ryan Loney\">\n        The way that you can do this with cameras, is you can detect people as they enter and remove their face. So you can ignore any biometric information and just track the person based on\n        pixels that are present in the detected region of interest. So they're able to analyze... Say a family walks in the door and they can group those people together with object detection and\n        then they can track their movement throughout the store without keeping track of their face, or any biometric, or any personal identifiable information, to avoid things like bias and to make\n        sure that they're protecting the privacy of the shoppers in the store, while still getting that really useful marketing analytics data. So that they can make better decisions about where to\n        place their products. That's one really good example of how Computer Vision, AI with OpenVINO is being used today.\n      </rh-cue>\n      <rh-cue start=\"09:49\" voice=\"Burr Sutter\">\n        And that is a great example, because you're definitely spot on. It is invasive when you hand someone a Bluetooth device and say, \"Please, keep this with you as you go throughout our store,\n        our mall or throughout our hospital, wherever you might be.\" Now you mentioned another example earlier in the conversation which was related to worker safety. \"Are they wearing a helmet?\" I\n        want to talk more about that concept in a real industrial setting, a manufacturing setting, where there might be a factory floor and there's certain requirements. Or better yet there's like\n        a quality assurance requirement, let's say, when it comes to looking at a factory line. I've run that use case often with some of our customers. Can you talk more about those kinds of use\n        cases?\n      </rh-cue>\n      <rh-cue start=\"10:23\" voice=\"Ryan Loney\">\n        One of our partners, Robotron, we published a case study, I think last year, where they were working with BMW at one of their factories. And they do quality control inspection, but they're\n        also doing things related to worker safety and analyzing. I use the safety hat example. There's a number of our ISVs and partners who have similar use cases and it comes down to, there's a\n        few reasons that are motivating this and some are related to insurance. It's important to make sure that if you want to have your factory insured, that your workers are protecting themselves\n        and wearing the gear regulatory compliance, you're being asked to properly protect from exposure to chemicals or potentially having something fall and hit someone on the head. So wearing a\n        safety vest, wearing goggles, wearing a helmet, these are things that you need to do inside the factory and you can really easily automate and detect and sometimes without bias.\n      </rh-cue>\n      <rh-cue start=\"11:21\" voice=\"Ryan Loney\">\n        I think that's one of the interesting things about the Robotron-BMW example is that they were also blurring, blacking out, so drawing a box to cover the face of the workers in the factory,\n        so that somebody who was analyzing the video footage and getting the alerts saying that, \"Bay 21 has a worker without a hat on,\" that it's not sending their face and in the alert and\n        potentially invading or going against privacy laws or just the ethics of the company. They don't want to introduce bias or have people targeted because it's much better to blur the face and\n        alert and have somebody take care of it on the floor. And then, if you ever need to audit that information later, they have a way to do it where people who need to be able to see who the\n        employee was and look up their personal information, they can do that.\n      </rh-cue>\n      <rh-cue start=\"12:17\" voice=\"Ryan Loney\">\n        But then just for the purposes of maintaining safety, they don't need to have access to that personal information, or biometric information. Because that's one thing that when you hear\n        about Computer Vision or person tracking, object detection, there's a lot of concern, and rightfully so, about privacy being invaded and about tracking information, face re-identification,\n        identifying people who may have committed crimes through video footage. And that's just not something that a lot of companies want to... They want to protect privacy and they don't want to\n        be in a situation where they might be violating someone's rights.\n      </rh-cue>\n      <rh-cue start=\"12:56\" voice=\"Burr Sutter\">\n        Well, privacy is certainly opening up Pandora's box. There's a lot to be explored in that area, especially in a digital world that we now live in. But for now, let's move on and explore a\n        different area. I'm interested in how machines and computers offer advantages specifically in certain use cases like a quality control scenario. I asked Ryan to explain how a AI/ML and\n        specifically machines, computers, could augment that capability.\n      </rh-cue>\n      <rh-cue start=\"13:20\" voice=\"Ryan Loney\">\n        I can give a specific example where we have a partner that's doing defect detection, looking for anomalies in batteries. I'm sure you've heard there's a lot of interest right now in\n        electric vehicles, a lot of batteries being produced. And so, if you go into one of these factories, they have images that they collect of every battery that's going through this assembly\n        line. And through these images, people can look and see and visually inspect what their eyes and say, \"This battery has a defect, send it back.\" And that's one step in the quality control\n        process, there's other steps I'm sure, like running diagnostic tests and measuring voltage and doing other types of non-visual inspection. But for the visual inspection piece, where you can\n        really easily identify some problems, it's much more efficient to introduce Computer Vision. And so, that's where we have this new library that we've introduced, called Anomalib.\n      </rh-cue>\n      <rh-cue start=\"14:17\" voice=\"Ryan Loney\">\n        So OpenVINO, while we're focused on inference, we're also thinking about the pipeline, or the funnel, that gets these models to OpenVINO. And so, we've invested in this anomaly\n        segmentation, anomaly detection library that we've recently open sourced and there's a great research paper about it, about Anomalib, but the idea is you can take just a few images and train\n        a model and start detecting these defects. And so, for this battery example, that's a more advanced example, but to make it simpler, take some bolts and... Take 10 bolts. You have one that\n        has a scratch on it, or one that is chipped, or has some damage to it, and you can easily get started in training to recognize the bolts that do not have an anomaly and the ones that do,\n        which is a small data set. And I think that's really one of the most important things today.\n      </rh-cue>\n      <rh-cue start=\"15:11\" voice=\"Ryan Loney\">\n        Challenges, one is access to data, but the other is needing a massive amount of data to do something meaningful. And so we're starting to try to change that dynamic with Anomalib. You may\n        not need a 100,000 images, you may need 100 images and you can start detecting anomalies in everything from batteries to bolts to, maybe even the wood varnish use case that you mentioned.\n\n      </rh-cue>\n      <rh-cue start=\"15:37\" voice=\"Burr Sutter\">\n        That is a very key point because often in that data scientist process, that data engineering data scientist process, the one key thing is, can you gather the data that you need for the\n        input for the model training? And we've often said, at least people I've worked with over the last couple years, \"You need a lot of data, you need tens of thousands of correct images, so we\n        can sort out the difference between dogs versus cats,\" let's say. Or you need dozens and dozens of situations where if it's a natural language processing scenario, a good customer\n        interaction, a good customer conversation. And this case it sounds like what you're saying is, \"Show us just the bad things, fewer images, fewer incorrect things, and then let us look for\n        those kind of anomalies.\" Can you tell us more about that? Because that is very interesting. The concept that I can use a much smaller data set as my input, as opposed to gathering terabytes\n        of data in some cases, to just simply get my model training underway.\n      </rh-cue>\n      <rh-cue start=\"16:30\" voice=\"Ryan Loney\">\n        Like you described, the idea is, if you have some good images and then you have some of the known defects, and you can just label, \"Here's a set of good images and here's a few of the\n        defects.\" And you can right away start detecting those specific defects that you've identified. And then, also be able to determine when it doesn't match the expected appearance of a non\n        defective item. So if I have the undamaged screw and then I introduce one with some new anomaly that's never been seen before, I can say this one is not a valid screw. And so, that's the\n        approach that we're taking and it's really important because so often you need to have subject matter experts. Take the battery example, there's these workers who are on the floor, in a\n        factory and they're the ones who know best when they look at these images, which one's going to have an issue, which one's defective.\n      </rh-cue>\n      <rh-cue start=\"17:31\" voice=\"Ryan Loney\">\n        And then they also need to take that subject matter expertise and then use it to annotate data sets. And when you have these tens of thousands of images you need to annotate, it's asking\n        those people to stop working on the factory floor so they can come annotate some images. That's a tough business call to make, right? But if you only need them to annotate a handful of\n        images, it's a much easier ask to get the ball rolling and demonstrate value. And maybe over time you will want to annotate more and more images because you'll get even better accuracy in\n        the model. Even better, even if it's just small incremental improvements, that's something that if it generates value for the business, it's something the business will invest in over time.\n        But you have to convince the decision makers that it's worth the time of these subject matter experts to stop what they're doing and go and label some images of the things that they're\n        working on in the factory.\n      </rh-cue>\n      <rh-cue start=\"18:27\" voice=\"Burr Sutter\">\n        And that labeling process can be very labor intensive. If the annotation is basically saying what is correct, what's wrong, what is this, what is that. And therefore if we can minimize that\n        timeframe to get the value quicker, then there's something that's useful for the business, useful for the organization, long before we necessarily go through a whole huge model training\n        phase.\n      </rh-cue>\n      <rh-cue start=\"18:49\" voice=\"Burr Sutter\">\n        So we talked about labeling and how that is labor intensive activity, but I love the idea of helping the human. And helping the human most specifically not get bored. Basically if the human\n        is eyeballing a bunch of widgets flying by, over time they make mistakes, they get bored and they don't pay as close attention as they should. That's why the constant of AI/ML, and\n        specifically Computer Vision augmenting that capability and really helping the human identify anomalies faster, more quickly, maybe with greater accuracy, could be a big win. We focused on\n        manufacturing, but let's actually go into healthcare and learn how these tools can be used in that sector and that industry. Ryan talked me about how OpenVINO's run time can be incorporated\n        into medical imaging equipment with Intel processors embedded in CT, MRI and ultrasound machines. While these inferences, this AI/ML workload, can be operating and executing right there in\n        the same physical room as the patient.\n      </rh-cue>\n      <rh-cue start=\"19:44\" voice=\"Ryan Loney\">\n        We did a presentation with GE last year, I think they said there's at least 80 countries that have their x-ray machines deployed. And they're doing things like helping doctors place\n        breathing tubes in patients. So during COVID, during the pandemic, that was a really important tool to help with nurses and doctors who were intubating patients, sometimes in a parking lot\n        or a hallway of a hospital. And when they had a statistic that GE said, I think one out of four breathing tubes gets placed incorrectly when you're doing it outside the operating room.\n        Because when you're in an operating room it's much more controlled and there's someone who's an expert at placing the tubes, it's something you have more of a controlled environment. But\n        when you're out, in a parking lot, in a tent, when the hospital's completely full and you're triaging patients with COVID, that's when they're more likely to make mistakes.And so, they had\n        this endotracheal tube placement, ETT, model that they trained and it helped to use an x-ray and give an alert and say, \"This tube is placed wrong, pull it out and do it again.\" And so,\n        things like that help doctors so that they can avoid mistakes. And having a breathing tube placed incorrectly can cause collapsed lung and a number of other unwanted side effects. So it's\n        really important to do it correctly. Another example is Samsung Medison. They actually are estimating fetal angle of progression. So this is analyzing ultrasound of pregnant women being able\n        to help take measurements that are usually hard to calculate, but it can be done in an automated way. They're already taking an ultrasound scan and now they're executing this model that can\n        take some of these measurements to help the doctor avoid potentially more intrusive alternative methods. So the patient wins, it makes their life better and the doctor is getting help from\n        this AI model. And those are just a few examples.\n      </rh-cue>\n      <rh-cue start=\"21:42\" voice=\"Burr Sutter\">\n        Those are some amazing examples when it comes to all these things, we're talking CT scans and x-rays, other examples of Computer Vision. One thing that's kind of interesting in this space,\n        I think, whenever I get a chance to work on, let's say an object detection model, and one of our workshops, by the way, is actually putting that out in front of people to say, \"Look, you can\n        use your phone and it basically sends the image over to our OpenShift with our data science platform and then analyzes what you see.\" And even in my case, where I take a picture of my dog as\n        an example, it can't really decide, is it a dog or a cat? I have a very funny looking dog.\n      </rh-cue>\n      <rh-cue start=\"22:15\" voice=\"Burr Sutter\">\n        And so there's always a percentage outcome. In other words, \"I think it's a dog, 52%.\" So I want to talk about that more. How important is it to get to that a hundred percent accuracy? How\n        important is it to really, depending on the use case, to allow for the gray area if you will, where it's an 80% accuracy or a 70% accuracy, and what are the trade offs there associated with\n        the application? Can you discuss that more?\n      </rh-cue>\n      <rh-cue start=\"22:38\" voice=\"Ryan Loney\">\n        Accuracy is definitely a touchy subject, because how you measure it makes a huge difference. I think what you were describing with the dog example, there's sort of a top five potential\n        classes that might maybe be identified. So let's say you're doing object detection and you detect a region of interest, and it says 65% confidence this is a dog. Well, the next potential\n        label that could be maybe 50% confidence or 20% confidence might be something similar to a dog. Or in the case of models that have been trained on the ImageNet dataset or on COCO dataset,\n        they have actual breeds of dogs. If I want to look at the top five labels for a dog, for my dog for example, she's a mix, mostly a Labrador retriever, but I may look at the top five labels\n        and it may say 65% confidence that she's a flat coated retriever.\n      </rh-cue>\n      <rh-cue start=\"23:32\" voice=\"Ryan Loney\">\n        And then confidence that she's a husky as 20%, and then 5% confidence that she's a greyhound or something. Those labels, all of them are dogs. So if I'm just trying to figure out, is this a\n        dog? I could probably find all of the classes within the data set and say, \"Well, these all, class ID 65, 132, 92 and 158, all belong to a group of dogs.\" So if I want to just write an\n        application to tell me if this is a dog or not, I would probably use that to determine if it's a dog. But how you measure that as accuracy, well that's where it gets a little bit\n        complicated. Because if you're being really strict about the definition and you're trying to validate against the data set of labeled images, and I have specific dog breeds or some specific\n        detail and it doesn't match, well then, the accuracy's going to go down.\n      </rh-cue>\n      <rh-cue start=\"24:25\" voice=\"Ryan Loney\">\n        And that's especially important when we talk about things like compression and quantization, which historically, has been difficult to get adoption in some domains, like healthcare, where\n        even the hint of accuracy going down implies that we're not going to be able to help. In some small case, maybe if it's even half a percent of the time, we won't detect that that tube is\n        placed incorrectly or that that patient's lung has collapsed or something like that. And that's something that really prevents adoption of some of these methods that can really boost\n        performance, like quantization. But if you take that example of... Different from the dog example, and you think about segmentation of kidneys. If I'm doing kidney segmentation, which is\n        taking a CT scan and then trying to pick the pixels out of that scan that belong to a kidney, how I measure accuracy may be how many of those pixels I'm able to detect and how many did I\n        miss?\n      </rh-cue>\n      <rh-cue start=\"25:25\" voice=\"Ryan Loney\">\n        Missing some of the pixels is maybe not a problem, depending on how you've built the application, because you still detect the kidney, and maybe you just need to apply padding around the\n        region of interest, so that you don't miss any of the actual kidney when you compress the model and when you quantize the model. But that requires a data scientist, an ML engineer, somebody\n        to really, they have to be able to go and apply that after the fact, after the inference happens, to make sure that you're not losing critical information. Because the next step from\n        detecting the kidney, may be detecting a tumor.\n      </rh-cue>\n      <rh-cue start=\"26:04\" voice=\"Ryan Loney\">\n        And so, maybe you can use the more optimized model to detect the kidney, but then you can use a slower model to detect the tumor. But that also requires somebody to architect and make that\n        decision or that trade off and say, \"Well, I need to add padding,\" or, \"I should only use the quantized model to detect the region of interest for the kidney.\" And then, use the model that\n        takes longer to do the inference just to find the tumor, which is going to be on a smaller size. The dimensions are going to be much smaller once we crop to the region of interest. But all\n        of those details, that's maybe not easy to explain in a few sentences and even the way I explained it is probably really confusing.\n      </rh-cue>\n      <rh-cue start=\"26:45\" voice=\"Burr Sutter\">\n        I do love that use case, like you mentioned, the cropping, even in one scenario that we worked on for another project, we specifically decided to pixelate the image that we had taken,\n        because we knew that we could get the outcome we wanted by even just using a smaller or having less resolution in our image. And therefore, as we transferred it from the mobile device, the\n        edge device, up into the cloud, we wanted that smaller image just for transfer purposes. And still, we could get the accuracy we needed by a lot of testing.\n      </rh-cue>\n      <rh-cue start=\"27:11\" voice=\"Burr Sutter\">\n        And one thing that's interesting about that, from my perspective, is, if you're doing image processing, sometimes it takes a while for this transaction to occur. I come from a traditional\n        application background, where I'm reading and writing things from a database, or a message broker, or moving data from one place to another. Those things happen sub-second normally, even\n        with great latency between your data centers, it's still sub-second in most cases. While a transaction like this one can actually take two seconds or four seconds, as it's doing its analysis\n        and actually coming back with its, \"I think it's a dog, I think it's a kidney, I think it's whatever.\" And providing me that accuracy statement. That concept of optimization is very\n        important in the overall application architecture. Would you agree with that or how do you think about that concept?\n      </rh-cue>\n      <rh-cue start=\"27:56\" voice=\"Ryan Loney\">\n        Definitely. It depends too on the use case. So if you think about how important it is to reduce the latency and increase the number of frames per second that you can process when you're\n        talking about a loss prevention model that's running at a grocery store. You want to keep the lines moving, you don't want every person who's at the self checkout to have to wait five\n        seconds for every item they scan. You need it to happen as quickly as possible. And if sometimes the accuracy decreases slightly, or I'd say the accuracy of the whole pipeline, so not just\n        looking at the individual model or the individual inference, but let's say that the whole pipeline is not as successful at detecting when somebody steals one item from the self checkout,\n        it's not going to be a life threatening situation. Whereas being hooked up to the x-ray machine with the tube placement model, they might be willing to have the doctor or the nurse wait five\n        seconds to get the result.\n      </rh-cue>\n      <rh-cue start=\"28:55\" voice=\"Ryan Loney\">\n        They don't need it to happen in 500 milliseconds. Their threshold for waiting is a little bit higher. That, I think, also drives some of the decision. You want to keep people moving through\n        the checkout line and you can afford to, potentially, if you lose a little bit of accuracy here and there, it's not going to cost the company that much money or it's not going to be life\n        threatening. It's going to be worth the trade off of keeping the line moving and not having people leave the store and not check out at all, to say, \"I'm not going to shop today because the\n        line's too long.\"\n      </rh-cue>\n      <rh-cue start=\"29:30\" voice=\"Burr Sutter\">\n        There are so many trade-offs in enterprise AI/ML use cases, things like latency, accuracy and availability, and certainly complexities abound, especially in an obviously ever-evolving\n        technological landscape where we are still very early in the adoption of AI/ML. And to navigate that complexity, that direct feedback from real world end users is essential to Ryan and his\n        team at Intel. What would you say are some of the big hurdles or big outcomes, big opportunities in that space? And do you agree that we're still at the very beginning, in our infancy if you\n        will, of adopting these technologies and discovering what they can do for us?\n      </rh-cue>\n      <rh-cue start=\"30:06\" voice=\"Ryan Loney\">\n        Yeah, I think we're definitely in the infancy and I think that what we've seen is, our customers are evolving and the people who are deploying on Intel hardware, they're trying to run more\n        complicated models. They're the models that are doing object detection or detecting defects and doing segmentation. In the past you could say, \"Here's a generic model that will do face\n        detection, or person detection, or vehicle detection, license plate detection.\" And those are general purpose models that you can just grab off the shelf and use them. But now we're moving\n        into the Anomalib scenarios, where I've got my own data and I'm trying to do something very specific and I'm the only one that has access to this data. You don't have that public data set\n        that you can go download that's under Creative Commons license for car batteries. It's just not something that's available.\n      </rh-cue>\n      <rh-cue start=\"30:57\" voice=\"Ryan Loney\">\n        And so, those use cases, the challenge with training those models and getting them optimized is the beginning of the pipeline. It's the data. You have to get the data, you have to annotate\n        it and the tools have to exist for you to do that. And that's part of the problem that we're trying to help solve. And then, the models are getting more complex. So if you think, just from\n        working with customers recently, they're no longer just trying to do image classification, \"Is it a dog or a cat?\" They've moved on to 3D point clouds and 3D segmentation models and things\n        that are like the speech synthesis example. These GPT models that are generating... You put a text input and it generates an image for you. It's just becoming much more advanced, much more\n        sophisticated and on larger images.\n      </rh-cue>\n      <rh-cue start=\"31:50\" voice=\"Ryan Loney\">\n        And so things like running super resolution and enhancing images, upscaling images, instead of just trying to take that 200 by 200 pixel image and classifying if it's a cat, now we're\n        talking about gigantic, huge images that we're processing and that all requires more resources or more optimized models. And every Computer Vision conference or AI conference, there's a new\n        latest and greatest architecture, there's new research paper, and things are getting adopted much faster. The lead time for a NeurIPS paper, CVPR, for a company to actually adopt and put\n        those into production, the time shortens every year.\n      </rh-cue>\n      <rh-cue start=\"32:34\" voice=\"Burr Sutter\">\n        Well Ryan, I got to tell you, I could talk to you, literally, all day about these topics, the various use cases, the various ways models are being optimized, how to put models into a\n        pipeline for average enterprise applications. I've enjoyed learning about OpenVINO and Anomalib. I'm fascinated by this, because I'll have a chance to go try this myself, taking advantage of\n        Red Hat OpenShift and taking advantage of our data science platform. On top of that, I will definitely go be poking at this myself. Thank you so much for your time today.\n      </rh-cue>\n      <rh-cue start=\"33:00\" voice=\"Ryan Loney\">\n        Thanks, Burr. This was a lot of fun. Thanks for having me.\n      </rh-cue>\n      <rh-cue start=\"33:05\" voice=\"Burr Sutter\">\n        You can check out the full transcript of our conversation and more resources, like a link to a white paper on OpenVINO and Anomalib at redhat.com/codecommentspodcast. This episode was\n        produced by Brent Simoneaux and Caroline Creaghead. Our sound designer is Christian Prohom. Our audio team includes Leigh Day, Stephanie Wonderlick, Mike Esser, Laura Barnes, Claire Allison,\n        Nick Burns, Aaron Williamson, Karen King, Boo Boo Howse, Rachel Ertel, Mike Compton, Ocean Matthews, Laura Walters, Alex Traboulsi, and Victoria Lawton. I'm your host, Burr Sutter. Thank you\n        for joining me today on Code Comments. I hope you enjoyed today's session and today's conversation, and I look forward to many more.\n      </rh-cue>\n    </rh-transcript>\n  </rh-audio-player>\n</div>\n\n<link rel=\"stylesheet\" href=\"demo.css\">\n<link rel=\"stylesheet\" href=\"../rh-audio-player-lightdom.css\">\n<script type=\"module\" src=\"customization.js\"></script>\n<!--playground-fold--><link rel=\"stylesheet\" href=\"../rhds-demo-base.css\">\n\n<!--playground-fold-end-->",
      "label": "Right To Left"
    },
    "demo/right-to-left/demo.css": {
      "content": ":host {\n  display: block;\n}\n\ndiv {\n  padding: 0 20px;\n}\n\n*[hidden] {\n  display: none;\n}\n\nlabel {\n  display: flex;\n  align-items: center;\n}\n\nlabel > *:last-child {\n  margin-left: 0.5em;\n}\n\nlabel > *:last-child:not([type=\"checkbox\"]) {\n  flex: 1 0 auto;\n}\n\n/*\n Warning:\n The following are demonstrations of using CSS variables to customize player color. \n They do not use our design token values for color.\n*/\nrh-audio-player.purple {\n  --rh-audio-player-background-color: #633ec5;\n  --rh-audio-player-range-thumb-color: #f56d6d;\n  --rh-audio-player-range-progress-color: #f56d6d;\n}\n\nrh-audio-player.purple.img {\n  --rh-audio-player-background-color: #000000;\n}\n\nrh-audio-player.purple.img::part(toolbar) {\n  background-image: url(\"https://www.redhat.com/cms/managed-files/episode-1-art-hero.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: right;\n}\n\nrh-audio-player.cyan {\n  --rh-audio-player-background-color: #00aee9;\n  --rh-audio-player-range-thumb-color: #ffe953;\n  --rh-audio-player-range-progress-color: #ffe953;\n}\n",
      "hidden": true
    },
    "demo/right-to-left/customization.js": {
      "content": "import '@rhds/elements/rh-audio-player/rh-audio-player.js';\nconst form = document.querySelector('form');\nconst player = document.querySelector('rh-audio-player');\nconst { poster } = player;\n\n/**\n * update audio player demo based on form selections\n */\nfunction updateDemo() {\n  const colorPalette = ['cyan', 'light'].includes(form.palette.value) ?\n    'light' : 'dark';\n  const colorClass =\n    ['cyan', 'purple', 'purple img'].includes(form.palette.value) ? form.palette.value : '';\n  player.poster = !form.poster.checked || form.palette.value === 'purple img' ? undefined : poster;\n  player.layout = form.layout.value !== '' ? form.layout.value : undefined;\n  if (colorPalette === player.colorPalette) {\n    const oldOn = player.colorPalette;\n    player.colorPalette = oldOn === 'dark' ? 'light' : 'dark';\n  }\n  player.setAttribute('class', colorClass);\n  player.colorPalette = colorPalette;\n  player.hasAccentColor = ['cyan', 'purple', 'purple img'].includes(form.palette.value);\n  player.requestUpdate();\n}\n\nif (form) {\n  form.addEventListener('input', updateDemo);\n  updateDemo();\n}\n",
      "hidden": true
    }
  }
};